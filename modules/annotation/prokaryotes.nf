include { pCollectFile } from './processes'
include { pDumpLogs } from '../utils/processes'

/**
*
* MMseqs2 is used to search for big input queries in large databases. 
* Multiple databases can be searched at the same time. Parts of the query and search database are loaded into the RAM.
# Huge databases along with to little RAM could lead to errors.
* Outputs will be saved in separate directories.
*
**/
process pMMseqs2 {
   
      container "${params.mmseqs2_image}"

      // Databases will be downloaded to a fixed place so that they can be used by future processes.
      // This fixed place has to be outside of the working-directory to be easy to find for every process.
      // Therefore this place has to be mounted to the docker container to be accessible during run time.
      // Another mount flag is used to get a key file (aws format) into the docker-container. 

      // This file is then used by s5cmd. The necessary mount points are generated by constructParametersObject(). 
      containerOptions Utils.constructParametersObject("mmseqs2", params, params.apptainer) + (params.apptainer ? "" : " --entrypoint='' " +  Utils.getDockerNetwork())

      tag "Sample: $sample, Database: $dbType"

      label 'large'

      secret { "${S3_DB_ACCESS}"!="" ? ["S3_${dbType}_ACCESS", "S3_${dbType}_SECRET"] : [] } 

      when params?.steps.containsKey("annotation") && params?.steps.annotation.containsKey("mmseqs2")

   input:
      val(binType)
      tuple val(sample), file(fasta), file(contig2GeneMapping), val(start), val(stop), val(numberOfChunks), val(dbType), val(parameters), val(columns), val(EXTRACTED_DB), val(DOWNLOAD_LINK), val(MD5SUM), val(S5CMD_PARAMS)

   output:
      tuple val("${dbType}"), val("${sample}"), val("${binType}"), val("${start}"), val("${stop}"), val("${numberOfChunks}"),\
	path("${sample}_${binType}.${start}.${stop}.${dbType}.blast.tsv"), optional:true, emit: blast
      tuple val("${sample}_${binType}"), val("${output}"), val(params.LOG_LEVELS.INFO), file(".command.sh"), \
        file(".command.out"), file(".command.err"), file(".command.log"), emit: logs

   shell:
   output = getOutput("${sample}", params.runid, "mmseqs2/${dbType}", "")
   S3_DB_ACCESS=params.steps?.annotation?.mmseqs2?."${dbType}"?.database?.download?.s5cmd && S5CMD_PARAMS.indexOf("--no-sign-request") == -1 ? "\$S3_${dbType}_ACCESS" : ""
   S3_DB_SECRET=params.steps?.annotation?.mmseqs2?."${dbType}"?.database?.download?.s5cmd && S5CMD_PARAMS.indexOf("--no-sign-request") == -1 ? "\$S3_${dbType}_SECRET" : ""
   ADDITIONAL_COLUMNS = "${columns}".trim() == "" ? "" : ",${columns}"
   template("mmseqs2.sh")
}


/**
*
* The MMseqs2 module taxonomy calls an internal module lca that implements an lowest common ancestor assignment for sequences by querying them against a seqTaxDB.
* Multiple databases can be searched at the same time. Parts of the query and search database are loaded into the RAM.
* Huge databases along with to little RAM could lead to errors.
* Outputs will be saved in separate directories.
*
**/
process pMMseqs2_taxonomy {
   
      container "${params.mmseqs2_image}"

      // Databases will be downloaded to a fixed place so that they can be used by future processes.
      // This fixed place has to be outside of the working-directory to be easy to find for every process.
      // Therefore this place has to be mounted to the docker container to be accessible during run time.
      // Another mount flag is used to get a key file (aws format) into the docker-container. 
      // This file is then used by s5cmd. The necessary mount points are generated by “constructParametersObject()”. 
      containerOptions Utils.constructParametersObject("mmseqs2_taxonomy", params, params.apptainer) + (params.apptainer ? "" : " --entrypoint='' " + Utils.getDockerNetwork())

      tag "Sample: $sample, Database_taxonomy: $dbType"

      label 'highmemLarge'

      secret { "${S3_TAX_DB_ACCESS}"!="" ? ["S3_TAX_${dbType}_ACCESS", "S3_TAX_${dbType}_SECRET"] : [] } 

      publishDir params.output, mode: "${params.publishDirMode}", saveAs: { filename -> getOutput("${sample}", params.runid, "mmseqs2_taxonomy/${dbType}", filename) }, \
         pattern: "{*.out,*.html,*.tsv}"
 
      when:
      runMMSeqsTaxonomy(params?.steps.containsKey("annotation") && params?.steps.annotation.containsKey("mmseqs2_taxonomy"), \
	   binType.equals("binned"), \
	   binType.equals("plasmid"), \
           params?.steps.containsKey("annotation") && params?.steps.annotation.containsKey("mmseqs2_taxonomy") && params?.steps.annotation.mmseqs2_taxonomy.runOnMAGs)

   input:
      val(binType)
      tuple val(sample), file(fasta), val(dbType), val(parameters), val(ramMode), val(initialMaxSensitivity), val(EXTRACTED_DB), val(DOWNLOAD_LINK), val(MD5SUM), val(S5CMD_PARAMS)
   
   output:
      tuple val("${sample}"), val("${dbType}"), path("${sample}_${binType}.${dbType}.taxonomy.tsv"), optional:true, emit: taxonomy
      tuple val("${sample}"), val("${dbType}"), path("${sample}_${binType}.${dbType}.krakenStyleTaxonomy.out"), optional:true, emit: krakenStyleTaxonomy
      tuple val("${sample}"), val("${dbType}"), path("${sample}_${binType}.${dbType}.krona.html"), optional:true, emit: kronaHtml
      tuple val("${sample}_${binType}"), val("${output}"), val(params.LOG_LEVELS.INFO), file(".command.sh"), \
        file(".command.out"), file(".command.err"), file(".command.log"), emit: logs


   shell:
   output = getOutput("${sample}", params.runid, "mmseqs2_taxonomy/${dbType}", "")
   // The maximum possible sensitivity is reduced each time the process is retried.
   // The reason for this behaviour is a bug that occurs at higher sensitivity levels.
   S3_TAX_DB_ACCESS=params.steps?.annotation?.mmseqs2_taxonomy?."${dbType}"?.database?.download?.s5cmd && S5CMD_PARAMS.indexOf("--no-sign-request") == -1 ? "\$S3_TAX_${dbType}_ACCESS" : ""
   S3_TAX_DB_SECRET=params.steps?.annotation?.mmseqs2_taxonomy?."${dbType}"?.database?.download?.s5cmd && S5CMD_PARAMS.indexOf("--no-sign-request") == -1 ? "\$S3_TAX_${dbType}_SECRET" : ""
   sensitivityRaw = initialMaxSensitivity - task.attempt + 1
   sensitivity = sensitivityRaw < 1 ? 1 : sensitivityRaw
   template("mmseqs2Taxonomy.sh")
}

workflow _wSplit {
  take:
    input
    chunkSize
  main:
    SAMPLE_IDX = 0
    FILE_IDX = 1
    FILE_2_IDX = 2
    FILE_3_IDX = 3
    COUNT_IDX=4
    CHUNK_SIZE_IDX=5
    ID_PLACEHOLDER = ""

    input | map { dataset -> [dataset[SAMPLE_IDX].toString(), ID_PLACEHOLDER, dataset[FILE_IDX], dataset[FILE_2_IDX]] } \
	| pCount | combine(chunkSize) | flatMap { sample -> \
   	Utils.splitFilesIndex(Integer.parseInt(sample[COUNT_IDX]), sample[CHUNK_SIZE_IDX], [sample[SAMPLE_IDX], sample[FILE_2_IDX], sample[FILE_3_IDX]]) } \
	| set { chunks }   	
  emit:
    chunks
}


workflow _wAnalyseProkaryotes {
  emit:
    sourceChannel
    contig2GeneMapping
    proteins
  main:

    // Collect all databases
    selectedDBs = params?.steps?.annotation?.mmseqs2
        .findAll { it.key != "chunkSize" }
        .collect {
            [
                it.key,
                it.value?.additionalParams?.search ?: "",
                it.value?.additionalParams?.additionalColumns ?: "",
                it.value?.database?.extractedDBPath ?: "",
                it.value.database?.download?.source ?: "",
                it.value.database?.download?.md5sum ?: "",
                it.value.database?.download?.s5cmd?.params ?: "",
            ]
        }

    selectedTaxDBs = params?.steps?.annotation?.mmseqs2_taxonomy
        .findAll { it.key != "runOnMAGs" }
        .collect {
            [
                it.key,
                it.value?.params ?: "",
                it.value?.ramMode ? "true" : "false",
                it.value?.initialMaxSensitivity ?: "",
                it.value?.database?.extractedDBPath ?: "",
                it.value.database?.download?.source ?: "",
                it.value.database?.download?.md5sum ?: "",
                it.value.database?.download?.s5cmd?.params ?: "",
            ]
        }

    // The following groupTuple operators need a counter for the expected number of bins or fasta files of a sample.
    // This allows groupTuple to not block until all samples are processed.

    // Run all amino acid outputs against all databases
    // Collect by sample name to bundle searches and avoid calls with small input files
     proteins \
        | map { [it[SAMPLE_IDX], it[PATH_IDX]] }
        | combine(fastaCounter, by: SAMPLE_IDX)
        | map { sample, path, size -> tuple(groupKey(sample, size), path) }
        | groupTuple()
        | set { groupedProkkaFaa }

    MMSEQS2_CHUNK_SIZE_DEFAULT = 7000

    // Split fasta files and run blast on each part in parallel
    _wSplit(
        groupedProkkaFaa
            | combine(contig2GeneMapping, by: SAMPLE_IDX),
        Channel.from(params.steps?.annotation?.mmseqs2?.chunkSize ?: MMSEQS2_CHUNK_SIZE_DEFAULT),
    )
        | combine(Channel.from(selectedDBs))
        | set { combinedMMseqs }

    pMMseqs2(sourceChannel, combinedMMseqs)

    FIRST_ELEM_IDX = 0
    FIRST_ELEM_DB_IDX = 0
    FIRST_ELEM_SAMPLE_IDX = 1
    FIRST_ELEM_TYPE_IDX = 2

    ELEM_PATH_IDX = 3

    pMMseqs2.out.blast
        | map { db, sample, type, start, stop, chunks, out -> [db + "_-_" + sample + "_-_" + type, db, sample, type, start, stop, chunks, out] }
        | map { key, db, sample, type, start, stop, chunks, out -> tuple(groupKey(key, chunks.toInteger()), [db, sample, type, out]) }
        | groupTuple()
        | map { key, dataset ->
            [
                dataset[FIRST_ELEM_IDX][FIRST_ELEM_DB_IDX],
                dataset[FIRST_ELEM_IDX][FIRST_ELEM_SAMPLE_IDX],
                dataset[FIRST_ELEM_IDX][FIRST_ELEM_TYPE_IDX],
                dataset.stream().map { elem -> elem[ELEM_PATH_IDX] }.collect(),
            ]
        }
        | combine(Channel.value("metaeuk"))
        | combine(Channel.value("blast"))
        | pCollectFile
        | flatMap { sample, type, dbType, blastFiles -> blastFiles.stream().map { fi -> [sample, type, dbType, fi] }.collect() }
        | set { collectedMMseqsResults }

    // The kegg database result is selected and reordered for the pKEGGFromMMseqs2 process input
    collectedMMseqsResults
        | filter { sample, type, dbType, blastFiles -> dbType == "kegg" }
        | map { sample, type, dbType, blastFiles -> [sample, type, blastFiles] }
        | pKEGGFromMMseqs2

    combinedMMseqsTax = groupedProkkaFaa | combine(Channel.from(selectedTaxDBs))
    pMMseqs2_taxonomy(sourceChannel, combinedMMseqsTax)

	pMMseqs2.out.logs \
    | mix(pMMseqs2_taxonomy.out.logs) \
	| mix(pResistanceGeneIdentifier.out.logs) \
	| mix(pKEGGFromMMseqs2.out.logs) | pDumpLogs

 emit:
    keggAnnotation = pKEGGFromMMseqs2.out.keggPaths
    mmseqs2_kronaHtml = pMMseqs2_taxonomy.out.kronaHtml
    mmseqs2_krakenTaxonomy = pMMseqs2_taxonomy.out.krakenStyleTaxonomy
    mmseqs2_taxonomy = pMMseqs2_taxonomy.out.taxonomy
    mmseqs2_blast = collectedMMseqsResults
}
