include { pCollectFile;  \
    _wCollectMetaEuk as _wCollectMetaEukGFF; \
    _wCollectMetaEuk as _wCollectMetaEukFAA; \
    _wCollectMetaEuk as _wCollectMetaEukFAS; \
    _wCollectMetaEuk as _wCollectMetaEukTaxPerContig; \
    _wCollectMetaEuk as _wCollectMetaEukTaxPerPred; \
    } from './processes'

include { pDumpLogs } from '../utils/processes'

/**
*
* The MMseqs2 module taxonomy calls an internal module lca that implements an lowest common ancestor assignment for sequences by querying them against a seqTaxDB.
* Multiple databases can be searched at the same time. Parts of the query and search database are loaded into the RAM.
* Huge databases along with to little RAM could lead to errors.
* Outputs will be saved in separate directories.
*
**/
process pMetaEuk {
   
      container "${params.metaeuk_image}"

      // Databases will be downloaded to a fixed place so that they can be used by future processes.
      // This fixed place has to be outside of the working-directory to be easy to find for every process.
      // Therefore this place has to be mounted to the docker container to be accessible during run time.
      // Another mount flag is used to get a key file (aws format) into the docker-container. 
      // This file is then used by s5cmd. The necessary mount points are generated by “constructParametersObject()”. 
      containerOptions Utils.constructParametersObject("metaeuk", params, params.apptainer) + (params.apptainer ? "" : " --entrypoint='' " + Utils.getDockerNetwork())

      tag "Sample: $sample, Database: $dbType"

      label 'highmemLarge'

      secret { "${S3_DB_ACCESS}"!="" ? ["S3_${dbType}_ACCESS", "S3_${dbType}_SECRET"] : [] } 

      when:
      params?.steps.containsKey("annotation") && params?.steps.annotation.containsKey("metaeuk")

   input:
      val(binType)
      tuple val(sample), file(fasta), file(contig2GeneMapping), val(start), val(stop), val(numberOfChunks), val(dbType), val(parameters), val(columns), val(EXTRACTED_DB), val(DOWNLOAD_LINK), val(MD5SUM), val(S5CMD_PARAMS)

   output:
      tuple val("${dbType}"), val("${sample}"), val("${binType}"), val("${start}"), val("${stop}"), val("${numberOfChunks}"),\
	     path("${sample}_${binType}.${start}.${stop}.${dbType}.gff"), optional:true, emit: gff 
      tuple val("${dbType}"), val("${sample}"), val("${binType}"), val("${start}"), val("${stop}"), val("${numberOfChunks}"),\
	     path("${sample}_${binType}.${start}.${stop}.${dbType}.tax_per_pred.tsv"), optional:true, emit: taxPerPred 
      tuple val("${dbType}"), val("${sample}"), val("${binType}"), val("${start}"), val("${stop}"), val("${numberOfChunks}"),\
	     path("${sample}_${binType}.${start}.${stop}.${dbType}.tax_per_contig.tsv"), optional:true, emit: taxPerContig  
      tuple val("${dbType}"), val("${sample}"), val("${binType}"), val("${start}"), val("${stop}"), val("${numberOfChunks}"),\
	     path("${sample}_${binType}.${start}.${stop}.${dbType}.codon.fas"), optional:true, emit: codon 
      tuple val("${dbType}"), val("${sample}"), val("${binType}"), val("${start}"), val("${stop}"), val("${numberOfChunks}"),\
	     path("${sample}_${binType}.${start}.${stop}.${dbType}.faa"), optional:true, emit: faa  
      tuple val("${sample}_${binType}_${start}_${stop}_${dbType}"), val("${output}"), val(params.LOG_LEVELS.INFO), file(".command.sh"), \
        file(".command.out"), file(".command.err"), file(".command.log"), emit: logs

   shell:
   output = Output.getOutput("${sample}", params.runid, "metaeuk/${dbType}", params.modules.annotation, "")
   S3_DB_ACCESS=params.steps?.annotation?.metaeuk?."${dbType}"?.database?.download?.s5cmd && S5CMD_PARAMS.indexOf("--no-sign-request") == -1 ? "\$S3_${dbType}_ACCESS" : ""
   S3_DB_SECRET=params.steps?.annotation?.metaeuk?."${dbType}"?.database?.download?.s5cmd && S5CMD_PARAMS.indexOf("--no-sign-request") == -1 ? "\$S3_${dbType}_SECRET" : ""
   ADDITIONAL_COLUMNS = "${columns}".trim() == "" ? "" : ",${columns}"
   template("metaeuk.sh")
}

/**
*
* The pCountFilter process is designed to count the number of sequences in a given FASTA file.
* It uses the seqkit tool to calculate the sequence statistics of a FASTA file, specifically extracting the sequence count.
* This process also allows you to optionally filter sequences. For example if only fasta files that were detected as eukaryotic should
* be used.
*
**/
process pCountFilter {

    label 'tiny'

    tag "Sample: $sample, BinID: $binID"

    container "${params.ubuntu_image}"

    input:
    val(splitFasta)
    tuple val(sample), path(fasta), path(metadata), path(header)

    output:
    tuple val("${sample}"), val("${binID}"), path("filtered/*"), path(metadata), env(COUNT) 

    shell:
    '''
    mkdir filtered
    mlr --itsv --otsv cat  !{header} | csvtk -t cut -f CONTIG > header_concat.tsv
    for fa in $(ls -1 !{fasta}); do 
        if [ ! -z "$(seqkit grep --quiet -f header_concat.tsv ${fa})" ]; then  
            if [ "!{splitFasta}" == "true" ]; then
                seqkit grep --quiet -f header_concat.tsv ${fa} > filtered/$(basename ${fa})
            else
                cp ${fa} filtered/;
            fi
        fi 
    done
    COUNT=$(seqkit stats -T <(cat filtered/*) | cut -d$'\t' -f 4 | tail -n 1)
    '''
}

/*
* 
* This method splits the input FASTA file in chunks.
*
*/
workflow _wSplitFasta {
  take:
    input
    chunkSize
    splitFasta
  main:
    SAMPLE_IDX = 0
    FILE_FASTA_IDX = 1 
    FILE_BIN_CONTIG_IDX = 2 
    FILE_CONTIG_HEADERS_IDX = 3 
    FILE_BIN_CONTIG_2_IDX = 3 
    COUNT_IDX = 4 
    CHUNK_SIZE_IDX = 5

    input | map { dataset -> [dataset[SAMPLE_IDX].toString(), dataset[FILE_FASTA_IDX], dataset[FILE_BIN_CONTIG_IDX], dataset[FILE_CONTIG_HEADERS_IDX]] } \
	     | set { fastaFile } 

	 pCountFilter(splitFasta, fastaFile) |  combine(chunkSize)  \
	   | flatMap { sample -> \
   	Utils.splitFilesIndex(Integer.parseInt(sample[COUNT_IDX]), sample[CHUNK_SIZE_IDX], [sample[SAMPLE_IDX], sample[FILE_BIN_CONTIG_IDX], sample[FILE_BIN_CONTIG_2_IDX]]) } \
	   | set { chunks } 
  emit:
    chunks
}



/**
* This workflow runs MetaEuk on a fasta file only if it was provided that the contig might be eukaryotic. For example by using whokaryote.
* 
* fasta: This channel contains fasta files as input.
*        Format: [SAMPLE, BIN_ID, PATH TO FASTA, KINGDOM]  
*        Example: [euk, euk_bin.5.fa, /path/to/bin/fasta/euk_bin.5.fa, Bacteria]
* 
* headerFiles:  These files include all contig header files.
*        Format: [SAMPLE, [HEADER_FILE]]
*        Example: [euk, [/path/to/euk_bin.3.fa_eukaryote_contig_headers.tsv]]
* 
* binContigMapping: This files states which contig belongs to which FASTA file 
*        Format: [SAMPLE, BIN_CONTIG_MAPPING]
*        Example: [euk, /path/to/euk_bin_contig_mapping.tsv]  
*
* fastaCounter: Number of FASTA files per sample 
*        Format: [SAMPLE, NUMBER_OF_FASTA_FILES]  
*        Example: [euk, 1]  
*
* splitFasta: Boolean flag for deciding whether a fasta should be split if certain fasta entries are detected as eukaryotic
*        Format: Boolean
*        Example: false
*/
workflow _wAnalyseEukaryotes {
  take:
    sourceChannel
    fasta
    headerFiles
    binContigMapping
    fastaCounter 
    splitFasta
  main:
    selectedEukDBs = params?.steps?.annotation?.metaeuk.findAll({ it.key != "chunkSize" }).collect({
            [it.key, it.value?.additionalParams?.search ?: "", \
		it.value?.additionalParams?.additionalColumns ?: "", \
	         it.value?.database?.extractedDBPath ?: "", \
             it.value.database?.download?.source ?: "", \
             it.value.database?.download?.md5sum ?: "", \
             it.value.database?.download?.s5cmd?.params ?: "" ]
      })

    SAMPLE_IDX=0
    FASTA_FILE_IDX=2

    // Collect and group all prokka files per sample 
    fasta | map { fa -> [ fa[SAMPLE_IDX], fa[FASTA_FILE_IDX]] } | combine(fastaCounter, by:SAMPLE_IDX) \
      | map { sample, path, size -> tuple( groupKey(sample, size), path ) }  | groupTuple() | set { groupedFasta }

    METAEUK_CHUNK_SIZE_DEFAULT=7000

    // Split fasta files and run blast on each part in parallel
    _wSplitFasta(groupedFasta | combine(binContigMapping, by: SAMPLE_IDX) | combine(headerFiles, by: SAMPLE_IDX), \
        Channel.from(params.steps?.annotation?.metaeuk?.chunkSize?:METAEUK_CHUNK_SIZE_DEFAULT), splitFasta) 
       | combine(Channel.from(selectedEukDBs)) \
       | set { combinedMetaEuk }

    pMetaEuk(sourceChannel, combinedMetaEuk)

    // Collect resulted chunks
	_wCollectMetaEukGFF(pMetaEuk.out.gff, Channel.value("gff"))
	_wCollectMetaEukFAA(pMetaEuk.out.faa, Channel.value("faa"))
	_wCollectMetaEukFAS(pMetaEuk.out.codon, Channel.value("codon"))
    _wCollectMetaEukTaxPerContig(pMetaEuk.out.taxPerContig, Channel.value("taxPerContig"))
    _wCollectMetaEukTaxPerPred(pMetaEuk.out.taxPerPred, Channel.value("taxPred"))

	pMetaEuk.out.logs | pDumpLogs
}
