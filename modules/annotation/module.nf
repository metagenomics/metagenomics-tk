include { wSaveSettingsList } from '../config/module'

include { pDumpLogs } from '../utils/processes'

import java.nio.file.Files
import java.nio.file.Paths

mode = 'mode_not_set'

// Function to generate the right output path for each tool/process to be feed into the publishDir call of said processes.
def getOutput(SAMPLE, RUNID, TOOL, filename){
    return SAMPLE + '/' + RUNID + '/' + params.modules.annotation.name + '/' + 
          params.modules.annotation.version.major + "." + 
          params.modules.annotation.version.minor + "." + 
          params.modules.annotation.version.patch +
          '/' + TOOL + '/' + filename
}

/**
* A function that takes a tool parameter key as input and extracts all underlying entries of said key.
* Should these entries contain references to external databases, s3/aws key-files, etc. a docker volume mount string is returned,
* so that docker processes run with this tool can include this string to have access to all external files. 
* See “/lib/Utils.groovy” for more information.
**/
def constructParametersObject(String tool){ 
  return params?.steps?.annotation?."$tool".findAll({ it.key != "runOnMAGs" }).collect{ Utils.getDockerMount(it.value?.database, params, 'true')}.join(" ")
}


/*
*
* This function decides if the MMSeqs taxonomy process should be executed on MAGs.
*
*/
def runMMSeqsTaxonomy(isMMSeqsTaxonomySettingSet, isBinned, isPlasmid, runOnBinned) {
    // True if the MMSeqsTaxonomy setting is set and plasmids are not used as input and 
    // either the sample is not binned 
    // or should explicitly run on binned samples 
    return isMMSeqsTaxonomySettingSet && !isPlasmid && (!isBinned || runOnBinned)
}


/**
*
* MMseqs2 is used to search for big input queries in large databases. 
* Multiple databases can be searched at the same time. Parts of the query and search database are loaded into the RAM.
# Huge databases along with to little RAM could lead to errors.
* Outputs will be saved in separate directories.
*
**/
process pMMseqs2 {
   
      container "${params.mmseqs2_image}"

      // Databases will be downloaded to a fixed place so that they can be used by future processes.
      // This fixed place has to be outside of the working-directory to be easy to find for every process.
      // Therefore this place has to be mounted to the docker container to be accessible during run time.
      // Another mount flag is used to get a key file (aws format) into the docker-container. 

      // This file is then used by s5cmd. The necessary mount points are generated by “constructParametersObject()”. 
      containerOptions constructParametersObject("mmseqs2")
 
      tag "Sample: $sample, Database: $dbType"

      label 'highmemLarge'

      secret { "${S3_DB_ACCESS}"!="" ? ["S3_${dbType}_ACCESS", "S3_${dbType}_SECRET"] : [] } 

      publishDir params.output, mode: "${params.publishDirMode}", saveAs: { filename -> getOutput("${sample}", params.runid, "mmseqs2/${dbType}", filename) }, \
         pattern: "{**.blast.tsv}"

      when params?.steps.containsKey("annotation") && params?.steps.annotation.containsKey("mmseqs2")

   input:
      val(binType)
      tuple val(sample), file(fasta), file(contig2GeneMapping), val(dbType), val(parameters), val(EXTRACTED_DB), val(DOWNLOAD_LINK), val(MD5SUM), val(S5CMD_PARAMS)
   
   output:
      tuple val("${dbType}"), val("${sample}"), val("${binType}"), path("${sample}_${binType}.${dbType}.blast.tsv"), optional:true, emit: blast
      tuple val("${sample}_${binType}"), val("${output}"), val(params.LOG_LEVELS.INFO), file(".command.sh"), \
        file(".command.out"), file(".command.err"), file(".command.log"), emit: logs

   shell:
   output = getOutput("${sample}", params.runid, "mmseqs2/${dbType}", "")
   S3_DB_ACCESS=params.steps?.annotation?.mmseqs2?."${dbType}"?.database?.download?.s5cmd && S5CMD_PARAMS.indexOf("--no-sign-request") == -1 ? "\$S3_${dbType}_ACCESS" : ""
   S3_DB_SECRET=params.steps?.annotation?.mmseqs2?."${dbType}"?.database?.download?.s5cmd && S5CMD_PARAMS.indexOf("--no-sign-request") == -1 ? "\$S3_${dbType}_SECRET" : ""
   '''
   mkdir -p !{params.polished.databases}
   # if no local database is referenced, start download part
   if [ -z "!{EXTRACTED_DB}" ] 
   then
         # polished params end with a / so no additional one is needed at the end
         DATABASE=!{params.polished.databases}mmseqs2 
         mkdir -p ${DATABASE}/!{dbType}
         LOCK_FILE=${DATABASE}/!{dbType}/lock.txt

	 # Check if access and secret keys are necessary for s5cmd
         if [ ! -z "!{S3_DB_ACCESS}" ]
         then
            export AWS_ACCESS_KEY_ID=!{S3_DB_ACCESS}
            export AWS_SECRET_ACCESS_KEY=!{S3_DB_SECRET}
         fi
         
         # Create and try to lock the given “LOCK_FILE”. If the file is locked no other process will download the same database simultaneously
         # and wait until the database is downloaded.  Depending on your database path (starts with s3:// | https:// | /../../.. ) 
         # one of the flock input commands is executed to download and decompress the database.
         # If a database is present at the given path, checksums are compared, if they are identical the download will be omitted.  
         flock ${LOCK_FILE} concurrentDownload.sh --output=${DATABASE}/!{dbType} \
            --link=!{DOWNLOAD_LINK} \
            --httpsCommand="wget -O mmseqs.!{dbType}.tar.zst !{DOWNLOAD_LINK}  && zstd --rm -T!{task.cpus} -d mmseqs.!{dbType}.tar.zst && tar -xvf mmseqs.!{dbType}.tar && rm mmseqs.!{dbType}.tar" \
            --s3FileCommand="s5cmd !{S5CMD_PARAMS} cp --concurrency !{task.cpus} !{DOWNLOAD_LINK} mmseqs.!{dbType}.tar.zst && zstd --rm -T!{task.cpus} -d mmseqs.!{dbType}.tar.zst && tar -xvf mmseqs.!{dbType}.tar && rm mmseqs.!{dbType}.tar" \
            --s3DirectoryCommand="s5cmd !{S5CMD_PARAMS} cp --concurrency !{task.cpus} !{DOWNLOAD_LINK} mmseqs.!{dbType}.tar.zst && zstd --rm -T!{task.cpus} -d mmseqs.!{dbType}.tar.zst && tar -xvf mmseqs.!{dbType}.tar && rm mmseqs.!{dbType}.tar" \
	    --s5cmdAdditionalParams="!{S5CMD_PARAMS}" \
            --localCommand="zstd -T!{task.cpus} -d !{DOWNLOAD_LINK} -o mmseqs.!{dbType}.tar && tar -xvf mmseqs.!{dbType}.tar && rm mmseqs.!{dbType}.tar" \
            --expectedMD5SUM=!{MD5SUM}
          
          # Path of the newly downloaded database. A mmseqs2 database consists out of multiple files,
          # the unique lookup file is searched to determine the basename of each database.
          FILEPATH=$(find ${DATABASE}/!{dbType} -name *.lookup -type f)
          # remove extension to get the files basename.
          MMSEQS2_DATABASE_DIR=${FILEPATH%.*}
   else
          # If an extracted database is present use that path. 
          MMSEQS2_DATABASE_DIR="!{EXTRACTED_DB}"
   fi
    MMSEQS_HEADER="query,target,pident,alnlen,mismatch,gapopen,qstart,qend,tstart,tend,evalue,bits,qlen,tlen,qcov,tcov"
    OUTPUT_TMP_TSV="!{sample}_!{binType}.!{dbType}.blast.tmp.tsv"
    OUTPUT_TSV="!{sample}_!{binType}.!{dbType}.blast.tsv"

    # According to the MMSeqs docs the --split-memory-limit parameter defines the RAM usage for *about* 80 percent of the total RAM consumption.
    # We set the ram limit parameter to 75 percent of the total available RAM to make sure to not run into out of memory errors.
    RAM_LIMIT="$(awk -v RATIO=75 -v RAM=$(echo !{task.memory} | cut -f 1 -d ' ') 'BEGIN { print int(RAM / 100 * RATIO) }')G"

    mkdir tmp
    # Only mmseqs2 databases can be used for every kind of search. Inputs have to be converted first.
    mmseqs createdb !{fasta} queryDB
    # Load all indices into memory to increase searching speed
    mmseqs touchdb --threads !{task.cpus} queryDB
    mmseqs touchdb --threads !{task.cpus} ${MMSEQS2_DATABASE_DIR}
    mmseqs search queryDB ${MMSEQS2_DATABASE_DIR} !{sample}_!{binType}.!{dbType}.results.database tmp !{parameters} \
	--threads !{task.cpus} --split-memory-limit ${RAM_LIMIT}
    # mmseqs2 searches produce output databases. These have to be converted to a more useful format. The blast -outfmt 6 in this case.
    mmseqs convertalis queryDB ${MMSEQS2_DATABASE_DIR} !{sample}_!{binType}.!{dbType}.results.database ${OUTPUT_TMP_TSV} \
	--threads !{task.cpus} --format-output ${MMSEQS_HEADER}

    # Try only if the output file is not empty, csvtk will fail otherwise
    if [ -s ${OUTPUT_TMP_TSV}  ]; then
        # Add header
        sed  -i "1i $(echo ${MMSEQS_HEADER} | tr ',' '\t')" ${OUTPUT_TMP_TSV}

        # Add BIN_ID and SAMPLE column
        csvtk -t -T join -f "locus_tag;query" \
	    <(csvtk -t -T concat !{contig2GeneMapping} | csvtk -t -T cut -f SAMPLE,BIN_ID,CONTIG,locus_tag) ${OUTPUT_TMP_TSV} > ${OUTPUT_TSV}

        # use "query" column name instead of "locus_tag"
        sed -i -e "1s/locus_tag/query/" ${OUTPUT_TSV}
    fi
    '''
}

MAX_SENSITIVITY = 6

/**
*
* The MMseqs2 module taxonomy calls an internal module lca that implements an lowest common ancestor assignment for sequences by querying them against a seqTaxDB.
* Multiple databases can be searched at the same time. Parts of the query and search database are loaded into the RAM.
* Huge databases along with to little RAM could lead to errors.
* Outputs will be saved in separate directories.
*
**/
process pMMseqs2_taxonomy {
   
      container "${params.mmseqs2_image}"

      // Databases will be downloaded to a fixed place so that they can be used by future processes.
      // This fixed place has to be outside of the working-directory to be easy to find for every process.
      // Therefore this place has to be mounted to the docker container to be accessible during run time.
      // Another mount flag is used to get a key file (aws format) into the docker-container. 
      // This file is then used by s5cmd. The necessary mount points are generated by “constructParametersObject()”. 
      containerOptions constructParametersObject("mmseqs2_taxonomy")
 
      tag "Sample: $sample, Database_taxonomy: $dbType"

      label 'highmemLarge'

      secret { "${S3_TAX_DB_ACCESS}"!="" ? ["S3_TAX_${dbType}_ACCESS", "S3_TAX_${dbType}_SECRET"] : [] } 

      publishDir params.output, mode: "${params.publishDirMode}", saveAs: { filename -> getOutput("${sample}", params.runid, "mmseqs2_taxonomy/${dbType}", filename) }, \
         pattern: "{*.out,*.html,*.tsv}"
 
      when:
      runMMSeqsTaxonomy(params?.steps.containsKey("annotation") && params?.steps.annotation.containsKey("mmseqs2_taxonomy"), \
	   binType.equals("binned"), \
	   binType.equals("plasmid"), \
           params?.steps.containsKey("annotation") && params?.steps.annotation.containsKey("mmseqs2_taxonomy") && params?.steps.annotation.mmseqs2_taxonomy.runOnMAGs)

   input:
      val(binType)
      tuple val(sample), file(fasta), val(dbType), val(parameters), val(ramMode), val(EXTRACTED_DB), val(DOWNLOAD_LINK), val(MD5SUM), val(S5CMD_PARAMS)
   
   output:
      tuple val("${dbType}"), val("${sample}"), path("${sample}_${binType}.${dbType}.taxonomy.tsv"), optional:true, emit: taxonomy
      tuple val("${dbType}"), val("${sample}"), path("${sample}_${binType}.${dbType}.krakenStyleTaxonomy.out"), optional:true, emit: krakenStyleTaxonomy
      tuple val("${dbType}"), val("${sample}"), path("${sample}_${binType}.${dbType}.krona.html"), optional:true, emit: kronaHtml
      tuple val("${sample}_${binType}"), val("${output}"), val(params.LOG_LEVELS.INFO), file(".command.sh"), \
        file(".command.out"), file(".command.err"), file(".command.log"), emit: logs


   shell:
   output = getOutput("${sample}", params.runid, "mmseqs2_taxonomy/${dbType}", "")
   // The maximum possible sensitivity is reduced each time the process is retried.
   // The reason for this behaviour is a bug that occurs at higher sensitivity levels.
   sensitivity = MAX_SENSITIVITY - task.attempt + 1
   S3_DB_ACCESS=params.steps?.annotation?.mmseqs2_taxonomy?."${dbType}"?.database?.download?.s5cmd && S5CMD_PARAMS.indexOf("--no-sign-request") == -1 ? "\$S3_TAX_${dbType}_ACCESS" : ""
   S3_DB_SECRET=params.steps?.annotation?.mmseqs2_taxonomy?."${dbType}"?.database?.download?.s5cmd && S5CMD_PARAMS.indexOf("--no-sign-request") == -1 ? "\$S3_TAX_${dbType}_SECRET" : ""
   '''
   mkdir -p !{params.polished.databases}
   # if no local database is referenced, start download part
      if [ -z "!{EXTRACTED_DB}" ]
      then
            # polished params end with a / so no additional one is needed at the end
            DATABASE=!{params.polished.databases}mmseqs2
            mkdir -p ${DATABASE}/!{dbType}
            LOCK_FILE=${DATABASE}/!{dbType}/lock.txt

	    # Check if access and secret keys are necessary for s5cmd
            if [ ! -z "!{S3_DB_ACCESS}" ]
            then
               export AWS_ACCESS_KEY_ID=!{S3_DB_ACCESS}
               export AWS_SECRET_ACCESS_KEY=!{S3_DB_SECRET}
            fi

            # Create and try to lock the given “LOCK_FILE”. If the file is locked no other process will download the same database simultaneously
            # and wait until the database is downloaded.  Depending on your database path (starts with s3:// | https:// | /../../.. )
            # one of the flock input commands is executed to download and decompress the database.
            # If a database is present at the given path, checksums are compared, if they are identical the download will be omitted.
            flock ${LOCK_FILE} concurrentDownload.sh --output=${DATABASE}/!{dbType} \
               --link=!{DOWNLOAD_LINK} \
               --httpsCommand="wget -O mmseqs.!{dbType}.tar.zst !{DOWNLOAD_LINK}  && zstd --rm -T!{task.cpus} -d mmseqs.!{dbType}.tar.zst && tar -xvf mmseqs.!{dbType}.tar && rm mmseqs.!{dbType}.tar" \
               --s3FileCommand="s5cmd !{S5CMD_PARAMS} cp --concurrency !{task.cpus}  !{DOWNLOAD_LINK} mmseqs.!{dbType}.tar.zst && zstd --rm -T!{task.cpus} -d mmseqs.!{dbType}.tar.zst && tar -xvf mmseqs.!{dbType}.tar && rm mmseqs.!{dbType}.tar" \
               --s3DirectoryCommand="s5cmd !{S5CMD_PARAMS} cp --concurrency !{task.cpus} !{DOWNLOAD_LINK} mmseqs.!{dbType}.tar.zst && zstd --rm -T!{task.cpus} -d mmseqs.!{dbType}.tar.zst && tar -xvf mmseqs.!{dbType}.tar && rm mmseqs.!{dbType}.tar" \
   	    --s5cmdAdditionalParams="!{S5CMD_PARAMS}" \
               --localCommand="zstd -T!{task.cpus} -d !{DOWNLOAD_LINK} -o mmseqs.!{dbType}.tar && tar -xvf mmseqs.!{dbType}.tar && rm mmseqs.!{dbType}.tar" \
               --expectedMD5SUM=!{MD5SUM}

             # Path of the newly downloaded database. A mmseqs2 database consists out of multiple files,
             # the unique lookup file is searched to determine the basename of each database.
             FILEPATH=$(find ${DATABASE}/!{dbType} -name *.lookup -type f)
             # remove extension to get the files basename.
             MMSEQS2_DATABASE_DIR=${FILEPATH%.*}
      else
             # If an extracted database is present use that path.
             MMSEQS2_DATABASE_DIR="!{EXTRACTED_DB}"
      fi

    mkdir tmp
    # Only mmseqs2 databases can be used for every kind of search. Inputs have to be converted first.
    mmseqs createdb !{fasta} queryDB
    # If the ramMode is set to true, the whole database will be loaded into the RAM. Do not forget to set the MMseqs2 parameter accordingly, --db-load-mode 3.
    if !{ramMode}
    then
        # Load all indices into memory to increase searching speed
        mmseqs touchdb --threads !{task.cpus} queryDB
        mmseqs touchdb --threads !{task.cpus} ${MMSEQS2_DATABASE_DIR}
    fi
    # Define taxonomies
    mmseqs taxonomy queryDB ${MMSEQS2_DATABASE_DIR} !{sample}_!{binType}.!{dbType}.taxresults.database tmp !{parameters}  --start-sens 3 --sens-steps 1 -s !{sensitivity} --threads !{task.cpus}
    # mmseqs2 searches produce output databases. These have to be converted to more useful formats.
    mmseqs createtsv queryDB !{sample}_!{binType}.!{dbType}.taxresults.database !{sample}_!{binType}.!{dbType}.taxonomy.tsv --threads !{task.cpus}
    mmseqs taxonomyreport ${MMSEQS2_DATABASE_DIR} !{sample}_!{binType}.!{dbType}.taxresults.database !{sample}_!{binType}.!{dbType}.krakenStyleTaxonomy.out
    mmseqs taxonomyreport ${MMSEQS2_DATABASE_DIR} !{sample}_!{binType}.!{dbType}.taxresults.database !{sample}_!{binType}.!{dbType}.krona.html --report-mode 1
   '''
}

process pResistanceGeneIdentifier {
   
   container "${params.rgi_image}"
      
   containerOptions Utils.getDockerMount(params?.steps?.annotation?.rgi?.database, params)
 
   tag "Sample: $sample, BinID: $binID"

   label 'small'

   publishDir params.output, mode: "${params.publishDirMode}", saveAs: { filename -> getOutput("${sample}", params.runid, "rgi", filename) }, \
         pattern: "{**.rgi.tsv,**.png,**.eps,**.csv}"

   when params.steps.containsKey("annotation") && params?.steps.annotation.containsKey("rgi")

   secret { "${S3_rgi_ACCESS}"!="" ? ["S3_rgi_ACCESS", "S3_rgi_SECRET"] : [] } 

   input:
      tuple val(sample), val(binID), file(fasta)
   
   output:
      tuple val("${sample}"), val("${binID}"), path("${binID}.rgi.tsv"), optional:true, emit: results
      tuple val("${sample}"), val("${binID}"), path("${binID}.rgi-1.csv"), optional:true, emit: resultsGenes
      tuple val("${sample}"), val("${binID}"), path("${binID}.rgi-1.eps"), \
	path("${sample}_${binID}.rgi-1.png"), optional:true, emit: png
      tuple val("${binID}"), val("${output}"), val(params.LOG_LEVELS.INFO), file(".command.sh"), \
        file(".command.out"), file(".command.err"), file(".command.log"), emit: logs

   shell:
   output = getOutput("${sample}", params.runid, "rgi", "")
   EXTRACTED_DB=params.steps?.annotation?.rgi?.database?.extractedDBPath ?: ""
   DOWNLOAD_LINK=params.steps?.annotation?.rgi?.database?.download?.source ?: ""
   MD5SUM=params?.steps?.annotation?.rgi?.database?.download?.md5sum ?: ""
   S5CMD_PARAMS=params.steps?.annotation?.rgi?.database?.download?.s5cmd?.params ?: ""
   S3_rgi_ACCESS=params.steps?.annotation?.rgi?.database?.download?.s5cmd && S5CMD_PARAMS.indexOf("--no-sign-request") == -1 ? "\$S3_rgi_ACCESS" : ""
   S3_rgi_SECRET=params.steps?.annotation?.rgi?.database?.download?.s5cmd && S5CMD_PARAMS.indexOf("--no-sign-request") == -1 ? "\$S3_rgi_SECRET" : ""
   '''
   mkdir -p !{params.polished.databases}
   ADDITIONAL_RGI_PARAMS="!{params.steps?.annotation?.rgi?.additionalParams}"

   # Check developer documentation
   CARD_JSON=""
   if [ -z "!{EXTRACTED_DB}" ] 
   then
        DATABASE=!{params.polished.databases}/rgi
        LOCK_FILE=${DATABASE}/lock.txt

	# Check if access and secret keys are necessary for s5cmd
        if [ ! -z "!{S3_rgi_ACCESS}" ]
        then
          export AWS_ACCESS_KEY_ID=!{S3_rgi_ACCESS}
          export AWS_SECRET_ACCESS_KEY=!{S3_rgi_SECRET}
        fi

        # Download CARD database
        mkdir -p ${DATABASE}
        flock ${LOCK_FILE} concurrentDownload.sh --output=${DATABASE} \
         --link=!{DOWNLOAD_LINK} \
         --httpsCommand="wget -O data !{DOWNLOAD_LINK} && tar -xvf data && rm data" \
         --s3DirectoryCommand="s5cmd !{S5CMD_PARAMS} cp --concurrency !{task.cpus}  !{DOWNLOAD_LINK} . " \
         --s3FileCommand="s5cmd !{S5CMD_PARAMS} cp --concurrency !{task.cpus} !{DOWNLOAD_LINK} data && tar -xvf data  && rm data" \
	 --s5cmdAdditionalParams="!{S5CMD_PARAMS}" \
         --localCommand="tar -xvf !{DOWNLOAD_LINK}" \
         --expectedMD5SUM=!{MD5SUM}

         CARD_JSON="$(readlink -f ${DATABASE}/out/card.json)"
   else
         CARD_JSON="!{EXTRACTED_DB}"
   fi

   # gunzip (if required) and strip '*' sign from amino acid files
   zcat -f !{fasta} | sed 's/*//g' > input.faa

   mkdir output
   OUTPUT_ID=!{binID}.rgi
   RGI_OUTPUT=output/${OUTPUT_ID}
   # load CARD database and run rgi
   rgi load --card_json ${CARD_JSON} --local
   rgi main --input_sequence input.faa \
               --output_file ${RGI_OUTPUT} --input_type protein --local \
               --alignment_tool DIAMOND --num_threads !{task.cpus} --clean ${ADDITIONAL_RGI_PARAMS}

   RGI_OUT_TMP=!{binID}.rgi.tmp.tsv
   RGI_OUT=!{binID}.rgi.tsv
   # Produce files only if there is an actual output
   if [ $(tail -n +2 ${RGI_OUTPUT}.txt | wc -l) -gt 0 ]; then 

     #  add sample and binid information to rgi output
     sed  '1 s/^/SAMPLE\tBIN_ID\t/g' ${RGI_OUTPUT}.txt  \
	| sed "2,$ s/^/!{sample}\t!{binID}\t/g" > ${RGI_OUT_TMP}

     # ORF_ID generated by rgi contains ORF names with whitespaces (e.g.: "POGDFCMJ_00141 Methionyl-tRNA formyltransferase")
     # The following lines shortens the ID (e.g.: POGDFCMJ_00141)
     paste -d$"\t" <(cut -f 3 ${RGI_OUT_TMP} | cut -d ' ' -f 1) ${RGI_OUT_TMP} \
	| sed '1 s/ORF_ID/ORF_SHORT_ID/' > ${RGI_OUT}

     # RGI Heatmap fails because of the following error: https://github.com/arpcard/rgi/issues/188
     trap 'if [ "$?" == 1 ] && ( grep -q "ValueError: zero-size array to reduction operation fmin which has no identity" stderr.log ); then echo "Heatmap could not be produced"; exit 0; fi' EXIT
     rgi heatmap --input output --output ${OUTPUT_ID} 2> >(tee -a stderr.log >&2)
   fi

   '''
}

/**
 * Prodigal - Protein-coding gene prediction for prokaryotic genomes
 * @param sample: Sample name
 * @param contigs: File containing all contigs in a .gz format
 * @return: Files containing all predicted proteins and genes in .faa and .ffn format
 *
 * Mainly used as input for hmmSearch/MagScot as the real annotation is done with Prokka.
 **/
process pProdigal {
      // Re-Use the gtdb-tk container for Prodigal to safe space, as it is ancient
      container "${params.gtdbtk_image}"

      tag "Sample: $sample"

      label 'small'

      publishDir params.output, mode: "${params.publishDirMode}", saveAs: { filename -> getOutput("${sample}", params.runid, "prodigal", filename) }

      when params.steps.containsKey("binning") && params?.steps.binning.containsKey("magscot")

   input:
      tuple val(sample), path(contigs)

   output:
      tuple val("${sample}"), file("${sample}.prodigal.faa"), emit: prodigal_faa
      tuple val("${sample}"), file("${sample}.prodigal.ffn"), emit: prodigal_ffn
      tuple file(".command.sh"), file(".command.out"), file(".command.err"), file(".command.log")

   shell:
   '''
   zcat !{contigs} | prodigal !{params.steps?.binning?.magscot?.prodigal?.additionalParams} -a !{sample}.prodigal.faa -d !{sample}.prodigal.ffn -o tmpfile
   '''
}


/**
 * HMMSearch - Search for protein domains in protein sequences
 * @param sample: Sample name
 * @param faaFile: File containing all predicted proteins in .faa format
 * @return: Files containing all annotated proteins, in all- and top-hits files,
 * as well as a cut down version of the top-hits file for MagScot.
 *
 * Mainly used as input for MagScot.
 **/
process pHmmSearch {

      // Re-Use the gtdb-tk container for Prodigal to safe space
      container "${params.gtdbtk_image}"

      containerOptions Utils.getDockerMount(params?.steps?.binning?.magscot?.hmmSearch?.database, params)

      tag "Sample: $sample"

      label 'highmemMedium'

      secret { "${S3_gtdb_ACCESS}"!="" ? ["S3_gtdb_ACCESS", "S3_gtdb_SECRET"] : [] } 

      publishDir params.output, mode: "${params.publishDirMode}", saveAs: { filename -> getOutput("${sample}", params.runid, "hmmSearch", filename) }

      when params.steps.containsKey("binning") && params?.steps.binning.containsKey("magscot")

   input:
      tuple val(sample), file(faaFile)

   output:
      tuple val("${sample}"), file("${sample}.hmm.tigr.hit.tsv"), optional:true, emit: tigr_hits
      tuple val("${sample}"), file("${sample}.hmm.pfam.hit.tsv"), optional:true, emit: pfam_hits
      tuple val("${sample}"), file("${sample}.hmm.tigr.out"), optional:true, emit: tigr_out
      tuple val("${sample}"), file("${sample}.hmm.pfam.out"), optional:true, emit: pfam_out
      tuple val("${sample}"), file("${sample}.hmm.allhits.tsv"), optional:true, emit: allhits
      tuple file(".command.sh"), file(".command.out"), file(".command.err"), file(".command.log")


   shell:
   output = getOutput("${sample}", params.runid, "hmmSearch", "")
   EXTRACTED_DB=params.steps?.binning?.magscot?.hmmSearch?.database?.extractedDBPath ?: ""
   DOWNLOAD_LINK=params.steps?.binning?.magscot?.hmmSearch?.database?.download?.source ?: ""
   MD5SUM=params?.steps?.binning?.magscot?.hmmSearch?.database?.download?.md5sum ?: ""
   S5CMD_PARAMS=params.steps?.binning?.magscot?.hmmSearch?.database?.download?.s5cmd?.params ?: ""
   S3_gtdb_ACCESS=params.steps?.binning?.magscot?.hmmSearch?.database?.download?.s5cmd && S5CMD_PARAMS.indexOf("--no-sign-request") == -1 ? "\$S3_gtdb_ACCESS" : ""
   S3_gtdb_SECRET=params.steps?.binning?.magscot?.hmmSearch?.database?.download?.s5cmd && S5CMD_PARAMS.indexOf("--no-sign-request") == -1 ? "\$S3_gtdb_SECRET" : ""
   '''
   ADDITIONAL_HMMSEARCH_PARAMS="!{params.steps?.binning?.magscot?.hmmSearch?.additionalParams}"

   gtdb_download.sh "!{EXTRACTED_DB}" "!{DOWNLOAD_LINK}" "!{S5CMD_PARAMS}" "!{task.cpus}" "!{params.polished.databases}" "!{MD5SUM}" "!{S3_gtdb_ACCESS}" "!{S3_gtdb_SECRET}" 

   GTDB=$(cat gtdbPath.txt)

   # Run hmmsearch
   hmmsearch --cpu !{task.cpus} ${ADDITIONAL_HMMSEARCH_PARAMS} -o !{sample}.hmm.tigr.out --tblout !{sample}.hmm.tigr.hit.tsv ${GTDB}/markers/tigrfam/tigrfam.hmm !{faaFile}
   hmmsearch --cpu !{task.cpus} ${ADDITIONAL_HMMSEARCH_PARAMS} -o !{sample}.hmm.pfam.out --tblout !{sample}.hmm.pfam.hit.tsv ${GTDB}/markers/pfam/Pfam-A.hmm !{faaFile}

   # Remove header and create all-hits file
   cat !{sample}.hmm.tigr.hit.tsv | grep -v "^#" | awk '{print $1"\t"$3"\t"$5}' > !{sample}.tigr
   cat !{sample}.hmm.pfam.hit.tsv | grep -v "^#" | awk '{print $1"\t"$4"\t"$5}' > !{sample}.pfam
   cat !{sample}.tigr !{sample}.pfam > !{sample}.hmm.allhits.tsv
   '''


}

/**
*
* pKEGGFromBlast is build to handle results in the outfmt 6 file standard.
* These search results will be compared to kegg link-files to produce a file where all available kegg information
* for these search results is collected in a centralized way/file.
* You need to call (and fill out) the aws credential file with -c to use this module as kegg is commercial an the database has to be placed in your project s3!
*
**/
process pKEGGFromBlast {

      tag "$sample"

      label 'small'

      container "${params.python_env_image}"

      // Databases will be downloaded to a fixed place so that they can be used by future processes.
      // These fixed place has to be outside of the working-directory to be easy to find for every process.
      // Therefore this place has to be mounted to the docker container to be accessible during runtime.
      // Another mount flag is used to get a key file (aws format) into the docker-container. 
      // This file is then used by s5cmd. 

      containerOptions Utils.getDockerMount(params.steps?.annotation?.keggFromBlast?.database, params)
      publishDir params.output, mode: "${params.publishDirMode}", saveAs: { filename -> getOutput("${sample}", params.runid, "keggFromBlast", filename) }, \
         pattern: "{**.tsv}"

      when params?.steps.containsKey("annotation") && params?.steps.annotation.containsKey("keggFromBlast")

   secret { "${S3_KEGG_ACCESS}"!="" ? ["S3_kegg_ACCESS", "S3_kegg_SECRET"] : [] } 

   input:
      tuple val(sample), val(binType), file(blast_result)

   output:
      tuple val("${sample}"), path("${sample}_${binType}_keggPaths.tsv"), emit: kegg_paths
      tuple val("${sample}_${binType}"), val("${output}"), val(params.LOG_LEVELS.INFO), file(".command.sh"), \
        file(".command.out"), file(".command.err"), file(".command.log"), emit: logs

   shell:
      output = getOutput("${sample}", params.runid, "keggFromBlast", "")
      DOWNLOAD_LINK=params.steps?.annotation?.keggFromBlast?.database?.download?.source ?: ""
      MD5SUM=params?.steps?.annotation?.keggFromBlast?.database?.download?.md5sum ?: ""
      S5CMD_PARAMS=params.steps?.annotation?.keggFromBlast?.database?.download?.s5cmd?.params ?: ""
      EXTRACTED_DB=params.steps?.annotation?.keggFromBlast?.database?.extractedDBPath ?: ""
      S3_KEGG_ACCESS=params.steps?.annotation?.keggFromBlast?.database?.download?.s5cmd && S5CMD_PARAMS.indexOf("--no-sign-request") == -1 ? "\$S3_kegg_ACCESS" : ""
      S3_KEGG_SECRET=params.steps?.annotation?.keggFromBlast?.database?.download?.s5cmd && S5CMD_PARAMS.indexOf("--no-sign-request") == -1 ? "\$S3_kegg_SECRET" : ""
      '''
      mkdir -p !{params.polished.databases}
      # Check developer documentation
      KEGG_DB=""
      if [[ -z "!{EXTRACTED_DB}" ]] 
      then
        DATABASE=!{params.polished.databases}/kegg
        LOCK_FILE=${DATABASE}/lock.txt

	# Check if access and secret keys are necessary for s5cmd
        if [ ! -z "!{S3_KEGG_ACCESS}" ]
        then
          export AWS_ACCESS_KEY_ID=!{S3_KEGG_ACCESS}
          export AWS_SECRET_ACCESS_KEY=!{S3_KEGG_SECRET}
        fi

        # Download CARD database
        mkdir -p ${DATABASE}
        flock ${LOCK_FILE} concurrentDownload.sh --output=${DATABASE} \
         --link=!{DOWNLOAD_LINK} \
         --httpsCommand="wget -O kegg.tar.gz !{DOWNLOAD_LINK} && tar -xzvf kegg.tar.gz && rm kegg.tar.gz " \
         --s3DirectoryCommand="s5cmd !{S5CMD_PARAMS} cp --concurrency !{task.cpus} !{DOWNLOAD_LINK} . " \
         --s3FileCommand="s5cmd !{S5CMD_PARAMS} cp !{DOWNLOAD_LINK} kegg.tar.gz && tar -xzvf kegg.tar.gz && rm kegg.tar.gz " \
	 --s5cmdAdditionalParams="!{S5CMD_PARAMS}" \
         --localCommand="tar -xzvf !{DOWNLOAD_LINK} " \
         --expectedMD5SUM=!{MD5SUM}

         KEGG_DB="${DATABASE}/out/"
      else
         KEGG_DB="!{EXTRACTED_DB}"
      fi
      blast2kegg.py !{blast_result} ${KEGG_DB} !{sample}_!{binType}_keggPaths.tsv
      '''
}


/**
*
* Helper method to get the appropriate flag for the prodigal mode depending on the set prodigalMode 
* and the parameter set as params.steps.annotation.prokka.prodigalMode
* 
**/
def getProdigalModeString(prodigalMode) {
    if(prodigalMode == "param") {
        prodigalMode = params.steps.annotation.prokka.prodigalMode
    }
    if (prodigalMode == "single") {
        prodigalModeStr = ""
    } else if (prodigalMode == "meta") {
        prodigalModeStr = "--metagenome"
    } else {
        error "Invalid prodigal mode: ${prodigalMode}"
    }
        return prodigalModeStr
}

/**
*
* Helper workflow to create the input for prokka, set inputs in correct order and get taxonomy from gtdb results if available
* 
**/
workflow _wCreateProkkaInput {
    take:
        fasta
        gtdb
    main:
        DATASET_IDX = 0
        BIN_ID_IDX = 1
        PATH_IDX = 2
        if(gtdb){
            //get GTDB domains from the tsv file with Taxonomy for all bins:
            GTDB_FILE_IDX = 0
            DOMAIN_IDX = 0
            gtdb | filter(it -> file(it[GTDB_FILE_IDX]).text?.trim()) \
             | splitCsv(sep: '\t', header: true) \
             | map { it ->  def command = it[GTDB_FILE_IDX].classification.split(';')[DOMAIN_IDX].minus('d__'); [it[GTDB_FILE_IDX].SAMPLE, it[GTDB_FILE_IDX].BIN_ID, command] } \
             | set { gtdbDomain }
            //set domain for each bin
            DOMAIN_PROKKA_INPUT_IDX = 3
            fasta   |  map {it -> [it[DATASET_IDX], it[BIN_ID_IDX], file(it[PATH_IDX]) ]} \
             | join(gtdbDomain, by:[DATASET_IDX, BIN_ID_IDX], remainder: true) \
             | filter({ it -> it[PATH_IDX]!=null }) \
	     | map { it -> [ it[DATASET_IDX], it[BIN_ID_IDX], it[PATH_IDX], it[DOMAIN_PROKKA_INPUT_IDX]?:params?.steps?.annotation?.prokka?.defaultKingdom ] } \
             | set { prokkaInput }
        } else {
            //if no gtdb annotation available, default to the defaultKingdom in params
            fasta | map { it -> [it[DATASET_IDX], it[BIN_ID_IDX], it[PATH_IDX], params?.steps?.annotation?.prokka?.defaultKingdom]} | set { prokkaInput }
        }
        
    emit:
        prokkaInput
}


/**
*
* Prokka is a tool to annotate, bacterial, archael and viral genomes.
* Input is a mode for the gene prediction tool prodigal included in prokka, which can be:
* - "single" for larger contigs 
* - "meta" for smaller contigs / metagenomes
* - "param" sets prodigalMode to whatever is defined as params.steps.annotation.prokka.prodigalMode
* Other input is a tuple consisting of sample, binID, path to fasta file and the domain (for gene prediction)
* locusTag setzen? Eindeutiger Tag den wir ersetzen koennen
* 
**/
process pProkka {

    container "${params.prokka_image}"

    label 'small'

    tag "Sample: $sample, BinID: $binID"

    publishDir params.output, mode: "${params.publishDirMode}", saveAs: { filename -> getOutput("${sample}",params.runid ,"prokka", filename) }

    when params.steps.containsKey("annotation") && params.steps.annotation.containsKey("prokka")

    input:
      val(prodigalMode)
      tuple val(sample), val(binID), file(fasta), val(domain), file(defaultCoverage), file(metabatCoverage)

    output:
      tuple val("${sample}"), val("${binID}"), file("*.gff.gz"), emit: gff 
      tuple val("${sample}"), val("${binID}"), file("*.faa.gz"), emit: faa 
      tuple val("${sample}"), val("${binID}"), file("*.fna.gz"), emit: fna 
      tuple val("${sample}"), val("${binID}"), file("*.ffn.gz"), emit: ffn 
      tuple val("${sample}"), val("${binID}"), file("*.fsa.gz"), emit: fsa 
      tuple val("${sample}"), val("${binID}"), file("*.gbk.gz"), optional: true, emit: gbk 
      tuple val("${sample}"), val("${binID}"), file("*.tbl.gz"), emit: tbl 
      tuple val("${sample}"), val("${binID}"), file("*.sqn.gz"), optional: true, emit: sqn
      tuple val("${sample}"), val("${binID}"), file("*.txt"), emit: txt 
      tuple val("${sample}"), val("${binID}"), file("*_prokka.tsv"), emit: tsv 
      tuple val("${binID}"), val("${output}"), val(params.LOG_LEVELS.INFO), file(".command.sh"), \
        file(".command.out"), file(".command.err"), file(".command.log"), emit: logs

    shell:
      output = getOutput("${sample}", params.runid, "prokka", "")
      prodigalModeStr = getProdigalModeString(prodigalMode)
      prokkaDomain = domain ? " --kingdom " + domain : ""
      template "prokka.sh"
}


/**
*
* This entry point uses a file to grab all fasta files in the referenced directories for annotation.
* You need to call (and fill out) the aws credential file with -c to use this module!
* The .tsv file has to have: DATASET, BIN_ID and PATH entries.
* 
* The "database_mode" is used to choose which database path is expected.
* If the databasepath starts with "https://" or "s3://" the object storage based mode is used.
* All other paths are seen as "local" mode paths and offline stored copys are expected.
*
**/
workflow wAnnotateFile {

   take:
      projectTableFile
   main:
      annotationTmpDir = params.tempdir + "/annotation"
      file(annotationTmpDir).mkdirs()
      projectTableFile | splitCsv(sep: '\t', header: true) \
      | map{ [it.DATASET, it.BIN_ID, file(it.PATH)] } | set { input } 

      f1 = file(params.tempdir + "/empty1")
      f1.text = ""

      f2 = file(params.tempdir + "/empty2")
      f2.text = ""

      input | map { bin -> bin[0]} | unique \
	| combine(Channel.value([f1,f2])) | set { coverage }

      DATASET_IDX = 0
      wSaveSettingsList(input | map { it -> it[DATASET_IDX] })

      input |  map { sample, bin, path -> [sample, bin] } |  groupTuple(by: DATASET_IDX) \
	|  map { sample, bins -> [sample, bins.size()] } | set { fastaCounter }

      _wAnnotation(Channel.value("out"), Channel.value("param"), input, null, coverage, fastaCounter)
   emit:
      keggAnnotation = _wAnnotation.out.keggAnnotation
}

/**
*
* See wAnnotateFile for a description.
*
**/
workflow wAnnotateList {
   take:
      sourceChannel
      prodigalMode
      fasta
      gtdb
      contigCoverage
      fastaCounter
   main:
      annotationTmpDir = params.tempdir + "/annotation"
      file(annotationTmpDir).mkdirs()
      _wAnnotation(sourceChannel, prodigalMode, fasta, gtdb, contigCoverage, fastaCounter)
    emit:
      keggAnnotation = _wAnnotation.out.keggAnnotation
      proteins = _wAnnotation.out.prokka_faa
}


/**
*
* The main annotation workflow. 
* It is build to handle one big input fasta file.
* Based on this file genes will be predicted and annotated using Prokka, these genes will be blasted against KEGG.
* Gtdb results are optional to set the domain for annotation with Prokka.
* At the end kegg- and prokka-infos of the results will be collected and presented.
*
**/ 
workflow _wAnnotation {
   take:
      sourceChannel
      prodigalMode
      fasta
      gtdb
      contigCoverage
      fastaCounter
   main:
      SAMPLE_IDX=0

      // Format input for prokka and run prokka:
      _wCreateProkkaInput(fasta, gtdb)

      pProkka(prodigalMode, _wCreateProkkaInput.out.prokkaInput | combine(contigCoverage, by: SAMPLE_IDX))
      
      // Collect all databases
      selectedDBs = params?.steps?.annotation?.mmseqs2.findAll().collect({ 
            [it.key, it.value?.params ?: "", \
	         it.value?.database?.extractedDBPath ?: "", \
             it.value.database?.download?.source ?: "", \
             it.value.database?.download?.md5sum ?: "", \
             it.value.database?.download?.s5cmd?.params ?: "" ]
      })

      selectedTaxDBs = params?.steps?.annotation?.mmseqs2_taxonomy.findAll({  it.key != "runOnMAGs"  }).collect({
            [it.key, it.value?.params ?: "", \
             it.value?.ramMode ? "true" : "false", \
             it.value?.database?.extractedDBPath ?: "", \
             it.value.database?.download?.source ?: "", \
             it.value.database?.download?.md5sum ?: "", \
             it.value.database?.download?.s5cmd?.params ?: "" ]
      })

      PATH_IDX=2

      // The following groupTuple operators need a counter for the expected number of bins or fasta files of a sample.
      // This allows groupTuple to not block until all samples are processed.

      // Create contig to gene mapping
      pProkka.out.tsv | map { [it[SAMPLE_IDX], it[PATH_IDX]] } | combine(fastaCounter, by:SAMPLE_IDX) \
	|  map { sample, path, size -> tuple( groupKey(sample, size), path ) } | groupTuple() | set { contig2GeneMapping }

      // Run all amino acid outputs against all databases
      // Collect by sample name to bundle searches and avoid calls with small input files
      pProkka.out.faa | map { [it[SAMPLE_IDX], it[PATH_IDX]] } | combine(fastaCounter, by:SAMPLE_IDX) \
	| map { sample, path, size -> tuple( groupKey(sample, size), path ) }  | groupTuple() | set { groupedProkkaFaa }

      groupedProkkaFaa | combine(contig2GeneMapping, by: SAMPLE_IDX) | combine(Channel.from(selectedDBs)) | set { combinedMMseqs }

      pMMseqs2(sourceChannel, combinedMMseqs)
      combinedMMseqsTax = groupedProkkaFaa | combine(Channel.from(selectedTaxDBs))
      pMMseqs2_taxonomy(sourceChannel, combinedMMseqsTax)
      DB_TYPE_IDX = 0
      pMMseqs2.out.blast | filter({ result -> result[DB_TYPE_IDX] == "kegg" }) \
	| map({ result -> result.remove(0); result }) \
	| set { mmseqs2Results } 

      // Run Resistance Gene Identifier with amino acid outputs
      pProkka.out.faa | pResistanceGeneIdentifier
      pKEGGFromBlast(mmseqs2Results)

      pProkka.out.logs | mix(pMMseqs2.out.logs) \
        | mix(pMMseqs2_taxonomy.out.logs) \
	| mix(pResistanceGeneIdentifier.out.logs) \
	| mix(pKEGGFromBlast.out.logs) | pDumpLogs
   emit:
      keggAnnotation = pKEGGFromBlast.out.kegg_paths
      mmseqs2_kronaHtml = pMMseqs2_taxonomy.out.kronaHtml
      mmseqs2_krakenTaxonomy = pMMseqs2_taxonomy.out.krakenStyleTaxonomy
      mmseqs2_taxonomy = pMMseqs2_taxonomy.out.taxonomy
      mmseqs2_blast = pMMseqs2.out.blast
      prokka_faa = pProkka.out.faa
      prokka_ffn = pProkka.out.ffn
      prokka_fna = pProkka.out.fna
      prokka_fsa = pProkka.out.fsa
      prokka_gbk = pProkka.out.gbk
      prokka_gff = pProkka.out.gff
      prokka_sqn = pProkka.out.sqn
      prokka_tbl = pProkka.out.tbl
      prokka_tsv = pProkka.out.tsv
      prokka_txt = pProkka.out.txt 
}
