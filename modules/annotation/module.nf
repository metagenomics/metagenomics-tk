include { wSaveSettingsList } from '../config/module'

include { pDumpLogs } from '../utils/processes'
include { pCollectFile } from './processes'
include { _wAnalyseEukaryotes } from './eukaryotes'



import java.nio.file.Files
import java.nio.file.Paths
import java.util.stream.Collectors

mode = 'mode_not_set'

// Function to generate the right output path for each tool/process to be feed into the publishDir call of said processes.
def getOutput(SAMPLE, RUNID, TOOL, filename){
    return SAMPLE + '/' + RUNID + '/' + params.modules.annotation.name + '/' + 
          params.modules.annotation.version.major + "." + 
          params.modules.annotation.version.minor + "." + 
          params.modules.annotation.version.patch +
          '/' + TOOL + '/' + filename
}


/*
*
* This function decides if the MMSeqs taxonomy process should be executed on MAGs.
*
*/
def runMMSeqsTaxonomy(isMMSeqsTaxonomySettingSet, isBinned, isPlasmid, runOnBinned) {
    // True if the MMSeqsTaxonomy setting is set and plasmids are not used as input and 
    // either the sample is not binned 
    // or should explicitly run on binned samples 
    return isMMSeqsTaxonomySettingSet && !isPlasmid && (!isBinned || runOnBinned)
}


/**
*
* MMseqs2 is used to search for big input queries in large databases. 
* Multiple databases can be searched at the same time. Parts of the query and search database are loaded into the RAM.
# Huge databases along with to little RAM could lead to errors.
* Outputs will be saved in separate directories.
*
**/
process pMMseqs2 {
   
      container "${params.mmseqs2_image}"

      // Databases will be downloaded to a fixed place so that they can be used by future processes.
      // This fixed place has to be outside of the working-directory to be easy to find for every process.
      // Therefore this place has to be mounted to the docker container to be accessible during run time.
      // Another mount flag is used to get a key file (aws format) into the docker-container. 

      // This file is then used by s5cmd. The necessary mount points are generated by constructParametersObject(). 
      containerOptions Utils.constructParametersObject("mmseqs2", params, params.apptainer) + (params.apptainer ? "" : " --entrypoint='' " +  Utils.getDockerNetwork())

      tag "Sample: $sample, Database: $dbType"

      label 'large'

      secret { "${S3_DB_ACCESS}"!="" ? ["S3_${dbType}_ACCESS", "S3_${dbType}_SECRET"] : [] } 

      when params?.steps.containsKey("annotation") && params?.steps.annotation.containsKey("mmseqs2")

   input:
      val(binType)
      tuple val(sample), file(fasta), file(contig2GeneMapping), val(start), val(stop), val(numberOfChunks), val(dbType), val(parameters), val(columns), val(EXTRACTED_DB), val(DOWNLOAD_LINK), val(MD5SUM), val(S5CMD_PARAMS)

   output:
      tuple val("${dbType}"), val("${sample}"), val("${binType}"), val("${start}"), val("${stop}"), val("${numberOfChunks}"),\
	path("${sample}_${binType}.${start}.${stop}.${dbType}.blast.tsv"), optional:true, emit: blast
      tuple val("${sample}_${binType}"), val("${output}"), val(params.LOG_LEVELS.INFO), file(".command.sh"), \
        file(".command.out"), file(".command.err"), file(".command.log"), emit: logs

   shell:
   output = getOutput("${sample}", params.runid, "mmseqs2/${dbType}", "")
   S3_DB_ACCESS=params.steps?.annotation?.mmseqs2?."${dbType}"?.database?.download?.s5cmd && S5CMD_PARAMS.indexOf("--no-sign-request") == -1 ? "\$S3_${dbType}_ACCESS" : ""
   S3_DB_SECRET=params.steps?.annotation?.mmseqs2?."${dbType}"?.database?.download?.s5cmd && S5CMD_PARAMS.indexOf("--no-sign-request") == -1 ? "\$S3_${dbType}_SECRET" : ""
   ADDITIONAL_COLUMNS = "${columns}".trim() == "" ? "" : ",${columns}"
   template("mmseqs2.sh")
}

/**
*
* The MMseqs2 module taxonomy calls an internal module lca that implements an lowest common ancestor assignment for sequences by querying them against a seqTaxDB.
* Multiple databases can be searched at the same time. Parts of the query and search database are loaded into the RAM.
* Huge databases along with to little RAM could lead to errors.
* Outputs will be saved in separate directories.
*
**/
process pMMseqs2_taxonomy {
   
      container "${params.mmseqs2_image}"

      // Databases will be downloaded to a fixed place so that they can be used by future processes.
      // This fixed place has to be outside of the working-directory to be easy to find for every process.
      // Therefore this place has to be mounted to the docker container to be accessible during run time.
      // Another mount flag is used to get a key file (aws format) into the docker-container. 
      // This file is then used by s5cmd. The necessary mount points are generated by “constructParametersObject()”. 
      containerOptions Utils.constructParametersObject("mmseqs2_taxonomy", params, params.apptainer) + (params.apptainer ? "" : " --entrypoint='' " + Utils.getDockerNetwork())

      tag "Sample: $sample, Database_taxonomy: $dbType"

      label 'highmemLarge'

      secret { "${S3_TAX_DB_ACCESS}"!="" ? ["S3_TAX_${dbType}_ACCESS", "S3_TAX_${dbType}_SECRET"] : [] } 

      publishDir params.output, mode: "${params.publishDirMode}", saveAs: { filename -> getOutput("${sample}", params.runid, "mmseqs2_taxonomy/${dbType}", filename) }, \
         pattern: "{*.out,*.html,*.tsv}"
 
      when:
      runMMSeqsTaxonomy(params?.steps.containsKey("annotation") && params?.steps.annotation.containsKey("mmseqs2_taxonomy"), \
	   binType.equals("binned"), \
	   binType.equals("plasmid"), \
           params?.steps.containsKey("annotation") && params?.steps.annotation.containsKey("mmseqs2_taxonomy") && params?.steps.annotation.mmseqs2_taxonomy.runOnMAGs)

   input:
      val(binType)
      tuple val(sample), file(fasta), val(dbType), val(parameters), val(ramMode), val(initialMaxSensitivity), val(EXTRACTED_DB), val(DOWNLOAD_LINK), val(MD5SUM), val(S5CMD_PARAMS)
   
   output:
      tuple val("${sample}"), val("${dbType}"), path("${sample}_${binType}.${dbType}.taxonomy.tsv"), optional:true, emit: taxonomy
      tuple val("${sample}"), val("${dbType}"), path("${sample}_${binType}.${dbType}.krakenStyleTaxonomy.out"), optional:true, emit: krakenStyleTaxonomy
      tuple val("${sample}"), val("${dbType}"), path("${sample}_${binType}.${dbType}.krona.html"), optional:true, emit: kronaHtml
      tuple val("${sample}_${binType}"), val("${output}"), val(params.LOG_LEVELS.INFO), file(".command.sh"), \
        file(".command.out"), file(".command.err"), file(".command.log"), emit: logs


   shell:
   output = getOutput("${sample}", params.runid, "mmseqs2_taxonomy/${dbType}", "")
   // The maximum possible sensitivity is reduced each time the process is retried.
   // The reason for this behaviour is a bug that occurs at higher sensitivity levels.
   S3_TAX_DB_ACCESS=params.steps?.annotation?.mmseqs2_taxonomy?."${dbType}"?.database?.download?.s5cmd && S5CMD_PARAMS.indexOf("--no-sign-request") == -1 ? "\$S3_TAX_${dbType}_ACCESS" : ""
   S3_TAX_DB_SECRET=params.steps?.annotation?.mmseqs2_taxonomy?."${dbType}"?.database?.download?.s5cmd && S5CMD_PARAMS.indexOf("--no-sign-request") == -1 ? "\$S3_TAX_${dbType}_SECRET" : ""
   sensitivityRaw = initialMaxSensitivity - task.attempt + 1
   sensitivity = sensitivityRaw < 1 ? 1 : sensitivityRaw
   template("mmseqs2Taxonomy.sh")
}


process pResistanceGeneIdentifier {
   
   container "${params.rgi_image}"
      
   containerOptions Utils.getDockerMount(params?.steps?.annotation?.rgi?.database, params, apptainer=params.apptainer) + (params.apptainer ? "" : Utils.getDockerNetwork())
 
   tag "Sample: $sample, BinID: $binID"

   label 'small'

   publishDir params.output, mode: "${params.publishDirMode}", saveAs: { filename -> getOutput("${sample}", params.runid, "rgi", filename) }, \
         pattern: "{**.rgi.tsv,**.png,**.eps,**.csv}"

   when params.steps.containsKey("annotation") && params?.steps.annotation.containsKey("rgi")

   secret { "${S3_rgi_ACCESS}"!="" ? ["S3_rgi_ACCESS", "S3_rgi_SECRET"] : [] } 

   input:
      tuple val(sample), val(binID), file(fasta)
   
   output:
      tuple val("${sample}"), val("${binID}"), path("${binID}.rgi.tsv"), optional:true, emit: results
      tuple val("${sample}"), val("${binID}"), path("${binID}.rgi-1.csv"), optional:true, emit: resultsGenes
      tuple val("${sample}"), val("${binID}"), path("${binID}.rgi-1.eps"), \
	path("${sample}_${binID}.rgi-1.png"), optional:true, emit: png
      tuple val("${binID}"), val("${output}"), val(params.LOG_LEVELS.INFO), file(".command.sh"), \
        file(".command.out"), file(".command.err"), file(".command.log"), emit: logs

   shell:
   output = getOutput("${sample}", params.runid, "rgi", "")
   EXTRACTED_DB=params.steps?.annotation?.rgi?.database?.extractedDBPath ?: ""
   DOWNLOAD_LINK=params.steps?.annotation?.rgi?.database?.download?.source ?: ""
   MD5SUM=params?.steps?.annotation?.rgi?.database?.download?.md5sum ?: ""
   S5CMD_PARAMS=params.steps?.annotation?.rgi?.database?.download?.s5cmd?.params ?: ""
   S3_rgi_ACCESS=params.steps?.annotation?.rgi?.database?.download?.s5cmd && S5CMD_PARAMS.indexOf("--no-sign-request") == -1 ? "\$S3_rgi_ACCESS" : ""
   S3_rgi_SECRET=params.steps?.annotation?.rgi?.database?.download?.s5cmd && S5CMD_PARAMS.indexOf("--no-sign-request") == -1 ? "\$S3_rgi_SECRET" : ""
   template("rgi.sh")
}

/**
 * Prodigal - Protein-coding gene prediction for prokaryotic genomes
 * @param sample: Sample name
 * @param contigs: File containing all contigs in a .gz format
 * @return: Files containing all predicted proteins and genes in .faa and .ffn format
 *
 * Mainly used as input for hmmSearch/MagScot as the real annotation is done with Prokka.
 **/
process pProdigal {
      // Re-Use the gtdb-tk container for Prodigal to safe space, as it is ancient
      container "${params.gtdbtk_image}"

      tag "Sample: $sample"

      label 'small'

      publishDir params.output, mode: "${params.publishDirMode}", saveAs: { filename -> getOutput("${sample}", params.runid, "prodigal", filename) }

      when params.steps.containsKey("binning") && params?.steps.binning.containsKey("magscot")

   input:
      tuple val(sample), path(contigs)

   output:
      tuple val("${sample}"), file("${sample}.prodigal.faa"), emit: prodigal_faa
      tuple val("${sample}"), file("${sample}.prodigal.ffn"), emit: prodigal_ffn
      tuple file(".command.sh"), file(".command.out"), file(".command.err"), file(".command.log")

   shell:
   '''
   zcat !{contigs} | prodigal !{params.steps?.binning?.magscot?.prodigal?.additionalParams} -a !{sample}.prodigal.faa -d !{sample}.prodigal.ffn -o tmpfile
   '''
}


/**
 * HMMSearch - Search for protein domains in protein sequences
 * @param sample: Sample name
 * @param faaFile: File containing all predicted proteins in .faa format
 * @return: Files containing all annotated proteins, in all- and top-hits files,
 * as well as a cut down version of the top-hits file for MagScot.
 *
 * Mainly used as input for MagScot.
 **/
process pHmmSearch {

      // Re-Use the gtdb-tk container for Prodigal to safe space
      container "${params.gtdbtk_image}"

      containerOptions Utils.getDockerMount(params?.steps?.binning?.magscot?.hmmSearch?.database, params, apptainer=params.apptainer) + (params.apptainer ? "" : Utils.getDockerNetwork()) 

      tag "Sample: $sample"

      label 'highmemMedium'

      secret { "${S3_gtdb_ACCESS}"!="" ? ["S3_gtdb_ACCESS", "S3_gtdb_SECRET"] : [] } 

      publishDir params.output, mode: "${params.publishDirMode}", saveAs: { filename -> getOutput("${sample}", params.runid, "hmmSearch", filename) }

      when params.steps.containsKey("binning") && params?.steps.binning.containsKey("magscot")

   input:
      tuple val(sample), file(faaFile)

   output:
      tuple val("${sample}"), file("${sample}.hmm.tigr.hit.tsv"), optional:true, emit: tigr_hits
      tuple val("${sample}"), file("${sample}.hmm.pfam.hit.tsv"), optional:true, emit: pfam_hits
      tuple val("${sample}"), file("${sample}.hmm.tigr.out"), optional:true, emit: tigr_out
      tuple val("${sample}"), file("${sample}.hmm.pfam.out"), optional:true, emit: pfam_out
      tuple val("${sample}"), file("${sample}.hmm.allhits.tsv"), optional:true, emit: allhits
      tuple file(".command.sh"), file(".command.out"), file(".command.err"), file(".command.log")


   shell:
   output = getOutput("${sample}", params.runid, "hmmSearch", "")
   EXTRACTED_DB=params.steps?.binning?.magscot?.hmmSearch?.database?.extractedDBPath ?: ""
   DOWNLOAD_LINK=params.steps?.binning?.magscot?.hmmSearch?.database?.download?.source ?: ""
   MD5SUM=params?.steps?.binning?.magscot?.hmmSearch?.database?.download?.md5sum ?: ""
   S5CMD_PARAMS=params.steps?.binning?.magscot?.hmmSearch?.database?.download?.s5cmd?.params ?: ""
   S3_gtdb_ACCESS=params.steps?.binning?.magscot?.hmmSearch?.database?.download?.s5cmd && S5CMD_PARAMS.indexOf("--no-sign-request") == -1 ? "\$S3_gtdb_ACCESS" : ""
   S3_gtdb_SECRET=params.steps?.binning?.magscot?.hmmSearch?.database?.download?.s5cmd && S5CMD_PARAMS.indexOf("--no-sign-request") == -1 ? "\$S3_gtdb_SECRET" : ""
   '''
   ADDITIONAL_HMMSEARCH_PARAMS="!{params.steps?.binning?.magscot?.hmmSearch?.additionalParams}"

   gtdb_download.sh "!{EXTRACTED_DB}" "!{DOWNLOAD_LINK}" "!{S5CMD_PARAMS}" "!{task.cpus}" "!{params.polished.databases}" "!{MD5SUM}" "!{S3_gtdb_ACCESS}" "!{S3_gtdb_SECRET}" || exit 1 

   GTDB=$(cat gtdbPath.txt)

   # Run hmmsearch
   hmmsearch --cpu !{task.cpus} ${ADDITIONAL_HMMSEARCH_PARAMS} -o !{sample}.hmm.tigr.out --tblout !{sample}.hmm.tigr.hit.tsv ${GTDB}/markers/tigrfam/tigrfam.hmm !{faaFile}
   hmmsearch --cpu !{task.cpus} ${ADDITIONAL_HMMSEARCH_PARAMS} -o !{sample}.hmm.pfam.out --tblout !{sample}.hmm.pfam.hit.tsv ${GTDB}/markers/pfam/Pfam-A.hmm !{faaFile}

   # Remove header and create all-hits file
   cat !{sample}.hmm.tigr.hit.tsv | grep -v "^#" | awk '{print $1"\t"$3"\t"$5}' > !{sample}.tigr
   cat !{sample}.hmm.pfam.hit.tsv | grep -v "^#" | awk '{print $1"\t"$4"\t"$5}' > !{sample}.pfam
   cat !{sample}.tigr !{sample}.pfam > !{sample}.hmm.allhits.tsv
   '''


}

/**
*
* pKEGGFromMMseqs2 is build to handle results in the outfmt 6 file standard.
* These search results will be compared to kegg link-files to produce a file where all available kegg information
* for these search results is collected in a centralized way/file.
* You need to call (and fill out) the aws credential file with -c to use this module as kegg is commercial an the database has to be placed in your project s3!
*
**/
process pKEGGFromMMseqs2 {

      tag "$sample"

      label 'small'

      container "${params.python_env_image}"

      // Databases will be downloaded to a fixed place so that they can be used by future processes.
      // These fixed place has to be outside of the working-directory to be easy to find for every process.
      // Therefore this place has to be mounted to the docker container to be accessible during runtime.
      // Another mount flag is used to get a key file (aws format) into the docker-container. 
      // This file is then used by s5cmd. 

      containerOptions Utils.getDockerMount(params.steps?.annotation?.keggFromMMseqs2?.database, params, apptainer=params.apptainer) + (params.apptainer ? "" : Utils.getDockerNetwork())

      publishDir params.output, mode: "${params.publishDirMode}", saveAs: { filename -> getOutput("${sample}", params.runid, "keggFromMMseqs2", filename) }, \
         pattern: "{**.tsv}"

      when params?.steps.containsKey("annotation") && params?.steps.annotation.containsKey("keggFromMMseqs2")

   secret { "${S3_KEGG_ACCESS}"!="" ? ["S3_kegg_ACCESS", "S3_kegg_SECRET"] : [] } 

   input:
      tuple val(sample), val(binType), file(blastResult)

   output:
      tuple val("${sample}"), path("*.keggPaths.tsv"), emit: keggPaths
      tuple val("${sample}_${binType}"), val("${output}"), val(params.LOG_LEVELS.INFO), file(".command.sh"), \
        file(".command.out"), file(".command.err"), file(".command.log"), emit: logs

   shell:
      output = getOutput("${sample}", params.runid, "keggFromMMseqs2", "")
      DOWNLOAD_LINK=params.steps?.annotation?.keggFromMMseqs2?.database?.download?.source ?: ""
      MD5SUM=params?.steps?.annotation?.keggFromMMseqs2?.database?.download?.md5sum ?: ""
      S5CMD_PARAMS=params.steps?.annotation?.keggFromMMseqs2?.database?.download?.s5cmd?.params ?: ""
      EXTRACTED_DB=params.steps?.annotation?.keggFromMMseqs2?.database?.extractedDBPath ?: ""
      S3_KEGG_ACCESS=params.steps?.annotation?.keggFromMMseqs2?.database?.download?.s5cmd && S5CMD_PARAMS.indexOf("--no-sign-request") == -1 ? "\$S3_kegg_ACCESS" : ""
      S3_KEGG_SECRET=params.steps?.annotation?.keggFromMMseqs2?.database?.download?.s5cmd && S5CMD_PARAMS.indexOf("--no-sign-request") == -1 ? "\$S3_kegg_SECRET" : ""
      '''
      # Check developer documentation
      KEGG_DB=""
      if [[ -z "!{EXTRACTED_DB}" ]] 
      then
        DATABASE=!{params.polished.databases}/kegg
        LOCK_FILE=${DATABASE}/lock.txt

	# Check if access and secret keys are necessary for s5cmd
        if [ ! -z "!{S3_KEGG_ACCESS}" ]
        then
          export AWS_ACCESS_KEY_ID=!{S3_KEGG_ACCESS}
          export AWS_SECRET_ACCESS_KEY=!{S3_KEGG_SECRET}
        fi

        # Download KEGG database
        mkdir -p ${DATABASE}
        flock ${LOCK_FILE} concurrentDownload.sh --output=${DATABASE} \
         --link=!{DOWNLOAD_LINK} \
         --httpsCommand="wgetStatic --no-check-certificate -qO- !{DOWNLOAD_LINK} | tar -xz " \
         --s3DirectoryCommand="s5cmd !{S5CMD_PARAMS} cp --concurrency !{task.cpus} !{DOWNLOAD_LINK} . " \
         --s3FileCommand="s5cmd !{S5CMD_PARAMS} cat !{DOWNLOAD_LINK} | tar -xz " \
	 --s5cmdAdditionalParams="!{S5CMD_PARAMS}" \
         --localCommand="tar -xzvf !{DOWNLOAD_LINK} " \
         --expectedMD5SUM=!{MD5SUM}

         KEGG_DB="${DATABASE}/out/"
      else
         KEGG_DB="!{EXTRACTED_DB}"
      fi
      blast2kegg.py !{blastResult} ${KEGG_DB} !{blastResult.baseName}.keggPaths.tsv
      '''
}


/**
*
* Helper method to get the appropriate flag for the prodigal mode depending on the set prodigalMode 
* and the parameter set as params.steps.annotation.prokka.prodigalMode
* 
**/
def getProdigalModeString(prodigalMode) {
    if(prodigalMode == "param") {
        prodigalMode = params.steps.annotation.prokka.prodigalMode
    }
    if (prodigalMode == "single") {
        prodigalModeStr = ""
    } else if (prodigalMode == "meta") {
        prodigalModeStr = "--metagenome"
    } else {
        error "Invalid prodigal mode: ${prodigalMode}"
    }
        return prodigalModeStr
}


workflow _wCreateProkkaInput {
  take:
    fasta
  main:
    DATASET_IDX = 0
    BIN_ID_IDX = 1
    PATH_IDX = 2
    DOMAIN_PROKKA_INPUT_IDX = 3
    fasta | map { it -> [ it[DATASET_IDX], it[BIN_ID_IDX], it[PATH_IDX], it[DOMAIN_PROKKA_INPUT_IDX]?:params?.steps?.annotation?.prokka?.defaultKingdom ] } \
	| set { prokkaInput }
  emit:
    prokkaInput
}

/**
*
* Helper workflow to create the input for prokka, set inputs in correct order and get taxonomy from gtdb results if available
* 
**/
workflow _wCreateProkkaGtdbInput {
    take:
        fasta
        gtdb
        gtdbMissing
    main:
        DATASET_IDX = 0
        BIN_ID_IDX = 1
        PATH_IDX = 2

        //get GTDB domains from the tsv file with Taxonomy for all bins:
        DOMAIN_IDX = 0
        gtdb | map { it ->  def command = it.classification.split(';')[DOMAIN_IDX].minus('d__'); [it.SAMPLE, it.BIN_ID, command] } \
             | set { gtdbDomain }

        //set domain for each bin
        DOMAIN_PROKKA_INPUT_IDX = 3
        fasta | map {it -> [it[DATASET_IDX], it[BIN_ID_IDX], file(it[PATH_IDX]) ]} \
           | join(gtdbDomain | mix(gtdbMissing | map { bin -> [bin.SAMPLE, bin.BIN_ID] }), by:[DATASET_IDX, BIN_ID_IDX], remainder: true) \
           | filter({ it -> it[PATH_IDX]!=null }) \
	   | map { it -> [ it[DATASET_IDX], it[BIN_ID_IDX], it[PATH_IDX], it[DOMAIN_PROKKA_INPUT_IDX]?:params?.steps?.annotation?.prokka?.defaultKingdom ] } \
           | set { prokkaInput }

    emit:
        prokkaInput
}


/**
*
* Prokka is a tool to annotate, bacterial, archael and viral genomes.
* Input is a mode for the gene prediction tool prodigal included in prokka, which can be:
* - "single" for larger contigs 
* - "meta" for smaller contigs / metagenomes
* - "param" sets prodigalMode to whatever is defined as params.steps.annotation.prokka.prodigalMode
* Other input is a tuple consisting of sample, binID, path to fasta file and the domain (for gene prediction)
* locusTag setzen? Eindeutiger Tag den wir ersetzen koennen
* 
**/
process pProkka {

    container "${params.prokka_image}"

    label 'small'

    tag "Sample: $sample, BinID: $binID"

    publishDir params.output, mode: "${params.publishDirMode}", saveAs: { filename -> getOutput("${sample}",params.runid ,"prokka", filename) }

    when params.steps.containsKey("annotation") && params.steps.annotation.containsKey("prokka")

    input:
      val(prodigalMode)
      tuple val(sample), val(binID), file(fasta), val(domain), file(defaultCoverage), file(metabatCoverage)

    output:
      tuple val("${sample}"), val("${binID}"), file("*.prokka.gff.gz"), emit: gff 
      tuple val("${sample}"), val("${binID}"), file("*.prodigal.gff.gz"), emit: prodigalGff 
      tuple val("${sample}"), val("${binID}"), file("*.faa.gz"), emit: faa 
      tuple val("${sample}"), val("${binID}"), file("*.fna.gz"), emit: fna 
      tuple val("${sample}"), val("${binID}"), file("*.ffn.gz"), emit: ffn 
      tuple val("${sample}"), val("${binID}"), file("*.fsa.gz"), emit: fsa 
      tuple val("${sample}"), val("${binID}"), file("*.gbk.gz"), optional: true, emit: gbk 
      tuple val("${sample}"), val("${binID}"), file("*.tbl.gz"), emit: tbl 
      tuple val("${sample}"), val("${binID}"), file("*.sqn.gz"), optional: true, emit: sqn
      tuple val("${sample}"), val("${binID}"), file("*.txt"), emit: txt 
      tuple val("${sample}"), val("${binID}"), file("*_prokka.tsv"), emit: tsv 
      tuple val("${binID}"), val("${output}"), val(params.LOG_LEVELS.INFO), file(".command.sh"), \
        file(".command.out"), file(".command.err"), file(".command.log"), emit: logs

    shell:
      output = getOutput("${sample}", params.runid, "prokka", "")
      prodigalModeStr = getProdigalModeString(prodigalMode)
      prokkaDomain = domain ? " --kingdom " + domain : ""
      template "prokka.sh"
}


/**
* Whokaryote processes contigs and Prodigals gene annotation to predict if the organism is eukaryotic or prokaryotic.
* The output lists which contig belongs to which kingdom.
**/
process pWhokaryote {

    container "${params.whokaryote_image}"

    label 'small'

    memory { Utils.getMemoryResources(params.resources.small, "${sample}", task.attempt, params.resources) }

    cpus { Utils.getCPUsResources(params.resources.small, "${sample}", task.attempt, params.resources) }

    tag "Sample: $sample, BinID: $binID"

    publishDir params.output, mode: "${params.publishDirMode}", saveAs: { filename -> getOutput("${sample}", params.runid , "whokaryote", filename) }

    when params.steps.containsKey("annotation") && params.steps.annotation.containsKey("whokaryote")

    input:
      tuple val(sample), val(binID), file(fasta), file(gff)

    output:
      tuple val("${sample}"), val("${binID}"), file("${binID}_featuretable.tsv"), emit: features 
      tuple val("${sample}"), val("${binID}"), file("${binID}_featuretable_predictions_T.tsv"), emit: predictions 
      tuple val("${sample}"), val("${binID}"), env("IS_EUKARYOTE"), file("${binID}_eukaryote_contig_headers.tsv"), emit: eukaryotes 
      tuple val("${sample}"), val("${binID}"), file("${binID}_prokaryote_contig_headers.tsv"), emit: prokaryotes 
      tuple val("${sample}"), val("${binID}"), file("${binID}_tiara_pred.tsv"), optional: true, emit: tiaraPred 
      tuple val("${binID}"), val("${output}"), val(params.LOG_LEVELS.INFO), file(".command.sh"), \
        file(".command.out"), file(".command.err"), file(".command.log"), emit: logs

    shell:
      output = getOutput("${sample}", params.runid, "whokaryote", "")
      template "whokaryote.sh"
}

/**
*
* This entry point uses a file to grab all fasta files in the referenced directories for annotation.
* You need to call (and fill out) the aws credential file with -c to use this module!
* The .tsv file has to have: SAMPLE, BIN_ID and PATH entries.
* 
* The "database_mode" is used to choose which database path is expected.
* If the databasepath starts with "https://" or "s3://" the object storage based mode is used.
* All other paths are seen as "local" mode paths and offline stored copys are expected.
*
**/
workflow wAnnotateFile {

   take:
      projectTableFile
   main:
      annotationTmpDir = params.tempdir + "/annotation"
      file(annotationTmpDir).mkdirs()
      projectTableFile | splitCsv(sep: '\t', header: true) \
      | map{ [it.SAMPLE, it.BIN_ID, file(it.PATH)] } | set { input } 

      f1 = file(params.tempdir + "/empty1")
      f1.text = ""

      f2 = file(params.tempdir + "/empty2")
      f2.text = ""

      input | map { bin -> bin[0]} | unique \
	| combine(Channel.value([f1,f2])) | set { coverage }

      SAMPLE_IDX = 0
      wSaveSettingsList(input | map { it -> it[SAMPLE_IDX] })

      input |  map { sample, bin, path -> [sample, bin] } |  groupTuple(by: SAMPLE_IDX) \
	|  map { sample, bins -> [sample, bins.size()] } | set { fastaCounter }

      _wAnnotation(Channel.value("out"), Channel.value("param"), input | _wCreateProkkaInput, coverage, fastaCounter)
   emit:
      keggAnnotation = _wAnnotation.out.keggAnnotation
}

/**
*
* See wAnnotateFile for a description.
*
**/
workflow wAnnotateList {
   take:
      sourceChannel
      prodigalMode
      fasta
      contigCoverage
      binContigMapping
      fastaCounter
   main:
      annotationTmpDir = params.tempdir + "/annotation"
      file(annotationTmpDir).mkdirs()
      _wAnnotation(sourceChannel, prodigalMode, fasta, contigCoverage, binContigMapping, fastaCounter)
    emit:
      keggAnnotation = _wAnnotation.out.keggAnnotation
      proteins = _wAnnotation.out.prokka_faa
      ffn = _wAnnotation.out.prokka_ffn
      gff = _wAnnotation.out.prokka_gff
      faa = _wAnnotation.out.prokka_faa
      mmseqs2_taxonomy = _wAnnotation.out.mmseqs2_taxonomy
      mmseqs2_blast = _wAnnotation.out.mmseqs2_blast
}

/**
*
* The pCount process is designed to count the number of sequences in a given FASTA file.
* It uses the seqkit tool to calculate the sequence statistics of a FASTA file, specifically extracting the sequence count.
*
**/
process pCount {

    label 'tiny'

    tag "Sample: $sample, BinID: $binID"

    container "${params.ubuntu_image}"

    input:
    tuple val(sample), val(binID), path(fasta), path(metadata)

    output:
    tuple val("${sample}"), val("${binID}"), path(fasta), path(metadata), env(COUNT) 

    shell:
    '''
    COUNT=$(seqkit stats -T <(cat !{fasta}) | cut -d$'\t' -f 4 | tail -n 1)
    '''
}

/**
*
* The pCount process is designed to count the number of sequences in a given FASTA file.
* It uses the seqkit tool to calculate the sequence statistics of a FASTA file, specifically extracting the sequence count.
*
**/

process pCountFilter {

    label 'tiny'

    tag "Sample: $sample, BinID: $binID"

    container "${params.ubuntu_image}"

    input:
    val(splitFasta)
    tuple val(sample), val(binID), path(fasta), path(metadata), path(header)

    output:
    tuple val("${sample}"), val("${binID}"), path("filtered/*"), path(metadata), env(COUNT) 

    shell:
    '''
    mkdir filtered
    mlr --itsv --otsv cat  !{header} | csvtk -t cut -f CONTIG > header_concat.tsv
    for fa in $(ls -1 !{fasta}); do 
        if [ ! -z "$(seqkit grep --quiet -f header_concat.tsv ${fa})" ]; then  
            if [ "!{splitFasta}" == "true" ]; then
                seqkit grep --quiet -f header_concat.tsv ${fa} > filtered/$(basename ${fa})
            else
                cp ${fa} filtered/;
            fi
        fi 
    done
    COUNT=$(seqkit stats -T <(cat filtered/*) | cut -d$'\t' -f 4 | tail -n 1)
    '''
}

/*
* 
* This method splits the input file in chunks.
*
*/
workflow _wSplit {
  take:
    input
    chunkSize
  main:
    SAMPLE_IDX = 0
    FILE_IDX = 1
    FILE_2_IDX = 2
    FILE_3_IDX = 3
    COUNT_IDX=4
    CHUNK_SIZE_IDX=5
    ID_PLACEHOLDER = ""

    input | map { dataset -> [dataset[SAMPLE_IDX].toString(), ID_PLACEHOLDER, dataset[FILE_IDX], dataset[FILE_2_IDX]] } \
	| pCount | combine(chunkSize) | flatMap { sample -> \
   	Utils.splitFilesIndex(Integer.parseInt(sample[COUNT_IDX]), sample[CHUNK_SIZE_IDX], [sample[SAMPLE_IDX], sample[FILE_2_IDX], sample[FILE_3_IDX]]) } \
	| set { chunks }   	
  emit:
    chunks
}

/*
* 
* This method splits the input file in chunks.
*
*/
workflow _wSplitFasta {
  take:
    input
    chunkSize
    splitFasta
  main:
    SAMPLE_IDX = 0
    FILE_IDX = 1
    FILE_2_IDX = 2
    FILE_3_IDX = 3
    FILE_4_IDX = 4
    COUNT_IDX=4
    CHUNK_SIZE_IDX=5
    ID_PLACEHOLDER = ""

    input |  map { dataset -> [dataset[SAMPLE_IDX].toString(), ID_PLACEHOLDER, dataset[1], dataset[FILE_2_IDX], dataset[FILE_3_IDX]] } \
	     | set { fastaFile } 
	 pCountFilter(splitFasta, fastaFile) |  combine(chunkSize)  \
	   | flatMap { sample -> \
   	Utils.splitFilesIndex(Integer.parseInt(sample[COUNT_IDX]), sample[CHUNK_SIZE_IDX], [sample[SAMPLE_IDX], sample[FILE_2_IDX], sample[FILE_3_IDX]]) } \
	   | set { chunks } 
  emit:
    chunks
}

/*
* This workflow prepares the input for Whokaryote and executes it.
*/
workflow _wRunWhokaryote {
  take:
    contigs
    annotation
  main:
    SAMPLE_IDX=0
    BIN_ID=1
    PATH_IDX=2

    // Whokaryote processes fasta and Prodigals GFF files, thats why they have to be merged here
    contigs | map { contig -> [contig[SAMPLE_IDX], contig[BIN_ID], contig[PATH_IDX]]} \
      | combine(annotation, by:[SAMPLE_IDX, BIN_ID]) | pWhokaryote
  emit:
      logs = pWhokaryote.out.logs
      eukaryotes = pWhokaryote.out.eukaryotes
}

/**
*
* The main annotation workflow. 
* It is build to handle one big input fasta file.
* Based on this file genes will be predicted and annotated using Prokka, these genes will be blasted against KEGG.
* Gtdb results are optional to set the domain for annotation with Prokka.
* At the end kegg- and prokka-infos of the results will be collected and presented.
*
**/ 
workflow _wAnnotation {
   take:
      sourceChannel
      prodigalMode
      fasta
      contigCoverage
      binContigMapping
      fastaCounter
   main:
      SAMPLE_IDX=0
      PATH_IDX=2

      pProkka(prodigalMode, fasta | combine(contigCoverage, by: SAMPLE_IDX))

       // Collect all databases
      selectedDBs = params?.steps?.annotation?.mmseqs2.findAll({ it.key != "chunkSize" }).collect({
            [it.key, it.value?.additionalParams?.search ?: "", \
		it.value?.additionalParams?.additionalColumns ?: "", \
	         it.value?.database?.extractedDBPath ?: "", \
            it.value.database?.download?.source ?: "", \
            it.value.database?.download?.md5sum ?: "", \
            it.value.database?.download?.s5cmd?.params ?: "" ]
      })

       selectedTaxDBs = params?.steps?.annotation?.mmseqs2_taxonomy.findAll({  it.key != "runOnMAGs"  }).collect({
            [it.key, it.value?.params ?: "", \
             it.value?.ramMode ? "true" : "false", \
             it.value?.initialMaxSensitivity ?: "", \
             it.value?.database?.extractedDBPath ?: "", \
             it.value.database?.download?.source ?: "", \
             it.value.database?.download?.md5sum ?: "", \
             it.value.database?.download?.s5cmd?.params ?: "" ]
      })

      // The following groupTuple operators need a counter for the expected number of bins or fasta files of a sample.
      // This allows groupTuple to not block until all samples are processed.

      // Create contig to gene mapping
      pProkka.out.tsv | map { [it[SAMPLE_IDX], it[PATH_IDX]] } | combine(fastaCounter, by:SAMPLE_IDX) \
	|  map { sample, path, size -> tuple( groupKey(sample, size), path ) } | groupTuple() | set { contig2GeneMapping }

      // Run all amino acid outputs against all databases
      // Collect by sample name to bundle searches and avoid calls with small input files
      pProkka.out.faa | map { [it[SAMPLE_IDX], it[PATH_IDX]] } | combine(fastaCounter, by:SAMPLE_IDX) \
	| map { sample, path, size -> tuple( groupKey(sample, size), path ) } | groupTuple() | set { groupedProkkaFaa }

      MMSEQS2_CHUNK_SIZE_DEFAULT=7000

      // Split fasta files and run blast on each part in parallel
      _wSplit(groupedProkkaFaa | combine(contig2GeneMapping, by: SAMPLE_IDX), \
	Channel.from(params.steps?.annotation?.mmseqs2?.chunkSize?:MMSEQS2_CHUNK_SIZE_DEFAULT)) \
           | combine(Channel.from(selectedDBs)) | set { combinedMMseqs }

      pMMseqs2(sourceChannel, combinedMMseqs)

      FIRST_ELEM_IDX = 0
      FIRST_ELEM_DB_IDX = 0
      FIRST_ELEM_SAMPLE_IDX = 1
      FIRST_ELEM_TYPE_IDX = 2

      ELEM_PATH_IDX = 3
      
      pMMseqs2.out.blast \
          // An artificial group key is created which consists of the db, sample name and the type of the data (e.g. binned)
	| map { db, sample, type, start, stop, chunks, out -> [db + "_-_" + sample + "_-_" + type, db, sample, type, start, stop, chunks, out] } \
        | map { key, db, sample, type, start, stop, chunks, out -> tuple(groupKey(key, chunks.toInteger()), [db, sample, type, out]) } \
          // Based on the artificial group key and the expected number of chunks, the groupTuple operator is executed
	| groupTuple() \
          // All values of each entry are then reordered and prepared for the pCollectFile process input
        | map { key, dataset -> [dataset[FIRST_ELEM_IDX][FIRST_ELEM_DB_IDX], dataset[FIRST_ELEM_IDX][FIRST_ELEM_SAMPLE_IDX], \
	dataset[FIRST_ELEM_IDX][FIRST_ELEM_TYPE_IDX], dataset.stream().map{ elem -> elem[ELEM_PATH_IDX] }.collect()] } \
	| combine(Channel.value("metaeuk")) \
	| combine(Channel.value("blast")) | pCollectFile
	pCollectFile.out.gff | flatMap({ sample, type, dbType, blastFiles -> blastFiles.stream().map({ fi -> [sample, type, dbType, fi] }).collect() }) \
        | set { collectedMMseqsResults }

	  // The kegg database result is selected and reordered for the pKEGGFromMMseqs2 process input
	collectedMMseqsResults | filter({ sample, type, dbType, blastFiles -> dbType == "kegg" }) \
        | map { sample, type, dbType, blastFiles -> [sample, type, blastFiles]}
        | pKEGGFromMMseqs2

      combinedMMseqsTax = groupedProkkaFaa | combine(Channel.from(selectedTaxDBs))
      pMMseqs2_taxonomy(sourceChannel, combinedMMseqsTax)

      // Run Resistance Gene Identifier with amino acid outputs
      pProkka.out.faa | pResistanceGeneIdentifier

     // This line runs Whokaryote and sends all sequences specified as eukaryotic to a workflow
     // for predicting, annotating and classifying eukaryotic genes and contigs
     _wRunWhokaryote(fasta, pProkka.out.prodigalGff)
      HEADER_IDX=3
      _wRunWhokaryote.out.eukaryotes | map { [it[SAMPLE_IDX], it[HEADER_IDX]] } \
      | combine(fastaCounter, by:SAMPLE_IDX) \
      |  map { sample, path, size -> tuple( groupKey(sample, size), path ) } \
      | groupTuple() | set { headerFiles }

      _wAnalyseEukaryotes(sourceChannel, fasta, headerFiles,\
      binContigMapping, fastaCounter, Channel.value("false"))

      pProkka.out.logs \
	| mix(_wRunWhokaryote.out.logs) \
	| mix(pMMseqs2.out.logs) \
        | mix(pMMseqs2_taxonomy.out.logs) \
	| mix(pResistanceGeneIdentifier.out.logs) \
	| mix(pKEGGFromMMseqs2.out.logs) | pDumpLogs
   emit:
      keggAnnotation = pKEGGFromMMseqs2.out.keggPaths
      mmseqs2_kronaHtml = pMMseqs2_taxonomy.out.kronaHtml
      mmseqs2_krakenTaxonomy = pMMseqs2_taxonomy.out.krakenStyleTaxonomy
      mmseqs2_taxonomy = pMMseqs2_taxonomy.out.taxonomy
      mmseqs2_blast = collectedMMseqsResults
      prokka_faa = pProkka.out.faa
      prokka_ffn = pProkka.out.ffn
      prokka_fna = pProkka.out.fna
      prokka_fsa = pProkka.out.fsa
      prokka_gbk = pProkka.out.gbk
      prokka_gff = pProkka.out.gff
      prokka_sqn = pProkka.out.sqn
      prokka_tbl = pProkka.out.tbl
      prokka_tsv = pProkka.out.tsv
      prokka_txt = pProkka.out.txt 
}
