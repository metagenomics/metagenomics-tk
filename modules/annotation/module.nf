nextflow.enable.dsl=2

include { pDumpLogs } from '../utils/processes'

import java.nio.file.Files
import java.nio.file.Paths

mode = 'mode_not_set'

// Function to generate the right output path for each tool/process to be feed into the publishDir call of said processes.
def getOutput(SAMPLE, RUNID, TOOL, filename){
    return SAMPLE + '/' + RUNID + '/' + params.modules.annotation.name + '/' + 
          params.modules.annotation.version.major + "." + 
          params.modules.annotation.version.minor + "." + 
          params.modules.annotation.version.patch +
          '/' + TOOL + '/' + filename
}

/**
* A function that takes a tool parameter key as input and extracts all underlying entries of said key.
* Should these entries contain references to external databases, s3/aws key-files, etc. a docker volume mount string is returned,
* so that docker processes run with this tool can include this string to have access to all external files. 
* See “/lib/Utils.groovy” for more information.
**/
def constructParametersObject(String tool){ 
  return params?.steps?.annotation?."$tool".findAll().collect{ Utils.getDockerMount(it.value?.database, params, 'true')}.join(" ")
}


/**
*
* MMseqs2 is used to search for big input queries in large databases. 
* Multiple databases can be searched at the same time. Parts of the query and search database are loaded into the RAM.
# Huge databases along with to little RAM could lead to errors.
* Outputs will be saved in separate directories.
*
**/
process pMMseqs2 {
   
      container "${params.mmseqs2_image}"

      // Databases will be downloaded to a fixed place so that they can be used by future processes.
      // This fixed place has to be outside of the working-directory to be easy to find for every process.
      // Therefore this place has to be mounted to the docker container to be accessible during run time.
      // Another mount flag is used to get a key file (aws format) into the docker-container. 

      // This file is then used by s5cmd. The necessary mount points are generated by “constructParametersObject()”. 
      containerOptions constructParametersObject("mmseqs2")
 
      tag "Sample: $sample, Database: $dbType"

      label 'large'

      publishDir params.output, mode: "${params.publishDirMode}", saveAs: { filename -> getOutput("${sample}", params.runid, "mmseqs2/${dbType}", filename) }, \
         pattern: "{**.blast.tsv}"

      when params?.steps.containsKey("annotation") && params?.steps.annotation.containsKey("mmseqs2")

   input:
      val(binType)
      tuple val(sample), file(fasta), val(dbType), val(parameters), val(EXTRACTED_DB), val(DOWNLOAD_LINK), val(MD5SUM), val(S5CMD_PARAMS)
   
   output:
      tuple val("${dbType}"), val("${sample}"), val("${binType}"), path("${sample}_${binType}.${dbType}.blast.tsv"), emit: blast
      tuple val("${sample}_${binType}"), val("${output}"), val(params.LOG_LEVELS.INFO), file(".command.sh"), \
        file(".command.out"), file(".command.err"), file(".command.log"), emit: logs

   shell:
   '''
   mkdir -p !{params.polished.databases}
   # if no local database is referenced, start download part
   if [ -z "!{EXTRACTED_DB}" ] 
   then
         # polished params end with a / so no additional one is needed at the end
         DATABASE=!{params.polished.databases}mmseqs2 
         mkdir -p ${DATABASE}/!{dbType}
         LOCK_FILE=${DATABASE}/!{dbType}/lock.txt
         
         # Create and try to lock the given “LOCK_FILE”. If the file is locked no other process will download the same database simultaneously
         # and wait until the database is downloaded.  Depending on your database path (starts with s3:// | https:// | /../../.. ) 
         # one of the flock input commands is executed to download and decompress the database.
         # If a database is present at the given path, checksums are compared, if they are identical the download will be omitted.  
         flock ${LOCK_FILE} concurrentDownload.sh --output=${DATABASE}/!{dbType} \
            --link=!{DOWNLOAD_LINK} \
            --httpsCommand="wget -O mmseqs.!{dbType}.tar.zst !{DOWNLOAD_LINK}  && zstd --rm -T!{task.cpus} -d mmseqs.!{dbType}.tar.zst && tar -xvf mmseqs.!{dbType}.tar && rm mmseqs.!{dbType}.tar" \
            --s3FileCommand="s5cmd !{S5CMD_PARAMS} cp !{DOWNLOAD_LINK} mmseqs.!{dbType}.tar.zst && zstd --rm -T!{task.cpus} -d mmseqs.!{dbType}.tar.zst && tar -xvf mmseqs.!{dbType}.tar && rm mmseqs.!{dbType}.tar" \
            --s3DirectoryCommand="s5cmd !{S5CMD_PARAMS} cp !{DOWNLOAD_LINK} mmseqs.!{dbType}.tar.zst && zstd --rm -T!{task.cpus} -d mmseqs.!{dbType}.tar.zst && tar -xvf mmseqs.!{dbType}.tar && rm mmseqs.!{dbType}.tar" \
	    --s5cmdAdditionalParams="!{S5CMD_PARAMS}" \
            --localCommand="zstd -T!{task.cpus} -d !{DOWNLOAD_LINK} -o mmseqs.!{dbType}.tar && tar -xvf mmseqs.!{dbType}.tar && rm mmseqs.!{dbType}.tar" \
            --expectedMD5SUM=!{MD5SUM}
          
          # Path of the newly downloaded database. A mmseqs2 database consists out of multiple files,
          # the unique lookup file is searched to determine the basename of each database.
          FILEPATH=$(find ${DATABASE}/!{dbType} -name *.lookup -type f)
          # remove extension to get the files basename.
          MMSEQS2_DATABASE_DIR=${FILEPATH%.*}
   else
          # If an extracted database is present use that path. 
          MMSEQS2_DATABASE_DIR="!{EXTRACTED_DB}"
   fi

    mkdir tmp
    # Only mmseqs2 databases can be used for every kind of search. Inputs have to be converted first.
    mmseqs createdb !{fasta} queryDB
    # Load all indices into memory to increase searching speed
    mmseqs touchdb --threads !{task.cpus} queryDB
    mmseqs touchdb --threads !{task.cpus} ${MMSEQS2_DATABASE_DIR}
    mmseqs search queryDB ${MMSEQS2_DATABASE_DIR} !{sample}_!{binType}.!{dbType}.results.database tmp !{parameters} --threads !{task.cpus}
    # mmseqs2 searches produce output databases. These have to be converted to a more useful format. The blast -outfmt 6 in this case.
    mmseqs convertalis queryDB ${MMSEQS2_DATABASE_DIR} !{sample}_!{binType}.!{dbType}.results.database !{sample}_!{binType}.!{dbType}.blast.tsv --threads !{task.cpus}
   '''
}

/**
*
* The MMseqs2 module taxonomy calls an internal module lca that implements an lowest common ancestor assignment for sequences by querying them against a seqTaxDB.
* Multiple databases can be searched at the same time. Parts of the query and search database are loaded into the RAM.
# Huge databases along with to little RAM could lead to errors.
* Outputs will be saved in separate directories.
*
**/
process pMMseqs2_taxonomy {
   
      container "${params.mmseqs2_image}"

      // Databases will be downloaded to a fixed place so that they can be used by future processes.
      // This fixed place has to be outside of the working-directory to be easy to find for every process.
      // Therefore this place has to be mounted to the docker container to be accessible during run time.
      // Another mount flag is used to get a key file (aws format) into the docker-container. 
      // This file is then used by s5cmd. The necessary mount points are generated by “constructParametersObject()”. 
      containerOptions constructParametersObject("mmseqs2_taxonomy")
 
      tag "Sample: $sample, Database_taxonomy: $dbType"

      label 'large'

      publishDir params.output, mode: "${params.publishDirMode}", saveAs: { filename -> getOutput("${sample}", params.runid, "mmseqs2_taxonomy/${dbType}", filename) }, \
         pattern: "{*.out,*.html,*.tsv}"
 
      when params?.steps.containsKey("annotation") && params?.steps.annotation.containsKey("mmseqs2_taxonomy")

   input:
      val(binType)
      tuple val(sample), file(fasta), val(dbType), val(parameters), val(EXTRACTED_DB), val(DOWNLOAD_LINK), val(MD5SUM), val(S5CMD_PARAMS)
   
   output:
      tuple val("${dbType}"), val("${sample}"), path("${sample}_${binType}.${dbType}.taxonomy.tsv"), emit: taxonomy
      tuple val("${dbType}"), val("${sample}"), path("${sample}_${binType}.${dbType}.krakenStyleTaxonomy.out"), emit: krakenStyleTaxonomy
      tuple val("${dbType}"), val("${sample}"), path("${sample}_${binType}.${dbType}.krona.html"), emit: kronaHtml
      tuple val("${sample}_${binType}"), val("${output}"), val(params.LOG_LEVELS.INFO), file(".command.sh"), \
        file(".command.out"), file(".command.err"), file(".command.log"), emit: logs


   shell:
   '''
   mkdir -p !{params.polished.databases}
   # if no local database is referenced, start download part
      if [ -z "!{EXTRACTED_DB}" ]
      then
            # polished params end with a / so no additional one is needed at the end
            DATABASE=!{params.polished.databases}mmseqs2
            mkdir -p ${DATABASE}/!{dbType}
            LOCK_FILE=${DATABASE}/!{dbType}/lock.txt

            # Create and try to lock the given “LOCK_FILE”. If the file is locked no other process will download the same database simultaneously
            # and wait until the database is downloaded.  Depending on your database path (starts with s3:// | https:// | /../../.. )
            # one of the flock input commands is executed to download and decompress the database.
            # If a database is present at the given path, checksums are compared, if they are identical the download will be omitted.
            flock ${LOCK_FILE} concurrentDownload.sh --output=${DATABASE}/!{dbType} \
               --link=!{DOWNLOAD_LINK} \
               --httpsCommand="wget -O mmseqs.!{dbType}.tar.zst !{DOWNLOAD_LINK}  && zstd --rm -T!{task.cpus} -d mmseqs.!{dbType}.tar.zst && tar -xvf mmseqs.!{dbType}.tar && rm mmseqs.!{dbType}.tar" \
               --s3FileCommand="s5cmd !{S5CMD_PARAMS} cp !{DOWNLOAD_LINK} mmseqs.!{dbType}.tar.zst && zstd --rm -T!{task.cpus} -d mmseqs.!{dbType}.tar.zst && tar -xvf mmseqs.!{dbType}.tar && rm mmseqs.!{dbType}.tar" \
               --s3DirectoryCommand="s5cmd !{S5CMD_PARAMS} cp !{DOWNLOAD_LINK} mmseqs.!{dbType}.tar.zst && zstd --rm -T!{task.cpus} -d mmseqs.!{dbType}.tar.zst && tar -xvf mmseqs.!{dbType}.tar && rm mmseqs.!{dbType}.tar" \
   	    --s5cmdAdditionalParams="!{S5CMD_PARAMS}" \
               --localCommand="zstd -T!{task.cpus} -d !{DOWNLOAD_LINK} -o mmseqs.!{dbType}.tar && tar -xvf mmseqs.!{dbType}.tar && rm mmseqs.!{dbType}.tar" \
               --expectedMD5SUM=!{MD5SUM}

             # Path of the newly downloaded database. A mmseqs2 database consists out of multiple files,
             # the unique lookup file is searched to determine the basename of each database.
             FILEPATH=$(find ${DATABASE}/!{dbType} -name *.lookup -type f)
             # remove extension to get the files basename.
             MMSEQS2_DATABASE_DIR=${FILEPATH%.*}
      else
             # If an extracted database is present use that path.
             MMSEQS2_DATABASE_DIR="!{EXTRACTED_DB}"
      fi

    mkdir tmp
    # Only mmseqs2 databases can be used for every kind of search. Inputs have to be converted first.
    mmseqs createdb !{fasta} queryDB
    # Load all indices into memory to increase searching speed
    mmseqs touchdb --threads !{task.cpus} queryDB
    mmseqs touchdb --threads !{task.cpus} ${MMSEQS2_DATABASE_DIR}
    # Define taxonomies
    mmseqs taxonomy queryDB ${MMSEQS2_DATABASE_DIR} !{sample}_!{binType}.!{dbType}.taxresults.database tmp !{parameters} --threads !{task.cpus}
    # mmseqs2 searches produce output databases. These have to be converted to more useful formats.
    mmseqs createtsv queryDB !{sample}_!{binType}.!{dbType}.taxresults.database !{sample}_!{binType}.!{dbType}.taxonomy.tsv --threads !{task.cpus}
    mmseqs taxonomyreport ${MMSEQS2_DATABASE_DIR} !{sample}_!{binType}.!{dbType}.taxresults.database !{sample}_!{binType}.!{dbType}.krakenStyleTaxonomy.out
    mmseqs taxonomyreport ${MMSEQS2_DATABASE_DIR} !{sample}_!{binType}.!{dbType}.taxresults.database !{sample}_!{binType}.!{dbType}.krona.html --report-mode 1
   '''
}

process pResistanceGeneIdentifier {
   
      container "${params.rgi_image}"
      
      containerOptions Utils.getDockerMount(params?.steps?.annotation?.rgi?.database, params)
 
      tag "$sample $binID"

      label 'large'

      publishDir params.output, mode: "${params.publishDirMode}", saveAs: { filename -> getOutput("${sample}", params.runid, "rgi", filename) }, \
         pattern: "{**.rgi.tsv}"

      when params.steps.containsKey("annotation") && params?.steps.annotation.containsKey("rgi")

   input:
      tuple val(sample), val(binID), file(fasta)
   
   output:
      tuple val("${sample}"), val("${binID}"), path("${sample}_${binID}.rgi.tsv"), emit: results
      tuple val("${sample}_${binID}"), val("${output}"), val(params.LOG_LEVELS.INFO), file(".command.sh"), \
        file(".command.out"), file(".command.err"), file(".command.log"), emit: logs

   shell:
   EXTRACTED_DB=params.steps?.annotation?.rgi?.database?.extractedDBPath ?: ""
   DOWNLOAD_LINK=params.steps?.annotation?.rgi?.database?.download?.source ?: ""
   MD5SUM=params?.steps?.annotation?.rgi?.database?.download?.md5sum ?: ""
   S5CMD_PARAMS=params.steps?.annotation?.rgi?.database?.download?.s5cmd?.params ?: ""
   '''
   mkdir -p !{params.polished.databases}
   ADDITIONAL_RGI_PARAMS=!{params.steps?.annotation?.rgi?.additionalParams}

   # Check developer documentation
   CARD_JSON=""
   if [ -z "!{EXTRACTED_DB}" ] 
   then
        DATABASE=!{params.polished.databases}/rgi
        LOCK_FILE=${DATABASE}/lock.txt

        # Download CARD database
        mkdir -p ${DATABASE}
        flock ${LOCK_FILE} concurrentDownload.sh --output=${DATABASE} \
         --link=!{DOWNLOAD_LINK} \
         --httpsCommand="wget -O data !{DOWNLOAD_LINK} && tar -xvf data && rm data" \
         --s3DirectoryCommand="s5cmd !{S5CMD_PARAMS} cp !{DOWNLOAD_LINK} . " \
         --s3FileCommand="s5cmd !{S5CMD_PARAMS} cp !{DOWNLOAD_LINK} data && tar -xvf data  && rm data" \
	 --s5cmdAdditionalParams="!{S5CMD_PARAMS}" \
         --localCommand="tar -xvf !{DOWNLOAD_LINK}" \
         --expectedMD5SUM=!{MD5SUM}

         CARD_JSON="$(readlink -f ${DATABASE}/out/card.json)"
   else
         CARD_JSON="!{EXTRACTED_DB}"
   fi

   # gunzip (if required) and strip '*' sign from amino acid files
   zcat -f !{fasta} | sed 's/*//g' > input.faa

   RGI_OUTPUT=!{binID}.rgi
   # load CARD database and run rgi
   rgi load --card_json ${CARD_JSON} --local
   rgi main --input_sequence input.faa \
               --output_file ${RGI_OUTPUT} --input_type protein --local \
               --alignment_tool DIAMOND --num_threads !{task.cpus} --clean ${ADDITIONAL_RGI_PARAMS}

   #  add sample and binid information to rgi output
   sed  '1 s/^/SAMPLE\tBIN_ID\t/g' ${RGI_OUTPUT}.txt | sed "2,$ s/^/!{sample}\t!{binID}\t/g" > !{sample}_!{binID}.rgi.tsv
   '''
}


process pGeneCoverage {

   tag "$sample"

   label 'small'

   container "${params.ubuntu_image}"

   publishDir params.output, mode: "${params.publishDirMode}", saveAs: { filename -> getOutput("${sample}", params.runid, "geneCoverage", filename) }

   when params.steps.containsKey("annotation")

   input:
      tuple val(sample), file(defaultAbundances), file(metabatAbundances), val(binID), path(genes)

   output:
      tuple val("${sample}"), val("${binID}"), path("${sample}_${binID}_default_abundances.tsv"), emit: metabatAbundances
      tuple val("${sample}"), val("${binID}"), path("${sample}_${binID}_metabat_abundances.tsv"), emit: defaultAbundances
      tuple val("${sample}_${binID}"), val("${output}"), val(params.LOG_LEVELS.INFO), file(".command.sh"), \
        file(".command.out"), file(".command.err"), file(".command.log"), emit: logs

   shell:
   output = getOutput("${sample}", params.runid, "geneCoverage", "")
   '''
   # Split in ContigIDs and GeneIDs
   seqkit fx2tab -n -i !{genes} \
	| tee gene_ids.tsv \
	| rev | cut -d '_' -f 2- \
	| rev > contig_ids.tsv

   # Create Contig to gene mapping
   echo -e "CONTIG\tGENE" > contig_gene_id.tsv
   paste -d$'\t' contig_ids.tsv gene_ids.tsv >> contig_gene_id.tsv

   # Add abundance values to genes
   join --header -t$'\t' -1 2 -2 1 <(sort -t$'\t' -k 2,2 !{defaultAbundances}) <(sort -k 1,1 -t$'\t' contig_gene_id.tsv) > !{sample}_!{binID}_default_abundances.tsv
   join --header -t$'\t' -1 2 -2 1 <(sort -t$'\t' -k 2,2 !{metabatAbundances}) <(sort -k 1,1 -t$'\t' contig_gene_id.tsv) > !{sample}_!{binID}_metabat_abundances.tsv
   '''
}


/**
*
* pKEGGFromBlast is build to handle results in the outfmt 6 file standard.
* These search results will be compared to kegg link-files to produce a file where all available kegg information
* for these search results is collected in a centralized way/file.
* You need to call (and fill out) the aws credential file with -c to use this module as kegg is commercial an the database has to be placed in your project s3!
*
**/
process pKEGGFromBlast {

      tag "$sample"

      label 'small'

      container "${params.python_env_image}"

      // Databases will be downloaded to a fixed place so that they can be used by future processes.
      // These fixed place has to be outside of the working-directory to be easy to find for every process.
      // Therefore this place has to be mounted to the docker container to be accessible during runtime.
      // Another mount flag is used to get a key file (aws format) into the docker-container. 
      // This file is then used by s5cmd. 

      containerOptions Utils.getDockerMount(params.steps?.annotation?.keggFromBlast?.database, params)
      publishDir params.output, mode: "${params.publishDirMode}", saveAs: { filename -> getOutput("${sample}", params.runid, "keggFromBlast", filename) }, \
         pattern: "{**.tsv}"

      when params?.steps.containsKey("annotation") && params?.steps.annotation.containsKey("keggFromBlast")

   input:
      tuple val(sample), val(binType), file(blast_result)

   output:
      tuple val("${sample}"), path("${sample}_${binType}_kegg.tsv"), emit: kegg_blast
      tuple val("${sample}_${binType}"), val("${output}"), val(params.LOG_LEVELS.INFO), file(".command.sh"), \
        file(".command.out"), file(".command.err"), file(".command.log"), emit: logs

   shell:
      output = getOutput("${sample}", params.runid, "keggFromBlast", "")
      DOWNLOAD_LINK=params.steps?.annotation?.keggFromBlast?.database?.download?.source ?: ""
      MD5SUM=params?.steps?.annotation?.keggFromBlast?.database?.download?.md5sum ?: ""
      S5CMD_PARAMS=params.steps?.annotation?.keggFromBlast?.database?.download?.s5cmd?.params ?: ""
      EXTRACTED_DB=params.steps?.annotation?.keggFromBlast?.database?.extractedDBPath ?: ""
      '''
      mkdir -p !{params.polished.databases}
      # Check developer documentation
      KEGG_DB=""
      if [[ -z "!{EXTRACTED_DB}" ]] 
      then
        DATABASE=!{params.polished.databases}/kegg
        LOCK_FILE=${DATABASE}/lock.txt

        # Download CARD database
        mkdir -p ${DATABASE}
        flock ${LOCK_FILE} concurrentDownload.sh --output=${DATABASE} \
         --link=!{DOWNLOAD_LINK} \
         --httpsCommand="wget -O kegg.tar.gz !{DOWNLOAD_LINK} && tar -xzvf kegg.tar.gz && rm kegg.tar.gz " \
         --s3DirectoryCommand="s5cmd !{S5CMD_PARAMS} cp !{DOWNLOAD_LINK} . " \
         --s3FileCommand="s5cmd !{S5CMD_PARAMS} cp !{DOWNLOAD_LINK} kegg.tar.gz && tar -xzvf kegg.tar.gz && rm kegg.tar.gz " \
	 --s5cmdAdditionalParams="!{S5CMD_PARAMS}" \
         --localCommand="tar -xzvf !{DOWNLOAD_LINK} " \
         --expectedMD5SUM=!{MD5SUM}

         KEGG_DB="${DATABASE}/out/"
      else
         KEGG_DB="!{EXTRACTED_DB}"
      fi
      '''
}


/**
*
* Helper method to get the appropriate flag for the prodigal mode depending on the set prodigalMode 
* and the parameter set as params.steps.annotation.prokka.prodigalMode
* 
**/
def getProdigalModeString(prodigalMode) {
    if(prodigalMode == "param") {
        prodigalMode = params.steps.annotation.prokka.prodigalMode
    }
    if (prodigalMode == "single") {
        prodigalModeStr = ""
    } else if (prodigalMode == "meta") {
        prodigalModeStr = "--metagenome"
    } else {
        error "Invalid prodigal mode: ${prodigalMode}"
    }
        return prodigalModeStr
}

/**
*
* Helper workflow to create the input for prokka, set inputs in correct order and get taxonomy from gtdb results if available
* 
**/
workflow _wCreateProkkaInput {
    take:
        fasta
        gtdb
    main:
        DATASET_IDX = 0
        BIN_ID_IDX = 1
        PATH_IDX = 2
        if(gtdb){
            //get GTDB domains from the tsv file with Taxonomy for all bins:
            GTDB_FILE_IDX = 0
            DOMAIN_IDX = 0
            gtdb | filter(it -> file(it[GTDB_FILE_IDX]).text?.trim()) \
             | splitCsv(sep: '\t', header: true) \
             | map { it ->  def command = it[GTDB_FILE_IDX].classification.split(';')[DOMAIN_IDX].minus('d__'); [it[GTDB_FILE_IDX].SAMPLE, it[GTDB_FILE_IDX].BIN_ID, command] } \
             | set { gtdbDomain }
            //set domain for each bin
            DOMAIN_PROKKA_INPUT_IDX = 3
            fasta  |  map {it -> [it[DATASET_IDX], it[BIN_ID_IDX], file(it[PATH_IDX]) ]} \
             | join(gtdbDomain, by:[DATASET_IDX, BIN_ID_IDX], remainder: true) \
             | map { it -> [ it[DATASET_IDX], it[BIN_ID_IDX], it[PATH_IDX], it[DOMAIN_PROKKA_INPUT_IDX]?:params?.steps?.annotation?.prokka?.defaultKingdom ] } \
             | set { prokkaInput }
        } else {
            //if no gtdb annotation available, default to the defaultKingdom in params
            fasta | map { it -> [it[DATASET_IDX], it[BIN_ID_IDX], it[PATH_IDX], params?.steps?.annotation?.prokka?.defaultKingdom]} | set { prokkaInput }
        }
        
    emit:
        prokkaInput
}


/**
*
* Prokka is a tool to annotate, bacterial, archael and viral genomes.
* Input is a mode for the gene prediction tool prodigal included in prokka, which can be:
* - "single" for larger contigs 
* - "meta" for smaller contigs / metagenomes
* - "param" sets prodigalMode to whatever is defined as params.steps.annotation.prokka.prodigalMode
* Other input is a tuple consisting of sample, binID, path to fasta file and the domain (for gene prediction)
* locusTag setzen? Eindeutiger Tag den wir ersetzen koennen
* 
**/
process pProkka {

    container "${params.prokka_image}"

    label 'small'

    tag "Sample: $sample, BinID: $binID"

    publishDir params.output, mode: "${params.publishDirMode}", saveAs: { filename -> getOutput("${sample}",params.runid ,"prokka", filename) }

    when params.steps.containsKey("annotation") && params.steps.annotation.containsKey("prokka")

    input:
      val(prodigalMode)
      tuple val(sample), val(binID), file(fasta), val(domain)

    output:
      tuple val("${sample}"), val("${binID}"), file("*.gff.gz"), emit: gff 
      tuple val("${sample}"), val("${binID}"), file("*.faa.gz"), emit: faa 
      tuple val("${sample}"), val("${binID}"), file("*.fna.gz"), emit: fna 
      tuple val("${sample}"), val("${binID}"), file("*.ffn.gz"), emit: ffn 
      tuple val("${sample}"), val("${binID}"), file("*.fsa.gz"), emit: fsa 
      tuple val("${sample}"), val("${binID}"), file("*.gbk.gz"), emit: gbk 
      tuple val("${sample}"), val("${binID}"), file("*.tbl.gz"), emit: tbl 
      tuple val("${sample}"), val("${binID}"), file("*.sqn.gz"), emit: sqn 
      tuple val("${sample}"), val("${binID}"), file("*.txt"), emit: txt 
      tuple val("${sample}"), val("${binID}"), file("*.tsv"), emit: tsv 
      tuple val("${sample}_${binID}"), val("${output}"), val(params.LOG_LEVELS.INFO), file(".command.sh"), \
        file(".command.out"), file(".command.err"), file(".command.log"), emit: logs

    shell:
      output = getOutput("${sample}", params.runid, "prokka", "")
      prodigalModeStr = getProdigalModeString(prodigalMode)

      '''
      # Prepare Input Variables
      BIN=!{fasta}
      BIN_PREFIX=$(echo "${BIN%.*}")
      BIN_ID="$(basename !{fasta})"

      # Run Prokka
      if [[ !{fasta} == *.gz ]]; then
        zcat -f !{fasta} > input.fasta
        prokka !{params.steps.annotation.prokka.additionalParams} !{prodigalModeStr} --partialgenes --cpus !{task.cpus} --outdir out --kingdom !{domain} input.fasta
        rm input.fasta
      else 
        prokka !{params.steps.annotation.prokka.additionalParams} !{prodigalModeStr} --partialgenes --cpus !{task.cpus} --outdir out --kingdom !{domain} !{fasta}
      fi

      # Prepare output 
      for f in out/* ; do suffix=$(echo "${f##*.}"); mv $f ${BIN_PREFIX}.${suffix}; done
      sed -i  -e "2,$ s/^/!{sample}\t${BIN_ID}\t/"  -e "1,1 s/^/SAMPLE\tBIN_ID\t/g" *.tsv
      mv *.tsv !{sample}_prokka_${BIN_ID}.tsv
      pigz --best --processes !{task.cpus} *gff *.faa *.fna *.ffn *.fsa *.gbk *.sqn *tbl
      '''
}


/**
*
* This entry point uses a file to grab all fasta files in the referenced directories for annotation.
* You need to call (and fill out) the aws credential file with -c to use this module!
* The .tsv file has to have: DATASET, BIN_ID and PATH entries.
* 
* The "database_mode" is used to choose which database path is expected.
* If the databasepath starts with "https://" or "s3://" the object storage based mode is used.
* All other paths are seen as "local" mode paths and offline stored copys are expected.
*
**/
workflow wAnnotateFile {

   take:
      projectTableFile
   main:
      annotationTmpDir = params.tempdir + "/annotation"
      file(annotationTmpDir).mkdirs()
      projectTableFile | splitCsv(sep: '\t', header: true) \
      | map{ [it.DATASET, it.BIN_ID, file(it.PATH)] } | set { input } 
      _wAnnotation(Channel.value("param"), input, null, Channel.empty())
   emit:
      keggAnnotation = _wAnnotation.out.keggAnnotation
}

/**
*
* See wAnnotateFile for a description.
*
**/
workflow wAnnotateList {
   take:
      sourceChannel
      prodigalMode
      fasta
      gtdb
      contigCoverage
   main:
      annotationTmpDir = params.tempdir + "/annotation"
      file(annotationTmpDir).mkdirs()
      _wAnnotation(sourceChannel, prodigalMode, fasta, gtdb, contigCoverage)
    emit:
      keggAnnotation = _wAnnotation.out.keggAnnotation
}


/**
*
* The main annotation workflow. 
* It is build to handle one big input fasta file.
* Based on this file genes will be predicted and annotated using Prokka, these genes will be blasted against KEGG.
* Gtdb results are optional to set the domain for annotation with Prokka.
* At the end kegg- and prokka-infos of the results will be collected and presented.
*
**/ 
workflow _wAnnotation {
   take:
      sourceChannel
      prodigalMode
      fasta
      gtdb
      contigCoverage
   main:
      // Format input for prokka and run prokka:
      _wCreateProkkaInput(fasta, gtdb)
      pProkka(prodigalMode, _wCreateProkkaInput.out.prokkaInput)
      
      // Collect all databases
      selectedDBs = params?.steps?.annotation?.mmseqs2.findAll().collect({ 
            [it.key, it.value?.params ?: "", \
	         it.value?.database?.extractedDBPath ?: "", \
             it.value.database?.download?.source ?: "", \
             it.value.database?.download?.md5sum ?: "", \
             it.value.database?.download?.s5cmd?.params ?: "" ]
      })

      selectedTaxDBs = params?.steps?.annotation?.mmseqs2_taxonomy.findAll().collect({
            [it.key, it.value?.params ?: "", \
             it.value?.database?.extractedDBPath ?: "", \
             it.value.database?.download?.source ?: "", \
             it.value.database?.download?.md5sum ?: "", \
             it.value.database?.download?.s5cmd?.params ?: "" ]
      })

      SAMPLE_IDX=0
      PATH_IDX=2
      // Run all amino acid outputs against all databases
      // Collect by sample name to bundle searches and avoid calls with small input files
      combinedMMseqs = pProkka.out.faa | map{ [it[SAMPLE_IDX], it[PATH_IDX]] }| groupTuple() | combine(Channel.from(selectedDBs))
      pMMseqs2(sourceChannel, combinedMMseqs)
      combinedMMseqsTax = pProkka.out.faa | map{ [it[SAMPLE_IDX], it[PATH_IDX]] }| groupTuple() | combine(Channel.from(selectedTaxDBs))
      pMMseqs2_taxonomy(sourceChannel, combinedMMseqsTax)
      DB_TYPE_IDX = 0
      pMMseqs2.out.blast | filter({ result -> result[DB_TYPE_IDX] == "kegg" }) \
	| map({ result -> result.remove(0); result }) \
	| set { mmseqs2Results } 

      // Run Resistance Gene Identifier with amino acid outputs
      pProkka.out.faa | pResistanceGeneIdentifier
      pKEGGFromBlast(mmseqs2Results)

      // Compute gene coverage based on contig coverage
      contigCoverage | combine(pProkka.out.ffn, by: SAMPLE_IDX) | pGeneCoverage

      pProkka.out.logs | mix(pMMseqs2.out.logs) \
        | mix(pMMseqs2_taxonomy.out.logs) \
	| mix(pResistanceGeneIdentifier.out.logs) \
	| mix(pKEGGFromBlast.out.logs) | pDumpLogs
   emit:
      keggAnnotation = pKEGGFromBlast.out.kegg_blast
      mmseqs2_kronaHtml = pMMseqs2_taxonomy.out.kronaHtml
      mmseqs2_krakenTaxonomy = pMMseqs2_taxonomy.out.krakenStyleTaxonomy
      mmseqs2_taxonomy = pMMseqs2_taxonomy.out.taxonomy
      mmseqs2_blast = pMMseqs2.out.blast
      prokka_faa = pProkka.out.faa
      prokka_ffn = pProkka.out.ffn
      prokka_fna = pProkka.out.fna
      prokka_fsa = pProkka.out.fsa
      prokka_gbk = pProkka.out.gbk
      prokka_gff = pProkka.out.gff
      prokka_sqn = pProkka.out.sqn
      prokka_tbl = pProkka.out.tbl
      prokka_tsv = pProkka.out.tsv
      prokka_txt = pProkka.out.txt 
}
