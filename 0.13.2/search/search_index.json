{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Metagenomics-Toolkit","text":""},{"location":"#introduction","title":"Introduction","text":"<p>The Metagenomics-Toolkit or Toolkit for short is a scalable, data agnostic workflow that automates the analysis of short and long metagenomic reads obtained from Illumina or Oxford Nanopore Technology devices, respectively. The Toolkit offers not only standard features expected in a metagenome workflow, such as quality control, assembly, binning, and annotation, but also distinctive features, such as plasmid identification based on various tools, the recovery of unassembled microbial community members, and the discovery of microbial interdependencies through a combination of dereplication, cooccurrence, and genome-scale metabolic modeling. Furthermore, the Metagenomics-Toolkit includes a machine learning-optimized assembly step that tailors the peak RAM value requested by a metagenome assembler to match actual requirements, thereby minimizing the dependency on dedicated high-memory hardware.</p>"},{"location":"#schematic-overview","title":"Schematic Overview","text":"<p>The Toolkit can be applied in two steps which can be either applied in one execution or in two executions consecutively.  In addition, each module c0an be executed separately. </p>"},{"location":"#per-sample-pipeline-overview","title":"Per-Sample Pipeline Overview","text":"<p>The per-sample part of the pipeline processes each dataset individually. This step includes quality control, assembly, binning and annotation of  each dataset individually. You can read more at the getting started page.</p> <p></p>"},{"location":"#aggregation-pipeline-overview","title":"Aggregation Pipeline Overview","text":"<p>The aggregation part of the pipeline processes and combines the outputs of the \"per-sample\" part. MAGs of all input datasets are dereplicated and the abundances of the MAG representatives are estimated. The MAG representatives are then used for cooccurrence analyses.</p> <p></p>"},{"location":"#emgb","title":"EMGB","text":"<p>The Exploratory Metagenome Browser (EMGB) is a web interface that has been developed for the visual exploration of metagenomic datasets.  The majority of the Toolkit output including detected MAGs, MAG taxonomy, genes, pathways can be viewed via the Exploratory Metagenome Browser. Large datasets containing millions of genes and their annotations are pre-processed and visualized, to be searched in real-time by the user.  The platform provides access to different aspects of one or more datasets via an interactive taxonomic tree, a contig viewer, dynamic KEGG metabolic maps and different statistics.</p> <p></p>"},{"location":"#citation","title":"Citation","text":"<p>Peter Belmann, Benedikt Osterholz, Nils Kleinb\u00f6lting, Alfred P\u00fchler, Andreas Schl\u00fcter, Alexander Sczyrba, Metagenomics-Toolkit: the flexible and efficient cloud-based metagenomics workflow featuring machine learning-enabled resource allocation, NAR Genomics and Bioinformatics, Volume 7, Issue 3, September 2025, lqaf093, https://doi.org/10.1093/nargab/lqaf093</p>"},{"location":"#further-reading","title":"Further Reading","text":"<ul> <li> <p>If you are interested in testing the basic workflow, we recommend that you read the Quickstart page.</p> </li> <li> <p>In case you want to run the workflow on a cluster then you can skip the Quickstart page and start directly with the Getting Started section.</p> </li> <li> <p>If you want to run a specific module, then you can start with the modules section. </p> </li> </ul>"},{"location":"aggregation/","title":"Aggregation","text":"<p>There are two ways to execute the Toolkit. You can either run all steps in one execution, or run the per\u2010sample analysis (e.g., assembly, binning, annotation, etc.) first and then combine the results (e.g., via dereplication and co\u2010occurrence) in a second run. The second option allows you to process multiple samples via independent Toolkit executions on different infrastructures and combine all results afterwards.</p>"},{"location":"aggregation/#requirements","title":"Requirements","text":"<ol> <li>SLURM: The Toolkit was mainly developed for cloud-based clusters using SLURM as a resource orchestrator.</li> <li>Docker: Install Docker by following the official Docker installation instructions.</li> <li>Java: In order to run Nextflow, you need to install Java on your machine. This can be achieved via <code>sudo apt install default-jre</code>.</li> <li>Nextflow should be installed. Please check the official Nextflow instructions</li> <li>This tutorial assumes that you have already executed the Toolkit as described in the full pipeline section.</li> <li>This tutorial requires at least one worker node with 28 cores and 230 GB of RAM.</li> </ol>"},{"location":"aggregation/#run-the-toolkit","title":"Run the Toolkit","text":"<pre><code>NXF_HOME=$PWD/.nextflow NXF_VER=25.04.2 nextflow run metagenomics/metagenomics-tk \\\n    -work-dir $(pwd)/work \\\n    -profile slurm \\\n    -ansi-log false \\\n    -entry wAggregatePipeline \\\n    -params-file  https://raw.githubusercontent.com/metagenomics/metagenomics-tk/refs/heads/master/default/fullPipelineAggregate.yml \\\n    --logDir logAggregate \\\n    --s3SignIn false \\\n    --scratch /vol/scratch \\\n    --databases /vol/scratch/databases \\\n    --input my_data_spades_output \\\n    --output output\n</code></pre> <p>where</p> <ul> <li><code>-work-dir</code> points to a directory that is shared between multiple machines.</li> <li><code>-profile</code> defines the execution profile that should be used (local or cluster computing).</li> <li><code>-entry</code> is the entry point of the aggregation workflow.</li> <li><code>-params-file</code> sets the parameters file which defines the parameters for all tools. (see input section below)</li> <li><code>--logDir</code> points to a directory where your trace TSV, a timeline HTML of the executed processes and a report regarding the resource consumption of the workflow is saved.</li> <li><code>--s3SignIn</code> defines if any S3 login for retrieving inputs is necessary. See the S3 configuration section for more information on how to configure the Toolkit for possible S3 input data.</li> <li><code>--scratch</code> is the directory on the worker node where all intermediate results are saved.</li> <li><code>--databases</code> is the directory on the worker node where all databases are saved. Already downloaded databases on a shared file system can be configured in the database setting of the corresponding database section in the configuration file.</li> <li><code>--output</code> is the output directory where all results are saved. If you want to know more about which outputs are created, then please refer to the modules section.</li> <li><code>--input</code> points to the output directory of the per-sample workflow.</li> </ul> <p>Parameter override</p> <p>Any parameters defined with a double dash are parameters that override parameters that are already specified in the YAML file.</p>"},{"location":"aggregation/#input","title":"Input","text":"Configuration File <pre><code>tempdir: \"tmp\"\ns3SignIn: true\noutput: \"output\"\ninput: \"fullPipelineOutput\"\nlogDir: log\nrunid: 1\nlogLevel: 1\nscratch: \"/vol/scratch\"\npublishDirMode: \"symlink\"\nsteps:\n  dereplication:\n    bottomUpClustering:\n      # stricter MIMAG medium quality\n      minimumCompleteness: 50\n      maximumContamination: 5\n      ANIBuffer: 20\n      mashBuffer: 2000\n      method: 'ANI'\n      additionalParams:\n        mash_sketch: \"\"\n        mash_dist: \"\"\n        # cluster cutoff\n        cluster: \" -c 0.05 \"\n        pyani: \" -m ANIb \"\n        representativeAniCutoff: 0.95\n  readMapping:\n    bwa2:\n      additionalParams:\n        bwa2_index: \"\"\n        bwa2_mem: \"\"\n     # This module produces two abundance tables.\n     # One table is based on relative abundance and the second one on the trimmed mean.\n     # Just using relative abundance makes it difficult to tell if a genome is part of a dataset.\n     # Thats why it makes sense to set at leat a low min covered fraction parameter.\n    coverm: \" --min-covered-fraction 80  --min-read-percent-identity 95 --min-read-aligned-percent 95 \"\n    covermONT: \" --min-covered-fraction 80  --min-read-aligned-percent 95 \"\n    minimap:\n      additionalParams:\n        minimap_index: \"\"\n        minimap: \"\"\n  cooccurrence:\n    inference:\n      additionalParams:\n        method: 'correlation'\n        rscript: \" --mincovthreshold 0.9 --maxzero 60 --minweight 0.4 \"\n        timeLimit: \"AUTO\"\n    metabolicAnnotation:\n      additionalParams:\n        metabolicEdgeBatches: 5\n        metabolicEdgeReplicates: 10\n        smetana: \" --flavor bigg --molweight \"\nresources:\n  highmemLarge:\n    cpus: 28\n    memory: 230\n  highmemMedium:\n    cpus: 14\n    memory: 113\n  large:\n    cpus: 28\n    memory: 58\n  medium:\n    cpus: 14\n    memory: 29\n  small:\n    cpus: 7\n    memory: 14\n  tiny:\n    cpus: 1\n    memory: 1\n</code></pre>"},{"location":"aggregation/#output","title":"Output","text":"<p>The meaning of the produced output can be inspected on the respective module page. You can check for the results in the <code>AGGREGATED</code> folder:</p> <p>For example you could check for the number of species clusters created through dereplication:</p> <pre><code>cat  my_data_spades_output/AGGREGATED/1/dereplication/*/bottomUpClustering/clusters/clusters.tsv\n</code></pre> <p>Parameter override</p> <p>Please note that the dereplication method produces more meaningful results when more than one sample is provided as input.</p>"},{"location":"aggregation/#further-reading","title":"Further Reading","text":"<ul> <li> <p>Pipeline Configuration: If you want to configure and optimize the Toolkit for your data or your infrastructure then you can continue with the configuration section.</p> </li> <li> <p>In case you want to import the output to EMGB, please go on to the EMGB configuration section. Please keep in mind that for EMGB only the per-sample part is necessary.</p> </li> <li> <p>You might want to adjust the resource requirements of the Toolkit to your infrastructure.</p> </li> </ul>"},{"location":"configuration/","title":"Global Pipeline Configuration","text":""},{"location":"configuration/#global-parameter-settings","title":"Global parameter settings","text":"<ul> <li> <p><code>tempdir</code>: Temporary directory for storing files that are used to collect intermediate files.</p> </li> <li> <p><code>output</code>: Output directory for storing pipeline results. If an S3 bucket is specified with the corresponding S3 credentials (See S3 configuration section) then    the output is written to S3.</p> </li> <li> <p><code>runid</code>: The run ID will be part of the output path and allows to distinguish between different pipeline configurations that were used for the same dataset.</p> </li> <li> <p><code>logDir</code>: A path to a directory which is used to store log files.</p> </li> <li> <p><code>scratch</code>: The scratch value can be either <code>false</code> or a path on a worker node. If a path is set, then the nextflow process in <code>slurm</code> mode is executed on the provided path.     If the standard mode is used, then the parameter is ignored.</p> </li> <li> <p><code>steps</code>: Steps allows to specify multiple pipeline modules for running the toolkit. We distinguish between two modes. You can either run one tool of    the pipeline or the whole pipeline with different configurations.</p> </li> <li> <p><code>databases</code>: This parameter specifies a place where files are downloaded to. If the <code>slurm</code> profile is used and databases should be downloaded, the path should point to a folder      which is not shared between the worker nodes (to reduce I/O on the shared folder resulting in a better performance). If this parameter is provided, the toolkit will create the specified     directory. If all your databases have already been extracted beforehand, you can simply omit this parameter.</p> </li> <li> <p><code>publishDirMode</code>: (optional) Per default results are symlinked to the chosen <code>output</code> directory. This default mode can be changed with this parameter.     A useful mode is \"copy\", to copy results instead of just linking them. Other modes to choose from here.  </p> </li> <li> <p><code>skipVersionCheck</code>: The toolkit is regurarly tested against a set of Nextflow versions. Setting the <code>--skipVersionCheck</code> allows you to use the toolkit with Nextflow versions    that were not tested.</p> </li> <li> <p><code>s3SignIn</code>: If your input data (not the databases) is not publicly accessible via S3, then you will have to set the <code>s3SignIn</code> parameter to <code>true</code>.</p> </li> </ul>"},{"location":"configuration/#s3-configuration","title":"S3 Configuration","text":"<p>All module inputs and outputs can be used in conjunction with S3. If you want to set a custom S3 configuration setting (i.e. custom S3 endpoint), you will have to modify the aws client parameters  with \" -c \".</p> <p>Example: <pre><code>aws {\n\n    client {\n\n      s3PathStyleAccess = true\n      connectionTimeout = 120000\n      maxParallelTransfers = 28 \n      maxErrorRetry = 10\n      protocol = 'HTTPS'\n      connectionTimeout = '2000'\n      endpoint = 'https://openstack.cebitec.uni-bielefeld.de:8080'\n      signerOverride = 'AWSS3V4SignerType'\n    }\n}\n</code></pre></p> <p>In addition you will have to set a Nextflow Secret with the following keys:</p> <pre><code>nextflow secrets set S3_ACCESS xxxxxxxxx\nnextflow secrets set S3_SECRET xxxxxxxxx\n</code></pre> <p><code>S3_ACCESS</code> corresponds to the aws S3 access key id and <code>S3_SECRET</code> is the aws S3 secret key. If your input data (not the databases) is publicly available then you have to set <code>s3SignIn:</code> to <code>false</code> in your config file. Please note that for using databases you have to set additional credentials (see database section). </p>"},{"location":"configuration/#configuration-of-computational-resources-used-for-pipeline-runs","title":"Configuration of Computational Resources used for Pipeline Runs","text":"<p>The toolkit uses the following machine types (flavors) for running tools. All flavors can be optionally adjusted by modifying the cpus and memory (in GB) parameters. If for example the largest flavor is not available in the infrastructure, <code>cpus</code> and <code>memory</code> parameters can be modified to fit the highmemMedium flavor. If larger flavors are available, it makes especially sense to increase the <code>cpus</code> and <code>memory</code> values of the <code>large</code> flavor to speed up for example assembly and read mapping.</p> <p>Example Configuration:</p> <pre><code>resources:\n  highmemLarge:\n    cpus: 28\n    memory: 230\n  highmemMedium:\n    cpus: 14\n    memory: 113\n  large:\n    cpus: 28\n    memory: 58\n  medium:\n    cpus: 14\n    memory: 29\n  small:\n    cpus: 7\n    memory: 14\n  tiny:\n    cpus: 1\n    memory: 1\n</code></pre> <p>Additional flavors can be defined that can be used by methods that dynamically compute resources on tool error (for example the assembly module).</p> <p>Example:</p> <pre><code>resources:\n  xlarge:\n    cpus: 56\n    memory: 512\n  highmemLarge:\n    cpus: 28\n    memory: 230\n  highmemMedium:\n    cpus: 14\n    memory: 113\n  large:\n    cpus: 28\n    memory: 58\n  medium:\n    cpus: 14\n    memory: 29\n  small:\n    cpus: 7\n    memory: 14\n  tiny:\n    cpus: 1\n    memory: 1\n</code></pre> <p>The full pipeline mode is able to predict the memory consumption of some assemblers (see assembly module). The prediction will also consider additional flavors which have been added to the resources section in the configuration file.</p>"},{"location":"configuration/#apptainer-experimental","title":"Apptainer (Experimental)","text":"<p>Apptainer allows containers to be executed without relying on a daemon process, making it a suitable software solution for HPC setups. Currently, the Toolkit only supports a limited set of processes that can be run using Apptainer, and even those are only tested using the local execution on one machine (<code>standard</code> profile).</p> <p>Apptainer was installed for testing on an Ubuntu machine by following the instructions on the official website. Here, the <code>apptainer-suid</code> package was installed.</p> <p>The following command executes the Toolkit on a local machine using Apptainer. The configuration and (resource) requirements correspond to the one on the Quickstart page, except that Smetana, which is part of the metabolomics module, is disabled. Please check the Quickstart page for the requirements for running the Toolkit. </p> Quickstart command for using Apptainer<pre><code>    NXF_VER=24.10.4 nextflow run metagenomics/metagenomics-tk \\\n          -profile standard \\\n          -entry wFullPipeline \\\n          -params-file  https://raw.githubusercontent.com/metagenomics/metagenomics-tk/refs/heads/master/default/quickstart_apptainer.yml \\\n          --apptainer \\\n          --logDir logs \\\n          --s3SignIn false \\\n          --scratch false \\\n          --output output \\\n          --databases $(pwd)/databases \\\n              --input.paired.path https://raw.githubusercontent.com/metagenomics/metagenomics-tk/refs/heads/master/test_data/fullPipeline/quickstart.tsv\n</code></pre>"},{"location":"database/","title":"Database Configuration","text":""},{"location":"database/#database-input-configuration","title":"Database input configuration","text":"<p>Whenever a database field can be specified as part of the tool configuration (such as in gtdb or checkm), you are able to provide different methods to fetch the database. In all settings, please make sure that the file has the same ending (e.g. .zip, .tar.gz) as specified in the corresponding tool section. In addition, as database names are used to name results with which they were created, said database names should contain the respective database number or date of creation. With this every result can be linked to one exact database version to clarify results.  Except for the <code>extractedDBPath</code> parameter, all other input types (https, s3,...) will download the database to the folder specified in the <code>database</code> parameter.</p>"},{"location":"database/#extracted-database-path","title":"Extracted Database Path","text":"<p>If you have already downloaded and extracted the database, you can specify the path using the <code>extractedDBPath</code> parameter. This setting is available in standard and slurm mode. In slurm mode the path can point to a db on the worker node.</p> <p>Example: <pre><code>database:\n  extractedDBPath: /vol/spool/gtdb/release202\n</code></pre></p>"},{"location":"database/#https-download","title":"HTTPS Download","text":"<p>The toolkit is able to download and extract the database, as long as the file ending equals the one specified in the corresponding tool section (.zip, tar.gz, tar.zst) This setting is available in standard and slurm mode. </p> <p>Example: <pre><code>database:\n  download:\n    source: 'https://openstack.cebitec.uni-bielefeld.de:8080/databases/gtdb.tar.gz'\n    md5sum: 77180f6a02769e7eec6b8c22d3614d2e \n</code></pre></p>"},{"location":"database/#local-file-path","title":"Local File Path","text":"<p>This setting allows you to reuse an already downloaded database. </p> <p>Example: <pre><code>database:\n  download:\n    source: '/vol/spool/gtdb.tar.gz'\n    md5sum: 77180f6a02769e7eec6b8c22d3614d2e \n</code></pre></p>"},{"location":"database/#s3-download","title":"S3 Download","text":"<p>You can specify an S3 link and configure the S3 call via the <code>s5cmd.params parameter. The</code>s5cmd.params` parameter allows you to set any setting available of the s5cmd commandline tool.  If you need credentials to access your databases, you can set them via the Nextflow secrets mechanism. The correct key name for the access and secret key can be found in the corresponding database section.</p> <p>In the following example the compressed file will be downloaded and extracted.</p> <p>Example for publicly available compressed database: <pre><code>database:\n  download:\n    source: 's3://databases/gtdb.tar.gz'\n    md5sum: 77180f6a02769e7eec6b8c22d3614d2e \n    s5cmd:\n      params: '--retry-count 30 --no-sign-request --no-verify-ssl --endpoint-url https://openstack.cebitec.uni-bielefeld.de:8080'\n</code></pre></p> <p>If your database is already extracted and available via S3, you can specify the S3 link using a wildcard as in the next example.</p> <pre><code>database:\n  download:\n    source: 's3://databases/gtdb/*'\n    md5sum: 77180f6a02769e7eec6b8c22d3614d2e \n    s5cmd:\n      params: '--retry-count 30 --no-verify-ssl --endpoint-url https://openstack.cebitec.uni-bielefeld.de:8080'\n</code></pre>"},{"location":"database/#updating-database-md5sums","title":"Updating Database MD5SUMs","text":"<p>The md5sum is computed over all md5sums of all files of the extracted database. If you need to update the md5sum because you updated your database you have to download the database  and run the following command</p> <pre><code>find /path/to/db -type f -exec md5sum {} \\; | sort | cut -d ' ' -f 1 | md5sum | cut -d ' ' -f 1\n</code></pre>"},{"location":"database/#database-download-strategy","title":"Database Download strategy","text":"<p>The toolkit allows to download databases on multiple nodes and tries to synchronize the download process between multiple jobs on a node. However not all possible combinations of profiles and download types are reasonable.</p> PROFILE Download to Shared NFS Download to worker database directory Reuse extracted directory STANDARD SLURM   On a disk local to the worker and nfs directory"},{"location":"developer_guidelines/","title":"Guidelines","text":""},{"location":"developer_guidelines/#commit-and-release-guidelines","title":"Commit and Release Guidelines","text":"<p>We are using git-chglog to automatically generate a changelog for the latest released based on our commit messages. Commit messages should follow the following format:</p> <pre><code>feat(scope): feature added in the scope\n</code></pre> <p>Example:</p> <pre><code>feat(assembly): megahit added\n</code></pre> <p><code>feat</code> can be replaced by one of the formats specified in the options sections of the config file (see example below). Scope can for example represent a module, a configuration or a specific document.</p> <p>A new release should be made the following way: </p> <ol> <li> <p>Checkout the dev branch.</p> </li> <li> <p>Update pipeline version in the nextflow manifest <code>nextflow.config</code>.</p> </li> <li> <p>Push the changes to the dev branch.</p> </li> <li> <p>Checkout the master branch.</p> </li> <li> <p>Merge the dev branch into the master branch. (<code>git merge dev</code>)</p> </li> <li> <p>Push the merged branch to the master branch.</p> </li> <li> <p>Create a release on Github based on the master branch.</p> </li> <li> <p>Run <code>git fetch</code> on the master branch to get the latest tag.</p> </li> <li> <p>Run <code>make changelog</code> and paste the output on the Github release section.</p> </li> <li> <p>Once the Github release is created, publish the wiki with newest tag.(see wiki section)</p> </li> </ol> <pre><code>style: github\ntemplate: CHANGELOG.tpl.md\ninfo:\n  title: CHANGELOG\n  repository_url: https://github.com/pbelmann/meta-omics-toolkit\noptions:\n  commits:\n    filters:\n      Type:\n        - feat\n        - fix\n        - refactor\n        - doc\n  commit_groups:\n    title_maps:\n      feat: Features\n      fix: Bug Fixes\n      refactor: Code Refactoring\n      doc: Documentation\n  header:\n    pattern: \"^(\\\\w*)(?:\\\\(([\\\\w\\\\$\\\\.\\\\-\\\\*\\\\s]*)\\\\))?\\\\:\\\\s(.*)$\"\n    pattern_maps:\n      - Type\n      - Scope\n      - Subject\n</code></pre>"},{"location":"developer_guidelines/#versioning","title":"Versioning","text":"<p>Following semantic versioning, we define the configuration input file and the output folder structure as our public <code>API</code>. Changes to the version numbers reflect updates to the config or the output folders and files. The toolkit consists of many modules that can be used in different combinations and because of this flexibility, we had to come up with a detailed versioning system. We version each module separately, as well as the pipeline itself.</p> <p>Module MAJOR version numbers are updated when a module-specific input parameter is updated or the output folder or file structure is changed. All module version numbers can be retrieved by running the toolkit with the <code>wGetModuleVersion</code> entry point and should be reported on the release page. </p> <p>The module version number is incorporated in the output directory (see output specification)  for easier parsing of the output directory. In the following we give examples when to increment which part of the version identifier:</p> <p>Given a version number MAJOR.MINOR.PATCH, increment the:</p> <ul> <li>MAJOR version when you make incompatible changes, as for example modifying the output structure. A script that was build to parse the output structure must be adapted then.</li> <li>MINOR version when you add functionality in a backward compatible manner. One example is adding an additional tool to the module. </li> <li>PATCH version when you make backwards compatible bug fixes. This is necessary when you for example increment the docker container version number that fixes a bug or increases the     speed of the tool.</li> </ul> <p>The pipeline specific version number defined in the manifest part of the nextflow.config should be changed if either any module specific version number is incremented or any module-independent parameter (e.g. <code>tempdir</code>) or output structure is changed. </p>"},{"location":"developer_guidelines/#testing","title":"Testing","text":"<p>Tests for local use are specified in the <code>scripts</code> folder. These scripts are also used as part of the continuous integration tests. If you want to run these scripts locally, you will have to override the paths to the databases you have downloaded:</p> <p>Examples: <pre><code>bash scripts/test_fullPipeline.sh  \" --steps.magAttributes.checkm.database=/vol/spool/checkm --steps.magAttributes.gtdb.database=/vol/spool/gtdb/release202 \"\nbash scripts/test_fragmentRecruitment.sh  \" --steps.fragmentRecruitment.frhit.genomes=test/bins/small/bin.*.fa --steps.fragmentRecruitment.frhit.samples=test/reads/small/reads.tsv \"\nbash scripts/test_dereplication.sh \"  --steps.dereplication.pasolli.input=test/bins/small/attributes.tsv \"\nbash scripts/test_magAttributes.sh \"  --steps.magAttributes.input=test/bins/small/attributes.tsv \"\n</code></pre></p>"},{"location":"developer_guidelines/#nextflow-versions","title":"Nextflow Versions","text":"<p>The toolkit is tested against the lowest and highest Nextflow version number specified in VERSIONS.txt.</p>"},{"location":"developer_guidelines/#modules","title":"Modules","text":"<p>Functionality is structured in modules (assembly, binning, dereplication, .etc). Each module can have multiple workflows. Every module follows the output definition specified in the output specification  document. The name and the version of the module is specified in the <code>modules</code> section of the <code>nextflow.config</code> file.</p>"},{"location":"developer_guidelines/#workflows","title":"Workflows","text":"<ol> <li> <p>Worfklow names that can not be used directly and are just meant for internal use should start with an underscore.</p> </li> <li> <p>At least every workflow that can be used by other external workflows should contain a short description of the functionality. </p> </li> <li> <p>Workflow names must start with <code>w</code>. </p> </li> </ol>"},{"location":"developer_guidelines/#process","title":"Process","text":"<p>Process names should start <code>p</code>. The in- and output of processes should contain a sample and/or a bin and contig id. Custom error strategies that do not follow the strategy defined in nextflow.config, should be documented (see Megahit example).</p>"},{"location":"developer_guidelines/#processes-should-publish-process-specific-files","title":"Processes should publish process specific files","text":"<p>Processes should publish <code>.command.sh</code>, <code>.command.out</code>, <code>.command.log</code> and <code>.command.err</code> files but never <code>.command.run</code>. In cases where processes process different data but publish it to the same folder these files would be overwritten on every run. For example when Prokka publishes log files of every genome to the same sample directory. For that reason these files need to be renamed, so that their names include a unique id (e.g. bin id).  Please output those files to channel with the following entries and connect this channel to the pDumpLogs process that you can import from the utils module:</p> <pre><code>include { pDumpLogs } from '../utils/processes'\n\n...\n\ntuple env(FILE_ID), val(\"${output}\"), val(params.LOG_LEVELS.INFO), file(\".command.sh\"), \\\n        file(\".command.out\"), file(\".command.err\"), file(\".command.log\"), emit: logs\n</code></pre> <p>Examples can be viewed in the Checkm and Prokka process.</p>"},{"location":"developer_guidelines/#logs","title":"Logs","text":"<p>Log files should be stored in the user provided <code>logDir</code> directory.</p>"},{"location":"developer_guidelines/#log-level","title":"Log Level","text":"<p>Every configuration file must have a <code>logLevel</code> attribute that can have the following values:</p> <pre><code>ALL = 0  All logs are published\nINFO = 1 Just necessary logs are published\n</code></pre> <p>These values can be used in the publish dir directive to enable or disable the output of logs.</p> <pre><code>   publishDir params.output, mode: \"${params.publishDirMode}\", saveAs: { filename -&gt; getOutput(params.runid, \"pasolli/mash/sketch\", filename) }, \\\n        pattern: \"{**.out,**.err, **.sh, **.log}\", enabled: params.logLevel &lt;= params.LOG_LEVELS.ALL\n</code></pre> <p>Furthermore the <code>params.LOG_LEVELS.*</code> parameters can be used inside of a process to enable or disable intermediate results for debugging purposes. In cases where the log is send to the pDumpLogs process (see Process section), you can specify the log level as part of the tuple:</p> <pre><code>tuple env(FILE_ID), val(\"${output}\"), val(params.LOG_LEVELS.INFO), file(\".command.sh\"), \\\n        file(\".command.out\"), file(\".command.err\"), file(\".command.log\"), emit: logs\n</code></pre>"},{"location":"developer_guidelines/#resources","title":"Resources","text":"<p>Usually all processes get a label that is linked to the flavor provided in the resources section of the configuration file (e.g. <code>small</code>, <code>medium</code>, <code>large</code>). In cases where we can foresee that a processes might need more RAM depending on the size of the input data, it is also possible to use the getMemoryResources and getCPUsResources methods provided by the Utils class:</p> <p>Example:</p> <pre><code>memory { Utils.getMemoryResources(params.resources.small, \"${sample}\", task.attempt, params.resources) }\n\ncpus { Utils.getCPUsResources(params.resources.tiny.small, \"${sample}\", task.attempt, params.resources) }\n</code></pre> <p>One example where it might make sense to use this option is when a process uses <code>csvtk join</code> and uses per default a label with little RAM assigned.</p>"},{"location":"developer_guidelines/#time-limit","title":"Time Limit","text":"<p>Every process must define a time limit which will never be reached on \"normal\" execution. This limit is only useful for errors in the execution environment which could lead to an endless execution of the process.</p> <p>You can use the setTimeLimit helper method to add a user configurable time limit.</p> <p>Example:</p> <pre><code>time Utils.setTimeLimit(params.steps.qc.fastp, params.modules.qc.process.fastp.defaults, params.resources.highmemMedium)\n</code></pre>"},{"location":"developer_guidelines/#databases","title":"Databases","text":"<p>If the same database is downloaded during runtime by multiple processes, it takes up an unnecessary ammount of disc space. One idea is too always use the same place to store these databases. This place should be described in <code>params.databases</code>. If other processes try to use this databases they can look at <code>params.databases</code> on the current machine.  If it is present it can be used, if not it should be downloaded. Through this procedure only one copy of each databases is used, which is space-saving. Links to the actual database should contain the database version number or the date of download.</p>"},{"location":"developer_guidelines/#configuration","title":"Configuration","text":"<p>Every process should be configurable by providing a parameters string to the tool in the process. Every module should use the following specification in the configuration file:</p> <pre><code>steps:\n  moduleName:\n    parameter: 42\n    processName:\n      additionalParams: \" --super-flag \"\n      timeLimit: \"AUTO\"\n</code></pre> <p>Please check the process chapter regarding possible values for the time limit attribute. Additional params can have a string value (like the example above) that is provided to the tool:</p> <pre><code>pProcess {\n\n   ...\n\n  shell:\n  \"\"\"\n  supertool !{params.steps.moduleName.processName.parameter}  !{params.steps.moduleName.processName.additionalParams}\n  \"\"\"\n}\n</code></pre> <p>The value of the <code>additionalParams</code> key can also be a map if multiple tools are used in the same process:</p> <pre><code>steps:\n  moduleName:\n    parameter: 42\n    processName:\n      additionalParams:\n         toolNameA: \" -c 84  \"\n         toolNameB: \" --super-flag \"\n</code></pre> <p><code>parameter</code> fields can hold hardcoded parameters that hold a defined value like a number that should not be a string. One use case of those parameters is that they can be reused for multiple tools.</p> <p>Example:</p> <pre><code>pProcess {\n\n   ...\n\n  shell:\n  \"\"\"\n  toolNameA --super-specific-number-flag !{params.steps.moduleName.parameter}\n  toolNameB --similar-flag-to-toolA !{params.steps.moduleName.parameter} \n  \"\"\"\n}\n</code></pre>"},{"location":"developer_guidelines/#internal-configuration","title":"Internal Configuration","text":"<p>The <code>_wConfigurePipeline</code> workflow in the main.nf file should be used for setting  pipeline parameters that are need for fullfilling the user provided configuration.</p> <p>Example: Lets assume the user enables the plasmid module. In that case it is mandatory that  the assembler produces a fastg file independend of the user provided settings of the assembler. In that case the fastg parameter of any assembler will be set to <code>true</code> by the <code>_wConfigurePipeline</code> method.</p>"},{"location":"developer_guidelines/#toolkit-docker-images","title":"Toolkit Docker Images","text":"<p>Dockerfiles of Docker images that are build by toolkit developers can be found in the <code>docker</code> directory. The name of the directory (i.e.: <code>toolkit-python-env</code> in <code>docker/toolkit-python-env</code>) is used for the docker image name. All images belong to the metagenomics quay.io organisation which is owned by the Computational Metagenomics group in Bielefeld. A docker repository in the <code>metagenomics</code> orginsation must be created by the organisation owner, before the actual image can be build. The version of the image specified in the <code>VERSION</code> file (i.e. <code>docker/toolkit-python-env/VERSION</code>) is used for the image tag (<code>metagenomics/toolkit-python-env:VERSION</code>). An image build is only triggered if the version in the VERSION file is updated on the dev or master branch.</p>"},{"location":"developer_guidelines/#wiki","title":"Wiki","text":"<p>For building the documentation we are using mkdocs in combination with mkdocs-material and mike for building versioned wiki pages. Using <code>TOOLKIT_TAG=0.5.0 make wiki_publish</code> the local wiki pages are uploaded to Github pages. Please use semantic versioning for specifying the version number.</p> <p>You can work on these html files locally by running <code>make dev_wiki</code>. But please note that by build the static html file for upload, the navigation might change. You can view the final html file by building the html file (see Makefile <code>make help</code>). </p>"},{"location":"developer_guidelines/#utils","title":"Utils","text":"<p>We do not want to duplicate code and thats why we should store methods in the lib/Utils.groovy file. The Utils class can be used in any module. </p>"},{"location":"developer_guidelines/#database-download","title":"Database Download","text":"<p>This section explains how a developer is able to implement the database download strategy as explained in the user documentation.  Example implementations can be found in the gtdb, checkm or rgi scripts.</p> <p>The first step is to check if the user provides an already extracted database: </p> <pre><code>DB_PATH=\"\"\nif [ -z \"!{EXTRACTED_DB}\" ]\nthen\n   # Fetch user parameter for not extracted db path and run flock (see next section)\n   DB_PATH=\"not extracted\"\nelse\n  # Set variable to extracted db path\nfi\n</code></pre> <p>Since the download is not directly handled by nextflow and paths to the files need to be downloaded, any file or directory must be mounted first to the container. For this reason you have to add the <code>setDockerMount</code> function with the database config as input to  the <code>containerOptions</code> parameter:</p> <pre><code>containerOptions \" other container options \" + setDockerMount(params.steps?.magAttributes?.checkm?.database)\n</code></pre>"},{"location":"developer_guidelines/#filesystem-locks","title":"Filesystem locks","text":"<p>Multiple jobs of the same process (e.g. GTDB) are able to synchronize the download of a database by using filesystem locks. The download is handled by the <code>concurrentDownload.sh</code> script and should be executed the following way:</p> <pre><code>flock LOCK_FILE concurrentDownload.sh --output=DATABASE \\\n           --httpsCommand=COMMAND \\\n           --localCommand=COMMAND \\\n           --s3FileCommand=COMMAND \\\n           --s3DirectoryCommand=COMMAND \\\n           --s5cmdAdditionalParams=S5CMD_PARAMS \\\n           --link=LINK \\\n           --expectedMD5SUM=USER_VERIFIED_DATABASE_MD5SUM\n</code></pre> <p>where   * <code>LOCK_FILE</code> is a file that is used for locking. Processes will check if the file is currently locked before trying to download anything.     This file should ideally placed in the <code>params.database</code> directory of the specific tool (e.g. !{params.databases}/rgi).</p> <ul> <li> <p><code>DATABASE</code> is the directory that is used for placing the specific database.</p> </li> <li> <p><code>COMMAND</code> is the command used to download and extract the database and to remove it afterwards.      (e.g. \"wget -O data.tar.gz $DOWNLOAD_LINK &amp;&amp; tar -xvf data.tar.gz ./card.json &amp;&amp; rm data.tar.gz\" for the <code>--httpsCommand</code> flag)</p> </li> <li> <p><code>USER_VERIFIED_DATABASE_MD5SUM</code> is the MD5SUM of the extracted database that the user should test manually before executing the pipeline.</p> </li> <li> <p><code>S5CMD_PARAMS</code> allows you to set s5cmd specific parameters. For more information check the s5cmd documentation. </p> </li> <li> <p><code>LINK</code> is the link that will be used to test if the file is accessible by S3, HTTPS or is available via a local path.</p> </li> <li> <p><code>USER_VERIFIED_DATABASE_MD5SUM</code> Before a database is downloaded, the script checks the MD5SUM of an already downloaded database against a user specified one.      If it does not equal, the script will download the database again.</p> </li> </ul>"},{"location":"developer_guidelines/#tests","title":"Tests","text":"<p>You can test your tool against different database inputs by using the <code>make runDatabaseTest</code> command. You will have to specify multiple databases  that are accessible via https, S3, local path etc. Please check github actions file for how to run these tests.</p>"},{"location":"developer_guidelines/#polished-variables","title":"Polished Variables","text":"<p>Sometimes user input variables must be polished before they can used in our code. Thats why the nextflow config adds a namespace to the params namespace called <code>polished</code>. For example the params.databases variable must end with a slash in order to be used as part of a docker mount. Thats why there is a variable <code>params.polished.databases</code> that should be used instead.  </p>"},{"location":"developer_guidelines/#other","title":"Other","text":"<ol> <li> <p>Magic numbers should not be used.</p> </li> <li> <p>Variable, method, workflow, folder and process names should be written in camelcase.</p> </li> </ol>"},{"location":"full_pipeline/","title":"Full Pipeline","text":"<p>The full pipeline mode allows you to run the per-sample part and the aggregation part in one execution (see schematic overview). In contrast to the Quickstart section, this chapter refers to the execution of the Toolkit on a cluster system. In this section, you will run the Toolkit with a dataset stored on a remote server and then learn how to replace it with your own local data.  You will then learn how to add additional analyses steps to your pipeline configuration and how to replace the tools in a module with alternative ones.</p>"},{"location":"full_pipeline/#requirements","title":"Requirements","text":"<ol> <li>SLURM: The Toolkit was mainly developed for cloud-based clusters using SLURM as a resource orchestrator.</li> <li>Docker: Install Docker by following the official Docker installation instructions.</li> <li>Java: In order to run Nextflow, you need to install Java on your machine, which can be achieved via <code>sudo apt install default-jre</code>.</li> <li>Nextflow should be installed. Please check the official Nextflow instructions</li> <li>You will need at least 150 GB of scratch space on every worker node. </li> <li>This tutorial requires at least one worker node with 28 cores and 230 GB of RAM.</li> </ol>"},{"location":"full_pipeline/#part-1-run-the-toolkit-with-a-basic-config","title":"Part 1: Run the Toolkit with a basic config","text":"<p>In this section you will learn how to run the Toolkit. The input data will be downloaded automatically. The following Toolkit analyses will be performed: quality control, assembly, binning, taxonomic classification, contamination and completeness of MAGs, and gene prediction and annotation via Prokka.</p> <p>Default Configuration</p> <p>Please note that in the following you will modify our default (best practice) configuration. For most cases you don't need to modify our default configuration, you might only need to remove or add analyses.</p> <pre><code>NXF_HOME=$PWD/.nextflow NXF_VER=25.04.2 nextflow run metagenomics/metagenomics-tk -work-dir $(pwd)/work \\\n    -profile slurm \\\n    -entry wFullPipeline \\\n    -ansi-log false \\\n    -params-file  https://raw.githubusercontent.com/metagenomics/metagenomics-tk/refs/heads/master/default/fullPipeline_illumina_nanpore_getting_started_part1.yml \\\n    --logDir log1 \\\n    --s3SignIn false \\\n    --scratch /vol/scratch \\\n    --databases /vol/scratch/databases \\\n    --output output \\\n    --input.paired.path https://raw.githubusercontent.com/metagenomics/metagenomics-tk/refs/heads/master/test_data/fullPipeline/reads_split.tsv\n</code></pre> <p>where</p> <ul> <li><code>NXF_HOME</code> points to the directory where Nextflow internal files and additional configs are stored. The default location is your home directory.  However, it might be that your home directory is not shared among all worker nodes and is only available on the master node.  In this example  the variable points to your current working directory (<code>$PWD/.nextflow</code>).</li> <li><code>-work-dir</code> points in this example to your current working directory and should point to a directory that is shared between all worker nodes.</li> <li><code>-profile</code> defines the execution profile that should be used (local or cluster computing).</li> <li><code>-entry</code> is the entrypoint of the Toolkit.</li> <li><code>-params-file</code> sets the parameters file which defines the parameters for all tools. (see input section below)</li> <li><code>--logDir</code> points to a directory where your trace TSV, a timeline HTML of the executed processes and a report regarding the resource consumption of the workflow is saved.</li> <li><code>--s3SignIn</code> defines if any S3 login for retrieving inputs is necessary. See the S3 configuration section for more information on how to configure the Toolkit for possible S3 input data.</li> <li><code>--scratch</code> is the directory on the worker node where all intermediate results are saved.</li> <li><code>--databases</code> is the directory on the worker node where all databases are saved. Already downloaded and extracted databases on a shared file system can be configured in the database setting of the corresponding database section in the configuration file.</li> <li><code>--output</code> is the output directory where all results are saved. If you want to know more about which outputs are created, then please refer to the modules section.</li> <li><code>--input.paired.path</code> is the path to a TSV file that lists the datasets that should be processed. Besides paired-end data there are also other input types. Please check the input section.</li> </ul> <p>Parameter override</p> <p>Any parameters defined with a double dash are parameters that override parameters that are already specified in the YAML file.</p>"},{"location":"full_pipeline/#input","title":"Input","text":"<p>Here you can see the actual input TSV and YAML which was used by the previous command and automatically downloaded by Nextflow.  The TSV file only describes the input data, while the YAML file represents the Toolkit configuration.</p> TSV TableConfiguration File <pre><code>SAMPLE  READS1  READS2\ntest1   https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/small/read1_1.fq.gz  https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/small/read2_1.fq.gz\ntest2   https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/small/read1_1.fq.gz  https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/small/read2_1.fq.gz\n</code></pre> <p>Must include the columns <code>SAMPLE</code>, <code>READS1</code> and <code>READS2</code>. <code>SAMPLE</code> must contain unique dataset identifiers without whitespaces or special characters. <code>READS1</code> and <code>READS2</code> are paired reads and can be HTTPS URLs, S3 links or files.</p> <pre><code>tempdir: \"tmp\"\ns3SignIn: false\ninput:\n  paired:\n    path: \"test_data/fullPipeline/reads_split.tsv\"\n    watch: false\noutput: output\nlogDir: log\nrunid: 1\ndatabases: \"/vol/scratch/databases\"\npublishDirMode: \"symlink\"\nlogLevel: 1\nscratch: \"/vol/scratch\"\nsteps:\n  qc:\n    fastp:\n       # For PE data, the adapter sequence auto-detection is disabled by default since the adapters can be trimmed by overlap analysis. However, you can specify --detect_adapter_for_pe to enable it.\n       # For PE data, fastp will run a little slower if you specify the sequence adapters or enable adapter auto-detection, but usually result in a slightly cleaner output, since the overlap analysis may fail due to sequencing errors or adapter dimers.\n       # -q, --qualified_quality_phred       the quality value that a base is qualified. Default 15 means phred quality &gt;=Q15 is qualified.\n       # --cut_front move a sliding window from front (5') to tail, drop the bases in the window if its mean quality is below cut_mean_quality, stop otherwise.\n       # --length_required  reads shorter than length_required will be discarded, default is 15. (int [=15])\n       # PE data, the front/tail trimming settings are given with -f, --trim_front1 and -t, --trim_tail1\n       additionalParams:\n         fastp: \" --detect_adapter_for_pe -q 20 --cut_front --trim_front1 3 --cut_tail --trim_tail1 3 --cut_mean_quality 10 --length_required 50 \"\n         reportOnly: false \n       timeLimit: \"AUTO\"\n    nonpareil:\n      additionalParams: \" -v 10 -r 1234 \"\n    kmc:\n      timeLimit: \"AUTO\"\n      additionalParams:\n        # Computes k-mer distribution based on k-mer length 13 and 21\n        #  -sm - use strict memory mode (memory limit from -m&lt;n&gt; switch will not be exceeded)\n        #  -cs&lt;value&gt; - maximal value of a counter\n        count: \" -sm -cs10000 \"\n        histo: \" -cx50000 \"\n\n  qcONT:\n    porechop:\n       additionalParams:\n         # Input files are split into chunks because of RAM issues\n         chunkSize: 450000\n         porechop: \"\"\n        # --keep_percent Throw out the worst 10% of reads. This is measured by bp, not by read count. So this option throws out the worst 10% of read bases. \n        # \n         filtlong: \" --min_length 1000  --keep_percent 90 \"\n    nanoplot:\n      additionalParams: \"\"\n  assembly:\n    megahit:\n      # --mem-flag 0 to use minimum memory, --mem-flag 1 (default) moderate memory and --mem-flag 2 all memory.\n      # meta-sensitive: '--min-count 1 --k-list 21,29,39,49,...,129,141' \n      # meta-large:  '--k-min  27  --k-max 127 --k-step 10' (large &amp; complex metagenomes, like soil)\n      additionalParams: \" --min-contig-len 1000 --presets meta-sensitive \"\n      fastg: true\n      resources:\n        RAM:\n          mode: 'PREDICT'\n          predictMinLabel: 'medium'\n  assemblyONT:\n    metaflye:\n      additionalParams: \" -i 1 \"\n      quality: \"AUTO\"\n  binning:\n    bwa2:\n      additionalParams: \n        bwa2: \" \"\n        # samtools flags are used to filter resulting bam file\n        samtoolsView: \" -F 3584 \" \n    contigsCoverage:\n      additionalParams: \" --min-covered-fraction 0 --min-read-percent-identity 100 --min-read-aligned-percent 100 \"\n    genomeCoverage:\n      additionalParams: \" --min-covered-fraction 0 --min-read-percent-identity 100 --min-read-aligned-percent 100 \"\n    # Primary binning tool\n    metabat:\n      # Set --seed positive numbers to reproduce the result exactly. Otherwise, random seed will be set each time.\n      additionalParams: \" --seed 234234  \"\n    # Secondary binning tool for use with MAGscot\n  binningONT:\n    minimap:\n      additionalParams: \n        minimap: \" \"\n        # samtools flags are used to filter resulting bam file\n        samtoolsView: \" -F 3584 \" \n    contigsCoverage:\n      additionalParams: \" --min-covered-fraction 0  --min-read-aligned-percent 100 \"\n    genomeCoverage:\n      additionalParams: \" --min-covered-fraction 0  --min-read-aligned-percent 100 \"\n    metabat:\n      additionalParams: \" --seed 234234  \"\n  magAttributes:\n    # gtdbtk classify_wf\n    # --min_af minimum alignment fraction to assign genome to a species cluster (0.5)\n    gtdb:\n      buffer: 1000\n      database:\n        download:\n          source: https://openstack.cebitec.uni-bielefeld.de:8080/databases/gtdbtk_r226_v2_data.tar.gz\n          md5sum: 0777ea44f9d7d96f9ca6ea43ed016799\n      additionalParams: \" --min_af 0.65 --scratch_dir . \"\n    checkm2:\n      database:\n        download:\n          source: \"https://openstack.cebitec.uni-bielefeld.de:8080/databases/checkm2_v2.tar.gz\"\n          md5sum: a634cb3d31a1f56f2912b74005f25f09\n      additionalParams: \"  \"\n  annotation:\n    prokka:\n      defaultKingdom: false\n      additionalParams: \" --mincontiglen 500 \"\n    whokaryote:\n      additionalParams: \" --minsize 3000 \"\nresources:\n  highmemLarge:\n    cpus: 28\n    memory: 230\n  highmemMedium:\n    cpus: 14\n    memory: 113\n  large:\n    cpus: 28\n    memory: 58\n  medium:\n    cpus: 14\n    memory: 29\n  small:\n    cpus: 7\n    memory: 14\n  tiny:\n    cpus: 1\n    memory: 1\n</code></pre>"},{"location":"full_pipeline/#output","title":"Output","text":"<p>You can read more about the produced output on the respective modules page.</p>"},{"location":"full_pipeline/#part-2-run-the-toolkit-with-your-own-data","title":"Part 2: Run the Toolkit with your own data","text":"<p>Now that you have been able to run the Toolkit on your system, let's use the same configuration, but with your own data that may already be  locally available . We will simulate the provisioning of local files by first downloading sample paired-end FASTQ files (size: 1.2 GB).  Please note that these are the same fastq files as in the previous part. The only difference to previous part is that you will download the data beforehand. </p> <pre><code>mkdir inputFiles\nwget -P inputFiles https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/small/read1_1.fq.gz \nwget -P inputFiles https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/small/read2_1.fq.gz \n</code></pre> <p>In addition, you have to tell the Toolkit the name of the sample and the path to the files that are the subject of the analysis. For this reason, we will create a file that contains three columns: sample, path to left reads, and path to right reads.</p> <pre><code>INPUT_FILES=$(pwd)/inputFiles/input.tsv\necho -e \"SAMPLE\\tREADS1\\tREADS2\" &gt; $INPUT_FILES\necho -e \"MYDATA\\t$(readlink -f inputFiles/read1*)\\t$(readlink -f inputFiles/read2*)\" &gt;&gt; $INPUT_FILES\n</code></pre> <p>Now that the files are created, you are ready to execute the Toolkit on your data but with a modified command of the first part. The only difference is that you modify the <code>--input.paired.path</code> variable.</p> <pre><code>NXF_HOME=$PWD/.nextflow NXF_VER=25.04.2 nextflow run metagenomics/metagenomics-tk -work-dir $(pwd)/work \\\n    -profile slurm \\\n    -entry wFullPipeline \\\n    -ansi-log false \\\n    -params-file  https://raw.githubusercontent.com/metagenomics/metagenomics-tk/refs/heads/master/default/fullPipeline_illumina_nanpore_getting_started_part1.yml \\\n    --logDir log2 \\\n    --s3SignIn false \\\n    --scratch /vol/scratch \\\n    --databases /vol/scratch/databases \\\n    --output my_data_output \\\n    --input.paired.path inputFiles/input.tsv\n</code></pre> <p>Now you can go through the <code>my_data_output</code> folder and check the results. The next section describes how to modify the analysis that was performed.</p>"},{"location":"full_pipeline/#part-3-exchange-tools-of-a-module","title":"Part 3: Exchange tools of a module","text":"<p>In some cases, you may also be interested in replacing a tool from one module with another. For example, you might be interested in comparing the assembler that is set as default with another one like MetaSpades.</p> <p>In this case you could replace the MEGAHIT part with the MetaSpades config.</p> <pre><code>NXF_HOME=$PWD/.nextflow NXF_VER=25.04.2 nextflow run metagenomics/metagenomics-tk -work-dir $(pwd)/work \\\n    -profile slurm \\\n    -ansi-log false \\\n    -entry wFullPipeline \\\n    -params-file  https://raw.githubusercontent.com/metagenomics/metagenomics-tk/refs/heads/master/default/fullPipeline_illumina_nanpore_getting_started_part3.yml \\\n    --logDir log3 \\\n    --s3SignIn false \\\n    --scratch /vol/scratch \\\n    --databases /vol/scratch/databases \\\n    --output my_data_spades_output \\\n    --input.paired.path inputFiles/input.tsv\n</code></pre> <p>This is the MetaSpades part that was used by the previous command instead of the MEGAHIT configuration:</p> <pre><code>  assembly:\n    metaspades:\n      additionalParams: \"  \"\n      fastg: true\n</code></pre> <p>If you now compare the contigs of the two assemblers with the following command,  you will notice that the MetaSpades assembly has a higher N50 than the MEGAHIT one. </p> <pre><code>cat my_data_spades_output/MYDATA/1/assembly/*/metaspades/MYDATA_contigs_stats.tsv\n#SAMPLE  file                    format  type    num_seqs  sum_len    min_len avg_len max_len Q1      Q2      Q3      sum_gap N50     Q20(%)  Q30(%)  GC(%)\n#MYDATA  MYDATA_contigs.fa.gz    FASTA   DNA     1761      4521427    1000    2567.5  26470   1149.0  1408.0  2119.0  0       3799    0.00    0.00    55.07\n\ncat my_data_output/MYDATA/1/assembly/*/megahit/MYDATA_contigs_stats.tsv\n#SAMPLE  file                    format  type    num_seqs  sum_len    min_len avg_len max_len Q1      Q2      Q3      sum_gap N50     Q20(%)  Q30(%)  GC(%)\n#MYDATA  MYDATA_contigs.fa.gz    FASTA   DNA     95227     60110517   56      631.2   346664  234.0   286.0   439.0   0       1229    0.00    0.00    57.34\n</code></pre>"},{"location":"full_pipeline/#part-4-add-further-analyses","title":"Part 4: Add further analyses","text":"<p>Now, suppose you also want to check the presence of your genes in other databases. With the help of the Toolkit, you could create your own database with the syntax as described  in the wiki. In this example, we will use the bacmet database. What you need to do here is to add the following part to the annotation section of the Toolkit configuration.</p> <p>The bacmet database snippet is the following:</p> <pre><code>    whokaryote:\n      additionalParams: \" --minsize 3000 \"\n    mmseqs2:\n      chunkSize: 20000\n      bacmet20_experimental:\n        additionalParams:\n          search : ' --max-seqs 300 --max-accept 50 -c 0.8 --cov-mode 0 --start-sens 4 --sens-steps 1 -s 6 --num-iterations 2 -e 0.001 --e-profile 0.01 --db-load-mode 3 '\n          additionalColumns: \"\"\n        database:\n          download:\n</code></pre> <p>By re-running the Toolkit with this configuration, you will see that the previous results were cached (see next snippet) and only the annotation part is re-executed.</p> <pre><code>...\n[a7/d8e782] Cached process &gt; wFullPipeline:_wProcessIllumina:wShortReadBinningList:_wBinning:pMetabat (MYDATA)\n[c3/537cbc] Cached process &gt; wFullPipeline:_wProcessIllumina:wShortReadBinningList:_wBinning:pCovermContigsCoverage (Sample: MYDATA)\n[bb/fcd1aa] Cached process &gt; wFullPipeline:_wProcessIllumina:wShortReadBinningList:_wBinning:pCovermGenomeCoverage (Sample: MYDATA)\n[8c/08e157] Cached process &gt; wFullPipeline:wMagAttributesList:_wMagAttributes:pGtdbtk (Sample: MYDATA)\n[4b/286c49] Cached process &gt; wFullPipeline:wMagAttributesList:_wMagAttributes:pCheckM2 (Sample: MYDATA)\n...\n</code></pre>"},{"location":"full_pipeline/#further-reading","title":"Further Reading","text":"<ul> <li> <p>Continue to the aggregation part of the Getting Started tutorial to learn how to aggregate data of multiple samples.</p> </li> <li> <p>You can check in our configuration section for how to further adapt the Toolkit to your infrastructure.</p> </li> <li> <p>In case you want to import the output to EMGB, please visit the EMGB section.</p> </li> <li> <p>If you want to add your own sequence databases, you can read here how to do this.</p> </li> <li> <p>You might want to adjust the resource requirements of the Toolkit to your infrastructure.</p> </li> </ul>"},{"location":"module_specification/","title":"Module Specification","text":""},{"location":"module_specification/#assembly","title":"Assembly","text":"<ul> <li>Version: 0.2.0</li> </ul>"},{"location":"module_specification/#output","title":"Output:","text":"<p>Assembly file names must fulfill the following name pattern:</p> <pre><code>SAMPLENAME_contigs.fa.gz\n</code></pre> <p>Contig names must be renamed according to the following pattern:</p> <p><code>SAMPLEID_SEQUENCECOUNTER_SEQUENCEHASH</code></p> <p>where</p> <ul> <li> <p><code>SAMPLEID</code> is the name of the dataset (e.g: <code>SRR234235</code>)</p> </li> <li> <p><code>SEQUENCECOUNTER</code> is the counter of the contig entry in the fasta file (e.g: 2)</p> </li> <li> <p><code>SEQUENCEHASH</code> are the last 5 characters of an md5sum hash of the fasta entry without the header and newline character.       (eg. echo -n \"ACGT\" | md5sum | cut -d ' ' -f 1 | cut -c -5 )</p> </li> </ul>"},{"location":"module_specification/#binning","title":"Binning","text":"<ul> <li>Version: 0.5.0</li> </ul>"},{"location":"module_specification/#output_1","title":"Output:","text":"<p>Binning file names must fulfill the following name pattern:</p> <pre><code>SAMPLENAME_bin.NUMBER.fa\n</code></pre> <p>Where <code>NUMBER</code> is a unique identifier per SAMPLE.</p> <p>Contig names must be renamed according to the following pattern:</p> <p><code>SAMPLEID_SEQUENCECOUNTER_SEQUENCEHASH MAG=BINNUMBER</code></p> <p>where</p> <ul> <li> <p><code>SAMPLEID</code>, <code>SEQUENCECOUNTER</code> and <code>SEQUENCEHASH</code>  definitions can be inspected in the assembly specification.</p> </li> <li> <p><code>BINNUMBER</code> is an unique identifier per SAMPLE.</p> </li> </ul>"},{"location":"module_specification/#mag-attributes","title":"MAG Attributes","text":"<ul> <li>Version: 0.1.0</li> </ul>"},{"location":"module_specification/#output_2","title":"Output:","text":"<pre><code>SAMPLENAME_TOOLNAME_CHUNK.tsv\n</code></pre> <p>where  * <code>TOOLNAME</code> could be for example <code>checkm</code>, <code>gtdb</code> etc.  * <code>CHUNK</code> is a random identifier that produces values for one part of all MAGs of a given sample.</p>"},{"location":"module_specification/#format","title":"FORMAT","text":"<p>The header line specifies the following columns: </p> <pre><code>BIN_ID  SAMPLE  BIN_ATTRIBUTE1  BIN_ATTRIBUTE2 ...\n</code></pre> <p>where    * <code>BIN_ID</code> is unique for the samples.   * <code>BIN_ATTRIBUTES</code> All column names that are not <code>BIN_ID</code> or <code>SAMPLE</code> can be any property of a MAG, like contamination, completeness etc.</p>"},{"location":"module_specification/#quality-control","title":"Quality Control","text":"<ul> <li>Version: 0.1.0</li> </ul>"},{"location":"module_specification/#output_3","title":"Output:","text":"<pre><code>SAMPLE_interleaved.qc.fq.gz\n</code></pre>"},{"location":"overview/","title":"Overview","text":""},{"location":"overview/#contents","title":"Contents","text":"<p>The Getting Started tutorial explains the first steps in running the Toolkit on a cloud-based cluster system. It consists of two main parts. </p> <ol> <li>In the first part you will learn how to run and configure the Toolkit.</li> <li>The second part tells you how to aggregate multiple samples of a Toolkit output.</li> </ol> <p>Other parameters can be found in the configuration section.</p>"},{"location":"overview/#full-pipeline-vs-separate-modules","title":"Full Pipeline vs. Separate Modules","text":"<p>The Metagenomics-Toolkit allows you to run either the full pipeline of assembly, binning and many other downstream analysis tasks or the individual modules. The Toolkit can be configured by providing the module configuration via a YAML file and a flag for the corresponding module or full pipeline mode. Options for the global pipeline configuration can be viewed here.</p> <p>The Full Pipeline mode consists mainly of two parts. One part (per-sample) of the Toolkit processes each dataset individually  by applying analyses such as quality control, assembly, binning and annotation. The second part aggregates and combines the results of the individual datasets by applying modules such as dereplication and co-occurrence analysis. You can either run the first and second part consecutively or separately where the second part can be applied on the output of the first  part.</p> <p>Sensitive data</p> <p>Please do never place sensitive information in any of the YAML configuration files since the configuration is part of the pipeline output.</p>"},{"location":"overview/#error-strategy","title":"Error Strategy","text":"<p>All tools follow the same error strategy. The execution of a tool is retried three times. If the run fails the fourth time, it will be ignored. If the execution is ignored, the Toolkit will continue to run all tools that do not depend on the output of the failed tool run. Exceptions of this handling are specified in the corresponding module section.</p>"},{"location":"pipeline_input/","title":"Pipeline Input Configuration","text":""},{"location":"pipeline_input/#configuration-of-input-parameters-of-the-full-pipeline-mode","title":"Configuration of input parameters of the full pipeline mode","text":""},{"location":"pipeline_input/#paired-end-input","title":"Paired End Input","text":"<p>The input should be a path to a tsv file containing a sample id, as well as a path to the left and right read.</p> <p>Example: <pre><code>input:\n  paired:\n    path: \"test_data/fullPipeline/reads_split.tsv\"\n</code></pre></p>"},{"location":"pipeline_input/#nanopore-input","title":"Nanopore Input","text":"<p>For Nanopore data a seperate input file should be specified.</p> <pre><code>input:\n  ont:\n    path: \"test_data/fullPipeline/ont.tsv\"\n</code></pre>"},{"location":"pipeline_input/#generic-sra","title":"Generic SRA","text":"<p>The toolkit is able to fetch fastq files based on SRA run accession ids from the NCBI or from a mirror based on S3:</p> <pre><code>input:\n  SRA:\n    pattern:\n      ont: \".+[^(_1|_2)].+$\"\n      illumina: \".+(_1|_2).+$\"\n    S3:\n      path: test_data/SRA/samples.tsv \n      bucket: \"s3://ftp.era.ebi.ac.uk\" \n      prefix: \"/vol1/fastq/\"\n      watch: false\n      patternONT: \".+[^(_1|_2)].+$\"\n      patternIllumina: \".+(_1|_2).+$\"\n</code></pre> <p>where:   * <code>path</code> is the path to a file containing a column with <code>ACCESSION</code> as header. The <code>ACCESSION</code> column contains either SRA run or study accessions.</p> <ul> <li> <p><code>bucket</code> is the S3 Bucket hosting the data.</p> </li> <li> <p><code>prefix</code> is the path to the actual SRA datasets.</p> </li> <li> <p><code>watch</code> if true, the file specified with the <code>path</code> attribute is watched and every time a new SRA run id is      appended, the pipeline is triggered. The pipeline will never finish in this mode. Please note that watch currently only works      if only one input type is specified (e.g \"ont\" or \"paired\" ...)</p> </li> <li> <p><code>patternONT</code> and <code>patternIllumina</code> are patterns that are applied on the specified mirror in order to select the correct input files.</p> </li> </ul>"},{"location":"pipeline_input/#ncbi-sra","title":"NCBI SRA","text":"<p>With the following mode SRA datasets can directly be fetched from SRA.</p> <pre><code>input:\n  SRA:\n    pattern:\n      ont: \".+[^(_1|_2)].+$\"\n      illumina: \".+(_1|_2).+$\"\n    NCBI:\n      path: test_data/SRA/samples.tsv\n</code></pre>"},{"location":"pipeline_specification/","title":"Pipeline Specification","text":""},{"location":"pipeline_specification/#output-and-best-practice","title":"Output and best practice","text":""},{"location":"pipeline_specification/#motivation","title":"Motivation","text":"<ul> <li> <p>The output section is a collection of <code>best practices</code> for storing results of the <code>meta-omics-toolkit</code> output. The definitions are motivated by the fact that the pipeline will be continuously updated and results of different pipeline versions and modes must be differentiated.</p> </li> <li> <p>The idea is to run the pipeline on results of previous runs.</p> </li> </ul>"},{"location":"pipeline_specification/#rules-for-dataset-output","title":"Rules for dataset output","text":"<p>Outputs are produced by using the <code>publish dir</code> directive.</p> <p><pre><code>DATASET_ID/RUN_ID/MODULE/VERSION/TOOL/\n</code></pre> where    * <code>DATASET_ID</code> specifies the ID of a dataset such as the SRA run ID.    * <code>RUN_ID</code> specifies one possible run of the full or partial pipeline. The <code>RUN_ID</code> identifier can be any user provided identifier to keep track of multiple pipeline runs.    * <code>MODULE</code> specifies the name of the pipeline module (e.g. binning).    * <code>VERSION</code> specifies the module version number which follows semantic versioning (1.2.0).    * <code>TOOL</code> specifies the name of the tool that is executed as part of the module (e.g <code>megahit</code> of the assembly module).</p> <p>It is suggested that a RUN_ID output should never contain multiple versions of the same module. E.g.:  <code>DATASET_ID/1/Binning/1.2.0/metabat</code> and <code>DATASET_ID/1/Binning/1.3.0/metabat</code>.</p> <p>If a partial pipeline run (B) uses outputs of a previous run (A) (e.g. a binning tool uses the output of an assembler) and the previous run (A) alreads contains the output of an older version of run (B), then a new RUN_ID folder must be created.</p> <p>If a partial pipeline run (B) uses outputs of a previous run (A) (e.g. a binning tool uses the output of an assembler) and the previous run (A) does not contain the output of an older version of run (B), then the existing RUN_ID folder must be reused.</p>"},{"location":"pipeline_specification/#run-versioning","title":"Run Versioning","text":"<p>Every dataset must contain a <code>TOOL</code> folder called <code>config</code>. The <code>config</code> folder contains descriptions of the parameters and the version used for the specific pipeline run.</p>"},{"location":"pipeline_specification/#examples","title":"Examples","text":""},{"location":"pipeline_specification/#example-1","title":"Example 1:","text":"<p>We assume that the following folder already exists:</p> <pre><code>/SRA1/1/ASSEMBLY/1.2/MEGAHIT\n</code></pre> <p>If the MODULE output does not contain a BINNING output then the existing RUN folder must be reused:</p> <pre><code>/SRA1/1/ASSEMBLY/1.2/MEGAHIT\n/SRA1/1/BINNING/0.3/METABAT\n</code></pre>"},{"location":"pipeline_specification/#example-2","title":"Example 2:","text":"<p>We assume that the following folders already exists:</p> <pre><code>/SRA1/1/ASSEMBLY/1.2/MEGAHIT\n/SRA1/1/BINNING/0.3/METABAT\n</code></pre> <p>If the MODULE output does contain a BINNING output then a new RUN folder must be created:</p> <pre><code>/SRA1/1/ASSEMBLY/1.2/MEGAHIT\n/SRA1/1/BINNING/0.3/METABAT\n/SRA1/2/BINNING/0.4/METABAT\n</code></pre>"},{"location":"pipeline_specification/#rules-for-aggregated-output","title":"Rules for aggregated output","text":"<p>Aggregates outputs in the same way as dataset outputs are produced, by using the publish dir directive with the only difference that no sample identifier are used and the path starts with <code>AGGREGATED</code>.</p> <p>Example:</p> <pre><code>AGGREGATED/RUN_ID/MODULE/VERSION/TOOL/\n</code></pre>"},{"location":"quickstart/","title":"Quickstart","text":"<p>This quickstart allows you to run a subset of the available tools implemented by the Toolkit on a machine to process two datasets. This tutorial has mainly been tested on a machine with 29 GB of RAM and 14 cores running an Ubuntu operating system.  You will need at least 250 GB of disk space. The disk were your docker images and containers are created has to be at least 17 GB.</p>"},{"location":"quickstart/#requirements","title":"Requirements","text":"<ol> <li>Docker: Install Docker by following the official Docker installation instructions.</li> <li>Java: In order to run Nextflow, you need to install Java on your machine, which can be achieved via <code>sudo apt install default-jre</code>.</li> <li>Nextflow should be installed. Please check the official Nextflow instructions</li> </ol>"},{"location":"quickstart/#run-the-toolkit","title":"Run the Toolkit","text":"<p>The following command will start a subset of all available modules offered by the Toolkit.  All databases will be downloaded to the database directory in your current working directory.</p> <pre><code>NXF_VER=25.04.2 nextflow run metagenomics/metagenomics-tk \\\n      -profile standard \\\n      -entry wFullPipeline \\\n          -params-file  https://raw.githubusercontent.com/metagenomics/metagenomics-tk/refs/heads/master/default/quickstart.yml \\\n      --logDir logs \\\n      --s3SignIn false \\\n      --scratch false \\\n      --output output \\\n      --databases $(pwd)/databases \\\n          --input.paired.path https://raw.githubusercontent.com/metagenomics/metagenomics-tk/refs/heads/master/test_data/fullPipeline/quickstart.tsv\n</code></pre> <p>You can read more about the outputs, which are placed in a directory named <code>output</code>, in the corresponding Modules sections.</p>"},{"location":"quickstart/#further-reading","title":"Further Reading","text":"<ul> <li> <p>If you want to configure, add or remove modules, please check the configuration section and  check the Getting Started section for an example.</p> </li> <li> <p>If you want to use your own datasets, then you can read the input configuration sections.  You can check the Getting Started section for an example.</p> </li> <li> <p>In case you want to directly scale out your workflow on a cluster, you should continue with the Getting Started section.</p> </li> </ul>"},{"location":"modules/annotation/","title":"Annotation","text":"<p>The annotation module is able to predict and annotate genes based on Prokka and a set of user provided databases. Contigs with annotated genes are checked with Whokaryote to determine whether they are from a eukaryote or from a prokaryote. A user can add additional formatted databases as part of the configuration by adding a key (Example: <code>kegg</code> ) with  a possible download strategy. See the database section for possible download strategies. In addition, the resistance gene identifier is executed by default.</p>"},{"location":"modules/annotation/#input","title":"Input","text":"CommandConfiguration FileTSV Table <pre><code>-entry wAnnotateLocal -params-file example_params/annotation.yml\n</code></pre> <p>Warning</p> <p>The configuration file shown here is for demonstration and testing purposes only.    Parameters that should be used in production can be viewed in the annotation section    of one of the yaml files located in the <code>default</code> folder of the Toolkit's Github repository.</p> <pre><code>output: \"output\"\nrunid: 1\ns3SignIn: false\nlogDir: log\ntempdir: \"tmp\"\nscratch: \"/vol/scratch\"\ndatabases: \"/mnt/databases\"\npublishDirMode: \"symlink\"\nsteps:\n   annotation:\n      input: \"test_data/annotation/input_small.tsv\"\n      mmseqs2:\n        chunkSize: 20000\n        kegg:\n          additionalParams:\n            search : ' -s 1 --max-seqs 100 --max-accept 50 --alignment-mode 1 --exact-kmer-matching 1 --db-load-mode 3'\n            additionalColumns: \"\"\n          database:\n            extractedDBPath: '/vol/spool/toolkit/kegg-mirror-202506_mmseqs/keggDB'\n#        bacmet20_experimental:\n#          params: ' -s 1 --exact-kmer-matching 1 --db-load-mode 3'\n#          database:\n#            download:\n#              source: https://openstack.cebitec.uni-bielefeld.de:8080/databases/bacmet20_experimental.tar.zst\n#              md5sum: 57a6d328486f0acd63f7e984f739e8fe\n        bacmet20_predicted:\n          database:\n            download:\n              source: https://openstack.cebitec.uni-bielefeld.de:8080/databases/bacmet20_predicted.tar.zst\n              md5sum: 55902401a765fc460c09994d839d9b64\n          additionalParams:\n            search : ' -s 1 --max-seqs 100 --max-accept 50 --alignment-mode 1 --exact-kmer-matching 1 --db-load-mode 3'\n            additionalColumns: \"\"\n\n#        vfdb:\n#          params: ' -s 1 --max-seqs 100 --max-accept 50 --alignment-mode 1 --exact-kmer-matching 1 --db-load-mode 3'\n#          database:\n#            download:\n#              source: https://openstack.cebitec.uni-bielefeld.de:8080/databases/vfdb_full_20250620.tar.zst\n#              md5sum: f2ab1259b36fa906ff91f2575e8ee442\n      keggFromMMseqs2:\n         database:\n           extractedDBPath: \"/vol/spool/toolkit/kegg-links-mirror-202506/\"\n      rgi: \n        database:\n          download:\n            source: https://openstack.cebitec.uni-bielefeld.de:8080/databases/card_broadstreet-v4.0.1.tar.bz2\n            md5sum: 2396eaf95f2d35c58c135f7db2211dbb\n        additionalParams: \"\"\n      mmseqs2_taxonomy:\n        runOnMAGs: false\n        gtdb:\n          params: ' --orf-filter-s 1 -e 1e-15'\n          ramMode: false\n          initialMaxSensitivity: 1\n          database:\n            download:\n              source: https://openstack.cebitec.uni-bielefeld.de:8080/databases/gtdb_r226_mmseqs_taxonomy.tar.gz\n              md5sum: a1d77c776a16666ad97ce4945e5e7a35\n      prokka:\n        prodigalMode: \"meta\"\n        defaultKingdom: false\n        additionalParams: \" --mincontiglen 200 \"\n      whokaryote:\n        additionalParams: \" --minsize 3000 \"\nresources:\n  highmemLarge:\n    cpus: 28\n    memory: 230\n  highmemMedium:\n    cpus: 14\n    memory: 113\n  large:\n    cpus: 28\n    memory: 58\n  medium:\n    cpus: 14\n    memory: 29\n  small:\n    cpus: 7\n    memory: 14\n  tiny:\n    cpus: 1\n    memory: 1\n</code></pre> <pre><code>DATASET BIN_ID  PATH\ntest3   bin.1   https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/bins/bin.1.fa\ntest1   bin.2   https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/bins/bin.2.fa\ntest1   bin.8   https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/bins/bin.8.fasta\ntest2   bin.9   https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/bins/bin.9.fasta\ntest2   bin.32  https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/bins/bin.32.fa\n</code></pre>"},{"location":"modules/annotation/#databases","title":"Databases","text":""},{"location":"modules/annotation/#mmseqs2","title":"MMseqs2","text":"<p>MMseqs2 needs a combination of different data, index and dbtype files as \"one\" database, be it in- or output. See MMseqs2 database for more information. As multiple and in most cases, big files are used, tar and zstd are utilized to compress and transport files. Input databases have to be compressed by these and need to end with <code>.tar.zst</code>. Naming inside an archive is irrelevant, as databases are picked automatically. Multiple databases per one archive are not supported, one archive, one database. If the database also includes a taxonomy  as described here, it can also be used for taxonomic classifications with MMseqs2 - Taxonomy. See database section for possible download strategies. If you need credentials to access your files via S3 then please use the following command:</p> <pre><code>nextflow secrets set S3_db_ACCESS XXXXXXX\nnextflow secrets set S3_db_SECRET XXXXXXX\n</code></pre> <p>where <code>db</code> is the name of the database that you use in your config file. Example:</p> <pre><code>....\n      vfdb:\n        params: ' -s 1 --max-seqs 100 --max-accept 50 --alignment-mode 1 --exact-kmer-matching 1 --db-load-mode 3'\n        database:\n          download:\n            source: s3://databases/vfdb_full_2022_07_29.tar.zst\n            md5sum: 7e32aaed112d6e056fb8764b637bf49e\n            s5cmd:\n              params: \" --retry-count 30 --endpoint-url https://openstack.cebitec.uni-bielefeld.de:8080 \" \n....\n</code></pre> <p>Based on these settings, you would set the following secret:</p> <pre><code>nextflow secrets set S3_vfdb_ACCESS XXXXXXX\nnextflow secrets set S3_vfdb_SECRET XXXXXXX\n</code></pre>"},{"location":"modules/annotation/#keggfromblast","title":"KEGGFromBlast","text":"<p>KeGGFromBlast is only executed if genes are searched against a KEGG database. There must be a <code>kegg</code> identifier (see example configuration file) in the annotation section. KeGGFromBlast needs a kegg database as input which must be a tar.gz file. See database section for possible download strategies. If you need credentials to access your files via S3 then please use the following command:</p> <pre><code>nextflow secrets set S3_kegg_ACCESS XXXXXXX\nnextflow secrets set S3_kegg_SECRET XXXXXXX\n</code></pre>"},{"location":"modules/annotation/#mmseqs-taxonomy","title":"MMSeqs Taxonomy","text":"<p>If you need credentials to access your files via S3 then please use the following command:</p> <pre><code>nextflow secrets set S3_TAX_db_ACCESS XXXXXXX\nnextflow secrets set S3_TAX_db_SECRET XXXXXXX\n</code></pre> <p>where <code>db</code> is the name of the database that you use in your config file. Example:</p> <pre><code>....\n    mmseqs2_taxonomy:\n      gtdb:\n        params: ' --orf-filter-s 1 -e 1e-15'\n        ramMode: false\n        database:\n          download:\n            source: s3://databases/gtdb_r214_1_mmseqs.tar.gz\n            md5sum: 3c8f12c5c2dc55841a14dd30a0a4c718\n            s5cmd:\n              params: \" --retry-count 30 --endpoint-url https://openstack.cebitec.uni-bielefeld.de:8080 \" \n....\n</code></pre> <p>Based on these settings, you would set the following secrets:</p> <pre><code>nextflow secrets set S3_TAX_gtdb_ACCESS XXXXXXX\nnextflow secrets set S3_TAX_gtdb_SECRET XXXXXXX\n</code></pre>"},{"location":"modules/annotation/#rgi","title":"RGI","text":"<p>RGI needs a CARD database which can be fetched via this link:  https://card.mcmaster.ca/latest/data. The compressed database must be a tar.bz2 file.  See database section for possible download strategies. If you need credentials to access your files via S3 then please use the following command:</p> <pre><code>nextflow secrets set S3_rgi_ACCESS XXXXXXX\nnextflow secrets set S3_rgi_SECRET XXXXXXX\n</code></pre>"},{"location":"modules/annotation/#output","title":"Output","text":""},{"location":"modules/annotation/#mmseqs2_1","title":"MMseqs2","text":"<p>Calculated significant matches of a nucleotide/protein query which was compared against a user provided set of databases.</p>"},{"location":"modules/annotation/#mmseqs2-taxonomy","title":"MMseqs2 - Taxonomy","text":"<p>By identifying homologous through searches against a provided MMseqs2 taxonomy-database, MMseqs2 can compute the lowest common ancestor.  This lowest common ancestor is a robust taxonomic label for unknown sequences. These labels are presented in form of an <code>*.taxonomy.tsv</code> file, a <code>*.krakenStyleTaxonomy.out</code> formatted in accordance to the KRAKEN tool outputs and an interactive KRONA plot in form of a html website <code>*.krona.html</code>.</p>"},{"location":"modules/annotation/#prokka","title":"Prokka","text":"<p>Prokka computes <code>*.err</code>, <code>*.faa</code>, <code>*.ffn</code>, <code>*.fna</code>, <code>*.fsa</code>, <code>*.gbk</code>, <code>*.gff</code>, <code>*.sqn</code>, <code>*.tbl</code>, <code>*.tbl</code> for every bin. For <code>*.gff</code> we differentiate between <code>*.prodigal.gff</code> which is produced by Prodigal and <code>*.prokka.gff</code> which is a modified and extended version of the original Prodigal GFF. <code>*.gbk</code> and <code>*.sqn</code> are skipped per default, since tbl2asn runs for quite a while! If you need those files generated by prokka, include: <code>--tbl2asn</code> in the prokka parameters to enable it. Details of all files can be read on the Prokka page. In addition, it also computes a summary tsv file which adheres to the magAttributes specification.</p>"},{"location":"modules/annotation/#whokaryote","title":"Whokaryote","text":"<p>Whokaryote outputs three files:</p> <code>*_[eukaryote|prokaryote]_contig_headers.tsv</code> <p>These are contig headers of contigs predicted to be of eukaryotic (prokaryotic) origin.</p> <code>*_featuretable.csv</code> <p>Features that were used to predict the eukaryotic or prokaryotic origin. </p> <code>*_featuretable_predictions_T.tsv</code> <p>A .tsv file with the calculated features and the predictions. </p> <code>*_tiara_pred.tsv</code> <p>A file that contains the Tiara predictions.</p>"},{"location":"modules/annotation/#keggfromblast_1","title":"KEGGFromBlast","text":"<p>Result <code>*.tsv</code> file filled with KEGG information (like modules, KO's, ...) which could be linked to the input hits.</p>"},{"location":"modules/annotation/#resistance-gene-identifier-rgi","title":"Resistance Gene Identifier (rgi)","text":"<p>The <code>*rgi.tsv</code> files contain the found CARD genes.</p>"},{"location":"modules/assembly/","title":"Assembly","text":""},{"location":"modules/assembly/#input","title":"Input","text":"Command for short read data with optional single end readsCommand for long read dataMegahit Configuration FileMetaspades Configuration FileMetaFlye Configuration FileTSV Table Short ReadTSV Table Nanopore <pre><code>-entry wShortReadAssembly -params-file example_params/assembly.yml\n</code></pre> <pre><code>-entry wOntAssembly -params-file example_params/assemblyONT.yml\n</code></pre> <p>Warning</p> <p>The configuration file shown here is for demonstration and testing purposes only.    Parameters that should be used in production can be viewed in the assembly section    of one of the yaml files located in the <code>default</code> folder of the Toolkit's Github repository.</p> <pre><code>tempdir: \"tmp\"\ns3SignIn: false\noutput: \"output\"\nlogDir: log\nrunid: 1\nlogLevel: 1\nscratch: \"/vol/scratch\"\npublishDirMode: \"symlink\"\nsteps:\n  assembly:\n    input:\n      paired: test_data/assembly/samples.tsv\n      single: test_data/assembly/samplesUnpaired.tsv\n    megahit:\n      additionalParams: \" --min-contig-len 200 \"\n      fastg: true\n      resources:\n         RAM: \n            mode: 'DEFAULT'\n            predictMinLabel: 'AUTO' \n\nresources:\n  highmemLarge:\n    cpus: 28\n    memory: 230\n  highmemMedium:\n    cpus: 14\n    memory: 113\n  large:\n    cpus: 28\n    memory: 58\n  medium:\n    cpus: 14\n    memory: 29\n  small:\n    cpus: 7\n    memory: 14\n  tiny:\n    cpus: 1\n    memory: 1\n</code></pre> <pre><code>tempdir: \"tmp\"\ns3SignIn: false\noutput: \"output\"\nlogDir: log\nrunid: 1\nlogLevel: 1\nscratch: \"/vol/scratch\"\npublishDirMode: \"symlink\"\nsteps:\n  assembly:\n    input: \n      paired: test_data/assembly/samples.tsv \n    metaspades:\n      additionalParams: \"  \"\n      fastg: true\nresources:\n  highmemLarge:\n    cpus: 28\n    memory: 200\n  highmemMedium:\n    cpus: 14\n    memory: 113\n  large:\n    cpus: 28\n    memory: 58\n  medium:\n    cpus: 14\n    memory: 29\n  small:\n    cpus: 7\n    memory: 14\n  tiny:\n    cpus: 1\n    memory: 1\n</code></pre> <p>Warning</p> <p>The configuration file shown here is for demonstration and testing purposes only.    Parameters that should be used in production can be viewed in the assemblyONT section    of one of the yaml files located in the <code>default</code> folder of the Toolkit's Github repository.</p> <pre><code>tempdir: \"tmp\"\ns3SignIn: false\noutput: \"output\"\nlogDir: log\nrunid: 1\nlogLevel: 1\nscratch: \"/vol/scratch\"\nsteps:\n  assemblyONT:\n    input: test_data/assembly/samplesONT.tsv \n    metaflye:\n      additionalParams: \" -i 1 \"\n      quality: \" --nano-raw \"\n\nresources:\n  highmemLarge:\n    cpus: 28\n    memory: 230\n  highmemMedium:\n    cpus: 14\n    memory: 113\n  large:\n    cpus: 28\n    memory: 58\n  medium:\n    cpus: 14\n    memory: 29\n  small:\n    cpus: 7\n    memory: 14\n  tiny:\n    cpus: 1\n    memory: 1\n</code></pre> <pre><code>SAMPLE  READS\ntest1   https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/small/interleaved.fq.gz\n</code></pre> <pre><code>SAMPLE  READS\nnano    https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/SRR16328449_qc.fq.gz\n</code></pre>"},{"location":"modules/assembly/#output","title":"Output","text":"<p>The output is a gzipped fasta file containing contigs.</p>"},{"location":"modules/assembly/#megahit","title":"Megahit","text":""},{"location":"modules/assembly/#error-handling","title":"Error Handling","text":"<p>On error with exit codes ([-9, 137, 247]) (e.g. due to memory restrictions), the tool is executed again with higher cpu and memory values. The memory and cpu values are in case of a retry selected based on the flavor with the next higher memory value. The highest possible cpu/memory value is restricted by the highest cpu/memory value of all flavors defined in the resource section  (see global configuration section). </p>"},{"location":"modules/assembly/#peak-memory-usage-prediction","title":"Peak memory usage prediction","text":"<p>Memory consumption of an assembler varies based on diversity. We trained a machine learning model on kmer frequencies and the nonpareil diversity index in order to be able to predict the memory peak consumption of megahit in our full pipeline mode. The required resources in order to run the assembler are thereby fitted to the resources that are actually needed for a specific dataset. If this mode is enabled then Nonpareil and kmc that are part of the quality control module are automatically executed before the assembler run.  </p> <p>Please note that this mode is only tested for Megahit with default parameters and the meta-sensitive mode (<code>--presets meta-sensitive</code>).</p> <pre><code>  resources:\n    RAM: \n      mode: MODE\n      predictMinLabel: LABEL\n</code></pre> <p>where      * MODE can be either 'PREDICT' for predicting memory usage or 'DEFAULT' for using a default flavor defined in the resources section.</p> <pre><code>* LABEL is the flavor that will be used if the predicted RAM is below the memory value defined as part of the LABEL flavor. It can also be set to AUTO to always use the predicted flavor.\n</code></pre>"},{"location":"modules/cooccurrence/","title":"Cooccurrence","text":"<p>The Cooccurrence module builds a cooccurrence network where each node is a MAG and every edge represents an association between them. The network can be inferred based on correlation or inverse covariance estimation by SPIEC-EASI. SPIEC-EASI is executed multiple times based on different parameter settings in order to find the most stable network. In addition, it is possible to compute multiple metrics for every edge based on genome-scale metabolic models and the SMETANA metrics. </p> <pre><code>-entry wCooccurrence -params-file example_params/cooccurrence.yml\n</code></pre>"},{"location":"modules/cooccurrence/#input","title":"Input","text":"CommandConfiguration File for CooccurrenceTSV TableGTDB TSV TableConfiguration File for analyzing edges in Cooccurrence GraphGTDB TSV for analyzing EdgesModel TSV for computing Metabolomics Metrics on Edges <pre><code>-entry wCooccurrence -params-file example_params/cooccurrence.yml\n</code></pre> <p>Warning</p> <p>The configuration file shown here is for demonstration and testing purposes only.    Parameters that should be used in production can be viewed in the cooccurrence section    of one of the yaml files located in the <code>default</code> folder of the Toolkit's Github repository.</p> <pre><code>tempdir: \"tmp\"\ns3SignIn: false\noutput: \"output\"\nrunid: 1\nlogLevel: 1\nlogDir: log\nscratch: \"/vol/scratch\"\npublishDirMode: \"symlink\"\nsteps:\n  cooccurrence:\n    input:\n      gtdb: test_data/cooccurrence/gtdb.tsv\n      count: test_data/cooccurrence/counts.tsv \n    inference:\n       additionalParams: \n         method: 'correlation'\n         rscript: \" --mincovthreshold 0.1 --maxzero 110 --minweight 0.4 \"\n         timeLimit: \"AUTO\"\n\nresources:\n  highmemLarge:\n    cpus: 28\n    memory: 230\n  highmemMedium:\n    cpus: 14\n    memory: 113\n  large:\n    cpus: 28\n    memory: 58\n  medium:\n    cpus: 14\n    memory: 29\n  small:\n    cpus: 7\n    memory: 14\n  tiny:\n    cpus: 1\n    memory: 1\n</code></pre> <p><pre><code>SAMPLE  READS\ntest1   https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/small/interleaved.fq.gz\n</code></pre> Contains abundance values of mags per sample.</p> <p><pre><code>BIN_ID  DOMAIN  SAMPLE  user_genome classification  fastani_reference   fastani_reference_radius    fastani_taxonomy    fastani_ani fastani_af  closest_placement_reference closest_placement_radius    closest_placement_taxonomy  closest_placement_ani   closest_placement_af    pplacer_taxonomy    classification_method   note    other_related_references(genome_id,species_name,radius,ANI,AF)  msa_percent translation_table   red_value   warnings\ntest1_bin.1.fa  BACTERIA    test2   test2_bin.1.fa  d__Bacteria;p__Actinobacteriota;c__Actinomycetia;o__Actinomycetales;f__Dermatophilaceae;g__Phycicoccus;s__Phycicoccus sp001428065   GCF_001428065.1 95.0    d__Bacteria;p__Actinobacteriota;c__Actinomycetia;o__Actinomycetales;f__Dermatophilaceae;g__Phycicoccus;s__Phycicoccus sp001428065   99.77   0.99    GCF_001428065.1 95.0    d__Bacteria;p__Actinobacteriota;c__Actinomycetia;o__Actinomycetales;f__Dermatophilaceae;g__Phycicoccus;s__Phycicoccus sp001428065   99.77   0.99    d__Bacteria;p__Actinobacteriota;c__Actinomycetia;o__Actinomycetales;f__Dermatophilaceae;g__Phycicoccus;s__  taxonomic classification defined by topology and ANI    topological placement and ANI have congruent species assignments    GCF_002846495.1, s__Phycicoccus duodecadis, 95.0, 90.33, 0.95; GCF_011383005.1, s__Phycicoccus sp011383005, 95.0, 82.92, 0.85; GCF_000720925.1, s__Phycicoccus jejuensis, 95.0, 82.49, 0.82; GCF_013201035.1, s__Phycicoccus sp013201035, 95.0, 82.18, 0.84; GCF_004025965.2, s__Phycicoccus sp004025965, 95.0, 81.96, 0.78; GCF_011326735.1, s__Phycicoccus endophyticus, 95.0, 81.12, 0.7 12.73   11  N/A N/A\ntest2_bin.2.fa  BACTERIA    test2   test2_bin.2.fa  d__Bacteria;p__Firmicutes_C;c__Negativicutes;o__Selenomonadales;f__Selenomonadaceae;g__Schwartzia;s__Schwartzia succinivorans   GCF_900129225.1 95.0    d__Bacteria;p__Firmicutes_C;c__Negativicutes;o__Selenomonadales;f__Selenomonadaceae;g__Schwartzia;s__Schwartzia succinivorans   99.97   1.0 GCF_900129225.1 95.0    d__Bacteria;p__Firmicutes_C;c__Negativicutes;o__Selenomonadales;f__Selenomonadaceae;g__Schwartzia;s__Schwartzia succinivorans   99.97   1.0 d__Bacteria;p__Firmicutes_C;c__Negativicutes;o__Selenomonadales;f__Selenomonadaceae;g__;s__ taxonomic classification defined by topology and ANI    topological placement and ANI have congruent species assignments    N/A 91.94   11  N/A N/A\n</code></pre> GTDB assignment of all samples that were produced by magAttributes module.</p> <p>Warning</p> <p>The configuration file shown here is for demonstration and testing purposes only.    Parameters that should be used in production can be viewed in the cooccurrence section    of one of the yaml files located in the <code>default</code> folder of the Toolkit's Github repository.</p> <pre><code>tempdir: \"/tmp\"\ns3SignIn: false\noutput: \"output\"\nrunid: 1\nlogLevel: 0\nlogDir: log\nscratch: \"/vol/scratch\"\npublishDirMode: \"symlink\"\nsteps:\n  cooccurrence:\n     input:\n       gtdb: test_data/cooccurrence/gtdb_large.tsv\n       count: test_data/cooccurrence/countsMedium.tsv\n       models: test_data/cooccurrence/models.tsv\n     inference:\n        additionalParams:\n          method: 'spiec-easi'\n          rscript: \" --mincovthreshold 0.1 --maxzero 110\"\n          timeLimit: \"AUTO\"\n     metabolicAnnotation:\n       additionalParams:\n         metabolicEdgeBatches: 5\n         metabolicEdgeReplicates: 10\n         smetana: \" --flavor bigg \"\n\nresources:\n  highmemLarge:\n    cpus: 28\n    memory: 230\n  highmemMedium:\n    cpus: 14\n    memory: 113\n  large:\n    cpus: 28\n    memory: 58\n  medium:\n    cpus: 14\n    memory: 29\n  small:\n    cpus: 7\n    memory: 14\n  tiny:\n    cpus: 1\n    memory: 1\n</code></pre> <p><pre><code>DOMAIN  PHYLUM  CLASS   ORDER   FAMILY  GENUS   SPECIES BIN_ID\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Acinetobacter    s__ ERR2592244_bin.1\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Neisseriaceae    g__Neisseria    s__Neisseria suis   ERR2592244_bin.10\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Formosimonas s__ ERR2592244_bin.11\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Formosimonas s__ ERR2592244_bin.12\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__JAGOEY01 s__ ERR2592244_bin.15\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Acinetobacter    s__ ERR2592244_bin.16\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Acidovorax   s__Acidovorax defluvii  ERR2592244_bin.17\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Acidovorax   s__ ERR2592244_bin.18\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Flavobacteriaceae    g__Flavobacterium   s__ ERR2592244_bin.2\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Pseudomonadaceae g__Pseudomonas_E    s__ ERR2592244_bin.20\nd__Bacteria p__Campylobacterota c__Campylobacteria  o__Campylobacterales    f__Arcobacteraceae  g__Aliarcobacter    s__Aliarcobacter cryaerophilus_A    ERR2592244_bin.23\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Pseudomonadaceae g__Pseudomonas_E    s__Pseudomonas_E paracarnis ERR2592244_bin.25\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Formosimonas s__ ERR2592244_bin.26\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Weeksellaceae    g__Kaistella    s__Kaistella chaponensis    ERR2592244_bin.27\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Formosimonas s__ ERR2592244_bin.28\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Acinetobacter    s__Acinetobacter celticus   ERR2592244_bin.29\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Veillonellales   f__Dialisteraceae   g__Dialister    s__Dialister invisus    ERR2592244_bin.3\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Psychrobacter    s__Psychrobacter faecalis   ERR2592244_bin.32\nd__Bacteria p__Firmicutes   c__Bacilli  o__Lactobacillales  f__Aerococcaceae    g__Trichococcus s__Trichococcus flocculiformis  ERR2592244_bin.33\nd__Bacteria p__Firmicutes   c__Bacilli  o__Lactobacillales  f__Streptococcaceae g__Lactococcus_A    s__Lactococcus_A raffinolactis  ERR2592244_bin.34\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Acinetobacter    s__Acinetobacter johnsonii  ERR2592244_bin.36\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Formosimonas s__ ERR2592244_bin.37\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Diaphorobacter_A s__ ERR2592244_bin.38\nd__Bacteria p__Fusobacteriota   c__Fusobacteriia    o__Fusobacteriales  f__Leptotrichiaceae g__JAGOYM01 s__JAGOYM01 sp018059165 ERR2592244_bin.4\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Veillonellales   f__Dialisteraceae   g__Dialister    s__Dialister sp000434475    ERR2592244_bin.40\nd__Bacteria p__Actinobacteriota c__Actinomycetia    o__Actinomycetales  f__Micrococcaceae   g__Galactobacter    s__ ERR2592244_bin.41\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Veillonellales   f__Dialisteraceae   g__Dialister    s__Dialister invisus    ERR2592244_bin.5\nd__Bacteria p__Fusobacteriota   c__Fusobacteriia    o__Fusobacteriales  f__Leptotrichiaceae g__JAGOYM01 s__ ERR2592244_bin.6\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Weeksellaceae    g__Kaistella    s__ ERR2592244_bin.7\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Veillonellales   f__Dialisteraceae   g__Dialister    s__Dialister invisus    ERR2592244_bin.8\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Ottowia  s__Ottowia sp018060485  ERR2592244_bin.9\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Weeksellaceae    g__Epilithonimonas  s__Epilithonimonas vandammei    ERR2592245_bin.11\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Weeksellaceae    g__Empedobacter s__Empedobacter falsenii_A  ERR2592245_bin.12\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Acinetobacter    s__Acinetobacter parvus ERR2592245_bin.13\nd__Bacteria p__Campylobacterota c__Campylobacteria  o__Campylobacterales    f__Arcobacteraceae  g__Aliarcobacter    s__ ERR2592245_bin.14\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Weeksellaceae    g__Cloacibacterium  s__ ERR2592245_bin.15\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Ottowia  s__Ottowia beijingensis ERR2592245_bin.16\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Acinetobacter    s__Acinetobacter harbinensis    ERR2592245_bin.17\nd__Bacteria p__Firmicutes   c__Bacilli  o__Lactobacillales  f__Streptococcaceae g__Streptococcus    s__Streptococcus parasuis   ERR2592245_bin.18\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__CTSOIL-112   s__ ERR2592245_bin.19\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Acidovorax   s__Acidovorax temperans ERR2592245_bin.2\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Neisseriaceae    g__26B  s__26B sp016894345  ERR2592245_bin.20\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Acinetobacter    s__Acinetobacter johnsonii  ERR2592245_bin.21\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Acinetobacter    s__Acinetobacter harbinensis    ERR2592245_bin.22\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Neisseriaceae    g__Neisseria    s__Neisseria suis   ERR2592245_bin.23\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Comamonas    s__Comamonas denitrificans_A    ERR2592245_bin.26\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Acinetobacter    s__Acinetobacter guillouiae ERR2592245_bin.27\nd__Bacteria p__Fusobacteriota   c__Fusobacteriia    o__Fusobacteriales  f__Leptotrichiaceae g__JAGOYM01 s__JAGOYM01 sp018059165 ERR2592245_bin.28\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Sphaerotilus s__Sphaerotilus sulfidivorans   ERR2592245_bin.29\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Psychrobacter    s__ ERR2592245_bin.3\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Psychrobacter    s__ ERR2592245_bin.30\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Acidovorax   s__Acidovorax defluvii  ERR2592245_bin.4\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Brachymonas  s__ ERR2592245_bin.5\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Moraxella_A  s__Moraxella_A sp002478835  ERR2592245_bin.6\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Neisseriaceae    g__Vitreoscilla s__ ERR2592245_bin.8\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Flavobacteriaceae    g__Flavobacterium   s__Flavobacterium sp017997335   ERR2592245_bin.9\nd__Bacteria p__Verrucomicrobiota    c__Kiritimatiellae  o__LD1-PB3  f__Lenti-01 g__Lenti-01 s__ ERR2592249_bin.10\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Brachymonas  s__ ERR2592249_bin.11\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Ottowia  s__Ottowia beijingensis ERR2592249_bin.12\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__CTSOIL-112   s__ ERR2592249_bin.13\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Acidaminococcales    f__Acidaminococcaceae   g__RZYP01   s__ ERR2592249_bin.14\nd__Bacteria p__Chloroflexota    c__Anaerolineae o__Promineofilales  f__Promineofilaceae g__Promineofilum    s__ ERR2592249_bin.15\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Acidovorax   s__Acidovorax temperans ERR2592249_bin.16\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__UBA1444  f__UBA1444  g__UBA1444  s__ ERR2592249_bin.18\nd__Bacteria p__Desulfobacterota c__Desulfobulbia    o__Desulfobulbales  f__Desulfobulbaceae g__Desulfobulbus    s__Desulfobulbus sp017998195    ERR2592249_bin.19\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Selenomonadales  f__Selenomonadaceae g__ s__ ERR2592249_bin.20\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__ s__ ERR2592249_bin.21\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Brachymonas  s__Brachymonas sp017996515  ERR2592249_bin.22\nd__Bacteria p__Firmicutes_A c__Clostridia   o__Peptostreptococcales f__Filifactoraceae  g__Acetoanaerobium  s__Acetoanaerobium noterae  ERR2592249_bin.23\nd__Bacteria p__Desulfobacterota c__Desulfobulbia    o__Desulfobulbales  f__Desulfobulbaceae g__Desulfobulbus    s__ ERR2592249_bin.25\nd__Bacteria p__Desulfobacterota c__Desulfobulbia    o__Desulfobulbales  f__Desulfobulbaceae g__Desulfobulbus    s__ ERR2592249_bin.27\nd__Bacteria p__Fusobacteriota   c__Fusobacteriia    o__Fusobacteriales  f__Leptotrichiaceae g__JAGOWQ01 s__ ERR2592249_bin.28\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Neisseriaceae    g__Neisseria    s__ ERR2592249_bin.29\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Neisseriaceae    g__Neisseria    s__Neisseria suis   ERR2592249_bin.3\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Acinetobacter    s__Acinetobacter towneri    ERR2592249_bin.30\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Enterobacterales f__Pasteurellaceae  g__Mesocricetibacter    s__ ERR2592249_bin.31\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Selenomonadales  f__JAGPMH01 g__JAGPMH01 s__JAGPMH01 sp018052945 ERR2592249_bin.33\nd__Bacteria p__Planctomycetota  c__Planctomycetia   o__Pirellulales f__PNKZ01   g__ s__ ERR2592249_bin.34\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Rhodocyclaceae   g__Propionivibrio   s__ ERR2592249_bin.36\nd__Bacteria p__Patescibacteria  c__Microgenomatia   o__UBA1406  f__ g__ s__ ERR2592249_bin.37\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Brachymonas  s__Brachymonas sp018054915  ERR2592249_bin.38\nd__Bacteria p__Acidobacteriota  c__Aminicenantia    o__ f__ g__ s__ ERR2592249_bin.39\nd__Bacteria p__Actinobacteriota c__Acidimicrobiia   o__Acidimicrobiales f__JAEUJM01 g__ s__ ERR2592249_bin.4\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Enterobacterales f__Enterobacteriaceae   g__Klebsiella   s__Klebsiella pneumoniae    ERR2592249_bin.6\nd__Bacteria p__Firmicutes   c__Bacilli  o__Lactobacillales  f__Streptococcaceae g__Streptococcus    s__ ERR2592249_bin.7\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Moraxella    s__ ERR2592249_bin.8\nd__Bacteria p__Patescibacteria  c__JAEDAM01 o__BD1-5    f__UBA2023  g__UBA5532  s__ ERR2592251_bin.1\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Flavobacteriaceae    g__Flavobacterium   s__ ERR2592251_bin.10\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Chitinophagales  f__Chitinophagaceae g__Niabella s__ ERR2592251_bin.11\nd__Bacteria p__Firmicutes   c__Bacilli  o__Lactobacillales  f__Aerococcaceae    g__Trichococcus s__Trichococcus flocculiformis  ERR2592251_bin.12\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Psychrobacter    s__Psychrobacter maritimus  ERR2592251_bin.13\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Xanthomonadales  f__Xanthomonadaceae g__Thermomonas  s__ ERR2592251_bin.14\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Veillonellales   f__Dialisteraceae   g__Dialister    s__Dialister invisus    ERR2592251_bin.16\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Ottowia  s__Ottowia sp018060485  ERR2592251_bin.17\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__ s__ ERR2592251_bin.18\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Veillonellales   f__Dialisteraceae   g__Dialister    s__Dialister invisus    ERR2592251_bin.19\nd__Bacteria p__Campylobacterota c__Campylobacteria  o__Campylobacterales    f__Arcobacteraceae  g__Aliarcobacter    s__Aliarcobacter acticola   ERR2592251_bin.2\nd__Bacteria p__Firmicutes_A c__Clostridia   o__Oscillospirales  f__Acutalibacteraceae   g__Fimenecus    s__Fimenecus sp000432435    ERR2592251_bin.20\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Xanthomonadales  f__Xanthomonadaceae g__Thermomonas  s__ ERR2592251_bin.3\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Acidovorax   s__Acidovorax defluvii  ERR2592251_bin.4\nd__Bacteria p__Firmicutes_A c__Clostridia   o__Lachnospirales   f__Lachnospiraceae  g__Agathobacter s__Agathobacter rectalis    ERR2592251_bin.6\nd__Bacteria p__Fusobacteriota   c__Fusobacteriia    o__Fusobacteriales  f__Leptotrichiaceae g__JAGOYM01 s__JAGOYM01 sp018059165 ERR2592251_bin.7\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Acinetobacter    s__ ERR2592251_bin.8\nd__Bacteria p__Firmicutes   c__Bacilli  o__Lactobacillales  f__Streptococcaceae g__Lactococcus_A    s__Lactococcus_A raffinolactis  ERR2592251_bin.9\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Comamonas    s__Comamonas denitrificans_A    ERR2592252_bin.1\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Brachymonas  s__ ERR2592252_bin.11\nd__Bacteria p__Fusobacteriota   c__Fusobacteriia    o__Fusobacteriales  f__Leptotrichiaceae g__Leptotrichia_A   s__ ERR2592252_bin.12\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Acidaminococcales    f__Acidaminococcaceae   g__RZYP01   s__ ERR2592252_bin.13\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Moraxella_A  s__ ERR2592252_bin.14\nd__Bacteria p__Actinobacteriota c__Actinomycetia    o__Propionibacteriales  f__Propionibacteriaceae g__Brooklawnia  s__ ERR2592252_bin.15\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__JAABTG01 f__JAABTG01 g__JAGNLM01 s__ ERR2592252_bin.16\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Neisseriaceae    g__Neisseria    s__ ERR2592252_bin.17\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Acinetobacter    s__Acinetobacter towneri    ERR2592252_bin.18\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Veillonellales   f__Veillonellaceae  g__Veillonella  s__Veillonella sp009929605  ERR2592252_bin.19\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Acidaminococcales    f__Acidaminococcaceae   g__RZYP01   s__ ERR2592252_bin.2\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Neisseriaceae    g__Neisseria    s__Neisseria suis   ERR2592252_bin.20\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Rhodocyclaceae   g__ s__ ERR2592252_bin.21\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Acidovorax   s__Acidovorax temperans ERR2592252_bin.22\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Moraxella    s__Moraxella porci  ERR2592252_bin.23\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Brachymonas  s__Brachymonas sp012729605  ERR2592252_bin.24\nd__Bacteria p__Desulfobacterota c__Desulfobulbia    o__Desulfobulbales  f__Desulfobulbaceae g__Desulfobulbus    s__Desulfobulbus sp017998195    ERR2592252_bin.25\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Enterobacterales f__Pasteurellaceae  g__Actinobacillus   s__ ERR2592252_bin.26\nd__Bacteria p__Verrucomicrobiota    c__Kiritimatiellae  o__RFP12    f__UBA3636  g__JAAZAI01 s__JAAZAI01 sp012514445 ERR2592252_bin.27\nd__Bacteria p__Acidobacteriota  c__Aminicenantia    o__ f__ g__ s__ ERR2592252_bin.28\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Moraxella_A  s__Moraxella_A sp002478835  ERR2592252_bin.29\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__CTSOIL-112   s__ ERR2592252_bin.3\nd__Bacteria p__Chloroflexota    c__Anaerolineae o__Anaerolineales   f__Anaerolineaceae  g__T78  s__ ERR2592252_bin.30\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Brachymonas  s__ ERR2592252_bin.31\nd__Bacteria p__Firmicutes   c__Bacilli  o__Lactobacillales  f__Streptococcaceae g__Streptococcus    s__ ERR2592252_bin.32\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Brachymonas  s__Brachymonas sp018054915  ERR2592252_bin.33\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Ottowia  s__Ottowia beijingensis ERR2592252_bin.34\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Moraxella    s__ ERR2592252_bin.35\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Sphaerotilus s__Sphaerotilus sulfidivorans   ERR2592252_bin.36\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Selenomonadales  f__Selenomonadaceae g__ s__ ERR2592252_bin.37\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Brachymonas  s__ ERR2592252_bin.38\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Diaphorobacter   s__ ERR2592252_bin.40\nd__Bacteria p__Firmicutes   c__Bacilli  o__Lactobacillales  f__Streptococcaceae g__Streptococcus    s__Streptococcus parasuis   ERR2592252_bin.41\nd__Bacteria p__Synergistota c__Synergistia  o__Synergistales    f__Synergistaceae   g__Syner-03 s__Syner-03 sp002316795 ERR2592252_bin.42\nd__Bacteria p__Campylobacterota c__Campylobacteria  o__Campylobacterales    f__Arcobacteraceae  g__Aliarcobacter    s__Aliarcobacter cryaerophilus_A    ERR2592252_bin.43\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Neisseriaceae    g__Neisseria    s__ ERR2592252_bin.44\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Brachymonas  s__Brachymonas chironomi    ERR2592252_bin.45\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Brachymonas  s__Brachymonas denitrificans    ERR2592252_bin.47\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Xanthomonadales  f__Xanthomonadaceae g__Pseudoxanthomonas    s__ ERR2592252_bin.49\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Brachymonas  s__Brachymonas sp017993135  ERR2592252_bin.5\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Neisseriaceae    g__Neisseria    s__ ERR2592252_bin.50\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Moraxella    s__ ERR2592252_bin.51\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Rhodocyclaceae   g__Azonexus s__Azonexus sp012839675 ERR2592252_bin.52\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Weeksellaceae    g__Cloacibacterium  s__ ERR2592252_bin.53\nd__Bacteria p__Patescibacteria  c__Saccharimonadia  o__Saccharimonadales    f__Nanogingivalaceae    g__Nanogingivalis   s__ ERR2592252_bin.54\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Bacteroidales    f__Paludibacteraceae    g__UPXZ01   s__UPXZ01 sp009929445   ERR2592252_bin.55\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Enterobacterales f__Pasteurellaceae  g__Mesocricetibacter    s__ ERR2592252_bin.6\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Weeksellaceae    g__Cloacibacterium  s__ ERR2592252_bin.7\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Xanthomonadales  f__Xanthomonadaceae g__Pseudoxanthomonas_A  s__ ERR2592252_bin.8\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Selenomonadales  f__JAGPMH01 g__JAGPMH01 s__JAGPMH01 sp018052945 ERR2592252_bin.9\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Weeksellaceae    g__Cloacibacterium  s__ ERR2592253_bin.1\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Veillonellales   f__Dialisteraceae   g__Dialister    s__Dialister invisus    ERR2592253_bin.10\nd__Bacteria p__Campylobacterota c__Campylobacteria  o__Campylobacterales    f__Arcobacteraceae  g__Aliarcobacter    s__ ERR2592253_bin.11\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Weeksellaceae    g__Kaistella    s__ ERR2592253_bin.12\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Neisseriaceae    g__Neisseria    s__ ERR2592253_bin.13\nd__Bacteria p__Fusobacteriota   c__Fusobacteriia    o__Fusobacteriales  f__Leptotrichiaceae g__JAGOYM01 s__JAGOYM01 sp018059165 ERR2592253_bin.14\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__JAAFJR01 s__JAAFJR01 sp016705065 ERR2592253_bin.15\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Moraxella_A  s__Moraxella_A sp002478835  ERR2592253_bin.16\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__UJ101    g__UJ101    s__ ERR2592253_bin.17\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Veillonellales   f__Dialisteraceae   g__Dialister    s__Dialister invisus    ERR2592253_bin.18\nd__Bacteria p__Firmicutes   c__Bacilli  o__Lactobacillales  f__Aerococcaceae    g__Trichococcus s__Trichococcus flocculiformis  ERR2592253_bin.19\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Acidovorax   s__Acidovorax defluvii  ERR2592253_bin.2\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Acinetobacter    s__ ERR2592253_bin.20\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Comamonas    s__Comamonas denitrificans_A    ERR2592253_bin.21\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Bacteroidales    f__DTU049   g__DTU049   s__DTU049 sp001513285   ERR2592253_bin.22\nd__Bacteria p__Actinobacteriota c__Actinomycetia    o__Actinomycetales  f__Dermatophilaceae g__Phycicoccus_A    s__Phycicoccus_A elongatus  ERR2592253_bin.23\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Flavobacteriaceae    g__Flavobacterium   s__ ERR2592253_bin.24\nd__Bacteria p__Campylobacterota c__Campylobacteria  o__Campylobacterales    f__Arcobacteraceae  g__Aliarcobacter    s__Aliarcobacter sp017996075    ERR2592253_bin.25\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Weeksellaceae    g__Cloacibacterium  s__ ERR2592253_bin.27\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Weeksellaceae    g__Kaistella    s__ ERR2592253_bin.29\nd__Bacteria p__Firmicutes   c__Bacilli  o__Lactobacillales  f__Streptococcaceae g__Lactococcus_A    s__Lactococcus_A raffinolactis  ERR2592253_bin.3\nd__Bacteria p__Actinobacteriota c__Actinomycetia    o__Actinomycetales  f__Cellulomonadaceae    g__Timonella    s__ ERR2592253_bin.30\nd__Bacteria p__Firmicutes_A c__Clostridia   o__Oscillospirales  f__Acutalibacteraceae   g__Fimenecus    s__Fimenecus sp000432435    ERR2592253_bin.31\nd__Bacteria p__Actinobacteriota c__Actinomycetia    o__Actinomycetales  f__Dermatophilaceae g__Phycicoccus_A    s__ ERR2592253_bin.33\nd__Bacteria p__Coprothermobacterota c__Coprothermobacteria  o__Coprothermobacterales    f__Coprothermobacteraceae   g__Coprothermobacter    s__Coprothermobacter proteolyticus  ERR2592253_bin.35\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__JAGOZY01 f__JAGOZY01 g__JAGOHU01 s__JAGOHU01 sp017995975 ERR2592253_bin.37\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Weeksellaceae    g__Empedobacter s__ ERR2592253_bin.38\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Neisseriaceae    g__Neisseria    s__Neisseria suis   ERR2592253_bin.39\nd__Bacteria p__Fusobacteriota   c__Fusobacteriia    o__Fusobacteriales  f__Leptotrichiaceae g__Sebaldella   s__ ERR2592253_bin.4\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__CTSOIL-112   s__CTSOIL-112 sp017986195   ERR2592253_bin.40\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Acinetobacter    s__Acinetobacter sp009707625    ERR2592253_bin.5\nd__Bacteria p__Campylobacterota c__Campylobacteria  o__Campylobacterales    f__Sulfurospirillaceae  g__Sulfurospirillum s__ ERR2592253_bin.6\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__JAGOEY01 s__JAGOEY01 sp017997515 ERR2592253_bin.7\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Ottowia  s__Ottowia sp018060485  ERR2592253_bin.8\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Brachymonas  s__Brachymonas sp017993135  ERR2592253_bin.9\nd__Bacteria p__Firmicutes   c__Bacilli  o__Lactobacillales  f__Lactobacillaceae g__Lactobacillus    s__Lactobacillus delbrueckii    ERR2592258_bin.1\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Brachymonas  s__Brachymonas sp018054915  ERR2592258_bin.11\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Bacteroidales    f__Bacteroidaceae   g__Prevotella   s__Prevotella sp015074785   ERR2592258_bin.12\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Veillonellales   f__Dialisteraceae   g__Dialister    s__Dialister succinatiphilus    ERR2592258_bin.13\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Enterobacterales f__Aeromonadaceae   g__JAGOXX01 s__ ERR2592258_bin.16\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Acidaminococcales    f__Acidaminococcaceae   g__RZYP01   s__ ERR2592258_bin.17\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Enterobacterales f__Aeromonadaceae   g__JAGOXX01 s__ ERR2592258_bin.18\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Veillonellales   f__Veillonellaceae  g__Veillonella_A    s__Veillonella_A sp900765165    ERR2592258_bin.19\nd__Bacteria p__Firmicutes_A c__Clostridia   o__Lachnospirales   f__Lachnospiraceae  g__Butyrivibrio s__ ERR2592258_bin.2\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Veillonellales   f__Veillonellaceae  g__Veillonella  s__Veillonella sp009929605  ERR2592258_bin.21\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Enterobacterales f__Succinivibrionaceae  g__Succinivibrio    s__Succinivibrio sp000431835    ERR2592258_bin.22\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Enterobacterales f__Aeromonadaceae   g__Tolumonas    s__Tolumonas sp008015085    ERR2592258_bin.23\nd__Bacteria p__Actinobacteriota c__Coriobacteriia   o__Coriobacteriales f__Atopobiaceae g__Olsenella_D  s__Olsenella_D sp002331575  ERR2592258_bin.25\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Selenomonadales  f__Selenomonadaceae g__ s__ ERR2592258_bin.26\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__CTSOIL-112   s__ ERR2592258_bin.27\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Enterobacterales f__Enterobacteriaceae   g__Escherichia  s__Escherichia coli ERR2592258_bin.28\nd__Bacteria p__Campylobacterota c__Campylobacteria  o__Campylobacterales    f__Sulfurospirillaceae  g__Sulfurospirillum s__Sulfurospirillum sp001548035 ERR2592258_bin.29\nd__Bacteria p__Firmicutes   c__Bacilli  o__Lactobacillales  f__Streptococcaceae g__Lactococcus  s__Lactococcus lactis   ERR2592258_bin.3\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Anaeromusales    f__Anaeromusaceae   g__Anaeromusa   s__ ERR2592258_bin.30\nd__Bacteria p__Desulfobacterota_I   c__Desulfovibrionia o__Desulfovibrionales   f__Desulfovibrionaceae  g__Desulfovibrio    s__ ERR2592258_bin.31\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Bacteroidales    f__Bacteroidaceae   g__Prevotella   s__Prevotella copri_A   ERR2592258_bin.32\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Enterobacterales f__Enterobacteriaceae   g__Klebsiella   s__Klebsiella pneumoniae    ERR2592258_bin.33\nd__Bacteria p__Firmicutes_A c__Clostridia   o__Lachnospirales   f__Lachnospiraceae  g__Agathobacter s__Agathobacter rectalis    ERR2592258_bin.34\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Selenomonadales  f__Selenomonadaceae g__Megamonas    s__Megamonas funiformis ERR2592258_bin.35\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Weeksellaceae    g__Cloacibacterium  s__ ERR2592258_bin.36\nd__Bacteria p__Actinobacteriota c__Actinomycetia    o__Actinomycetales  f__Bifidobacteriaceae   g__Bifidobacterium  s__Bifidobacterium thermophilum ERR2592258_bin.37\nd__Bacteria p__Patescibacteria  c__Saccharimonadia  o__Saccharimonadales    f__Nanosyncoccaceae g__UBA2834  s__ ERR2592258_bin.38\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Selenomonadales  f__Selenomonadaceae g__Mitsuokella  s__Mitsuokella multacida    ERR2592258_bin.39\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Bacteroidales    f__UBA932   g__Egerieousia  s__ ERR2592258_bin.4\nd__Bacteria p__Actinobacteriota c__Actinomycetia    o__Actinomycetales  f__Bifidobacteriaceae   g__Bifidobacterium  s__Bifidobacterium adolescentis ERR2592258_bin.40\nd__Bacteria p__Actinobacteriota c__Actinomycetia    o__Actinomycetales  f__Bifidobacteriaceae   g__Bifidobacterium  s__Bifidobacterium longum   ERR2592258_bin.42\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Bacteroidales    f__Tannerellaceae   g__Macellibacteroides   s__ ERR2592258_bin.43\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Bacteroidales    f__Bacteroidaceae   g__Prevotella   s__ ERR2592258_bin.44\nd__Bacteria p__Fusobacteriota   c__Fusobacteriia    o__Fusobacteriales  f__ g__ s__ ERR2592258_bin.45\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Bacteroidales    f__Rikenellaceae    g__Alistipes    s__Alistipes putredinis ERR2592258_bin.46\nd__Bacteria p__Actinobacteriota c__Actinomycetia    o__Actinomycetales  f__Bifidobacteriaceae   g__Bifidobacterium  s__Bifidobacterium sp002742445  ERR2592258_bin.47\nd__Bacteria p__Campylobacterota c__Campylobacteria  o__Campylobacterales    f__Arcobacteraceae  g__Aliarcobacter    s__Aliarcobacter cryaerophilus_A    ERR2592258_bin.48\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Bacteroidales    f__Bacteroidaceae   g__Prevotella   s__Prevotella sp018054505   ERR2592258_bin.5\nd__Bacteria p__Acidobacteriota  c__Aminicenantia    o__ f__ g__ s__ ERR2592258_bin.50\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Bacteroidales    f__Paludibacteraceae    g__ s__ ERR2592258_bin.51\nd__Bacteria p__Acidobacteriota  c__Mor1 o__J045 f__J045 g__JAGOJY01 s__ ERR2592258_bin.52\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Veillonellales   f__Dialisteraceae   g__Dialister    s__Dialister hominis    ERR2592258_bin.54\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Selenomonadales  f__Selenomonadaceae g__ s__ ERR2592258_bin.55\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Selenomonadales  f__JAGPMH01 g__JAGPMH01 s__JAGPMH01 sp018052945 ERR2592258_bin.56\nd__Bacteria p__Campylobacterota c__Campylobacteria  o__Campylobacterales    f__Arcobacteraceae  g__Aliarcobacter    s__ ERR2592258_bin.57\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Selenomonadales  f__Selenomonadaceae g__Selenomonas_A    s__Selenomonas_A sp018052705    ERR2592258_bin.58\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Veillonellales   f__Dialisteraceae   g__Dialister    s__Dialister hominis    ERR2592258_bin.6\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Veillonellales   f__Dialisteraceae   g__Dialister    s__Dialister hominis    ERR2592258_bin.7\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Bacteroidales    f__UBA932   g__Egerieousia  s__ ERR2592258_bin.8\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Selenomonadales  f__Selenomonadaceae g__ s__ ERR2592258_bin.9\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Acidovorax   s__Acidovorax defluvii  ERR2592260_bin.1\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Hydromonas   s__ ERR2592260_bin.11\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__UBA2334  s__ ERR2592260_bin.12\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Acinetobacter    s__Acinetobacter sp003987695    ERR2592260_bin.13\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Sphaerotilus s__Sphaerotilus montanus    ERR2592260_bin.14\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Flavobacteriaceae    g__Flavobacterium   s__Flavobacterium sp017997335   ERR2592260_bin.15\nd__Bacteria p__Fusobacteriota   c__Fusobacteriia    o__Fusobacteriales  f__Leptotrichiaceae g__JAGOYM01 s__JAGOYM01 sp018059165 ERR2592260_bin.16\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Acidovorax   s__Acidovorax temperans ERR2592260_bin.17\nd__Bacteria p__Firmicutes   c__Bacilli  o__Lactobacillales  f__Aerococcaceae    g__Trichococcus s__Trichococcus flocculiformis  ERR2592260_bin.19\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__JAGOEY01 s__ ERR2592260_bin.2\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Flavobacteriaceae    g__Flavobacterium   s__Flavobacterium sp017995345   ERR2592260_bin.20\nd__Bacteria p__Firmicutes   c__Bacilli  o__Lactobacillales  f__Streptococcaceae g__Lactococcus_A    s__Lactococcus_A raffinolactis  ERR2592260_bin.21\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Giesbergeria s__ ERR2592260_bin.23\nd__Bacteria p__Firmicutes   c__Bacilli  o__Lactobacillales  f__Aerococcaceae    g__Trichococcus s__Trichococcus flocculiformis  ERR2592260_bin.24\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Sphaerotilus s__Sphaerotilus sulfidivorans   ERR2592260_bin.25\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Sphaerotilus s__Sphaerotilus sulfidivorans   ERR2592260_bin.26\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Acinetobacter    s__Acinetobacter parvus ERR2592260_bin.3\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Weeksellaceae    g__Cloacibacterium  s__ ERR2592260_bin.4\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Weeksellaceae    g__Kaistella    s__Kaistella chaponensis    ERR2592260_bin.7\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Weeksellaceae    g__Cloacibacterium  s__Cloacibacterium caeni_A  ERR2592260_bin.8\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Moraxella_A  s__Moraxella_A sp002478835  ERR2592260_bin.9\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Neisseriaceae    g__Neisseria    s__Neisseria suis   ERR2592262_bin.1\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Weeksellaceae    g__Cloacibacterium  s__Cloacibacterium caeni_A  ERR2592262_bin.3\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Ottowia  s__Ottowia sp018060485  ERR2592262_bin.4\nd__Bacteria p__Firmicutes   c__Bacilli  o__Lactobacillales  f__Aerococcaceae    g__Trichococcus s__Trichococcus flocculiformis  ERR2592262_bin.5\nd__Bacteria p__Fusobacteriota   c__Fusobacteriia    o__Fusobacteriales  f__Leptotrichiaceae g__JAGOYM01 s__JAGOYM01 sp018059165 ERR2592262_bin.6\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Brachymonas  s__Brachymonas sp018054915  ERR2592264_bin.1\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Thiotrichales    f__Thiotrichaceae   g__ s__ ERR2592264_bin.10\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Xanthomonadales  f__Xanthomonadaceae g__Pseudoxanthomonas_A  s__ ERR2592264_bin.12\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Selenomonadales  f__JAGPMH01 g__JAGPMH01 s__JAGPMH01 sp018052945 ERR2592264_bin.13\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__CTSOIL-112   s__ ERR2592264_bin.14\nd__Bacteria p__Firmicutes_A c__Clostridia   o__Lachnospirales   f__Lachnospiraceae  g__Agathobacter s__Agathobacter rectalis    ERR2592264_bin.15\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Thiotrichales    f__Thiotrichaceae   g__ s__ ERR2592264_bin.16\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Veillonellales   f__Dialisteraceae   g__Dialister    s__Dialister hominis    ERR2592264_bin.17\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Enterobacterales f__Enterobacteriaceae   g__Escherichia  s__Escherichia coli ERR2592264_bin.18\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Neisseriaceae    g__Neisseria    s__ ERR2592264_bin.19\nd__Bacteria p__Firmicutes   c__Bacilli  o__Lactobacillales  f__Streptococcaceae g__Streptococcus    s__ ERR2592264_bin.2\nd__Bacteria p__Actinobacteriota c__Actinomycetia    o__Actinomycetales  f__Bifidobacteriaceae   g__Bifidobacterium  s__Bifidobacterium adolescentis ERR2592264_bin.20\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Enterobacterales f__Enterobacteriaceae   g__Klebsiella   s__Klebsiella pneumoniae    ERR2592264_bin.21\nd__Bacteria p__Firmicutes   c__Bacilli  o__Erysipelotrichales   f__Coprobacillaceae g__Catenibacterium  s__Catenibacterium sp000437715  ERR2592264_bin.22\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Acinetobacter    s__Acinetobacter parvus ERR2592264_bin.24\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Sphaerotilus s__Sphaerotilus sulfidivorans   ERR2592264_bin.25\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Weeksellaceae    g__Cloacibacterium  s__ ERR2592264_bin.26\nd__Bacteria p__Campylobacterota c__Campylobacteria  o__Campylobacterales    f__Arcobacteraceae  g__Aliarcobacter    s__Aliarcobacter cryaerophilus_A    ERR2592264_bin.27\nd__Bacteria p__Firmicutes_A c__Clostridia   o__Oscillospirales  f__Acutalibacteraceae   g__Fimenecus    s__Fimenecus sp000432435    ERR2592264_bin.28\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Veillonellales   f__Dialisteraceae   g__Dialister    s__Dialister hominis    ERR2592264_bin.29\nd__Bacteria p__Firmicutes_A c__Clostridia   o__Lachnospirales   f__Lachnospiraceae  g__Blautia_A    s__Blautia_A wexlerae   ERR2592264_bin.30\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Ottowia  s__ ERR2592264_bin.31\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Weeksellaceae    g__Epilithonimonas  s__ ERR2592264_bin.32\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Selenomonadales  f__Selenomonadaceae g__Megamonas    s__Megamonas funiformis ERR2592264_bin.5\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Selenomonadales  f__Selenomonadaceae g__Megamonas    s__Megamonas funiformis ERR2592264_bin.7\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Thiotrichales    f__Thiotrichaceae   g__ s__ ERR2592264_bin.8\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Veillonellales   f__Dialisteraceae   g__Dialister    s__Dialister hominis    ERR2592264_bin.9\nd__Bacteria p__Desulfobacterota c__Syntrophia   o__Syntrophales f__UBA5619  g__UBA5619  s__ ERR2592265_bin.1\nd__Bacteria p__Desulfobacterota c__Syntrophia   o__Syntrophales f__Fen-1087 g__Fen-1087 s__ ERR2592265_bin.10\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__JAABTG01 f__JAABTG01 g__JAGNLM01 s__ ERR2592265_bin.11\nd__Bacteria p__Synergistota c__Synergistia  o__Synergistales    f__Synergistaceae   g__Syner-03 s__Syner-03 sp002316795 ERR2592265_bin.13\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Veillonellales   f__Dialisteraceae   g__Dialister    s__Dialister invisus    ERR2592265_bin.14\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__UBA1444  f__UBA1444  g__UBA1444  s__ ERR2592265_bin.15\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Ottowia  s__Ottowia sp018060485  ERR2592265_bin.16\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Brachymonas  s__Brachymonas sp012729605  ERR2592265_bin.17\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Neisseriaceae    g__Neisseria    s__Neisseria suis   ERR2592265_bin.18\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Veillonellales   f__Dialisteraceae   g__Dialister    s__Dialister hominis    ERR2592265_bin.19\nd__Bacteria p__UBA1439  c__UBA1439  o__UBA1439  f__UBA1439  g__UBA1439  s__UBA1439 sp002329605  ERR2592265_bin.20\nd__Bacteria p__Firmicutes_A c__Clostridia   o__Lachnospirales   f__Lachnospiraceae  g__Eisenbergiella   s__Eisenbergiella sp900066775   ERR2592265_bin.21\nd__Bacteria p__Firmicutes_A c__Clostridia   o__Lachnospirales   f__Lachnospiraceae  g__Agathobacter s__Agathobacter rectalis    ERR2592265_bin.22\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Brachymonas  s__Brachymonas denitrificans    ERR2592265_bin.23\nd__Bacteria p__Firmicutes_A c__Clostridia   o__Oscillospirales  f__Ruminococcaceae  g__Gemmiger s__Gemmiger qucibialis  ERR2592265_bin.24\nd__Bacteria p__Verrucomicrobiota    c__Verrucomicrobiae o__Verrucomicrobiales   f__SLCJ01   g__ s__ ERR2592265_bin.25\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Xanthomonadales  f__Xanthomonadaceae g__Pseudoxanthomonas    s__Pseudoxanthomonas sp018060215    ERR2592265_bin.26\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__CTSOIL-112   s__ ERR2592265_bin.27\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Brachymonas  s__Brachymonas chironomi    ERR2592265_bin.28\nd__Bacteria p__Actinobacteriota c__Actinomycetia    o__Actinomycetales  f__Bifidobacteriaceae   g__Bifidobacterium  s__Bifidobacterium adolescentis ERR2592265_bin.29\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Veillonellales   f__Dialisteraceae   g__Dialister    s__Dialister hominis    ERR2592265_bin.3\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Enterobacterales f__Enterobacteriaceae   g__Escherichia  s__Escherichia coli ERR2592265_bin.30\nd__Bacteria p__Verrucomicrobiota    c__Kiritimatiellae  o__LD1-PB3  f__Lenti-01 g__Lenti-01 s__ ERR2592265_bin.4\nd__Bacteria p__Verrucomicrobiota    c__Verrucomicrobiae o__Verrucomicrobiales   f__Akkermansiaceae  g__Akkermansia  s__Akkermansia muciniphila  ERR2592265_bin.5\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Veillonellales   f__Dialisteraceae   g__Dialister    s__Dialister invisus    ERR2592265_bin.6\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Ottowia  s__Ottowia beijingensis ERR2592265_bin.8\nd__Bacteria p__Desulfobacterota c__Desulfobulbia    o__Desulfobulbales  f__Desulfobulbaceae g__Desulfobulbus    s__Desulfobulbus sp017998195    ERR2592265_bin.9\nd__Bacteria p__Firmicutes_A c__Clostridia   o__Lachnospirales   f__Lachnospiraceae  g__Agathobacter s__Agathobacter rectalis    ERR2592268_bin.1\nd__Bacteria p__Firmicutes_E c__DTU015   o__D8A-2    f__D2   g__UBA3907  s__UBA3907 sp018052475  ERR2592268_bin.10\nd__Bacteria p__Synergistota c__Synergistia  o__Synergistales    f__Synergistaceae   g__Syner-03 s__Syner-03 sp002306075 ERR2592268_bin.12\nd__Bacteria p__Synergistota c__Synergistia  o__Synergistales    f__Synergistaceae   g__ s__ ERR2592268_bin.13\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Bacteroidales    f__Paludibacteraceae    g__UPXZ01   s__ ERR2592268_bin.14\nd__Bacteria p__Patescibacteria  c__Dojkabacteria    o__B142 f__OLB20    g__ s__ ERR2592268_bin.15\nd__Bacteria p__Firmicutes_A c__Clostridia   o__Oscillospirales  f__Acutalibacteraceae   g__Fimenecus    s__Fimenecus sp000432435    ERR2592268_bin.16\nd__Bacteria p__Verrucomicrobiota    c__Verrucomicrobiae o__Verrucomicrobiales   f__Akkermansiaceae  g__Akkermansia  s__Akkermansia muciniphila  ERR2592268_bin.19\nd__Bacteria p__Desulfobacterota c__Desulfobulbia    o__Desulfobulbales  f__Desulfobulbaceae g__Desulfobulbus    s__ ERR2592268_bin.2\nd__Bacteria p__Patescibacteria  c__Saccharimonadia  o__Saccharimonadales    f__JAGPDM01 g__JAGPDM01 s__ ERR2592268_bin.20\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Xanthomonadales  f__Xanthomonadaceae g__Pseudoxanthomonas_A  s__ ERR2592268_bin.21\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Selenomonadales  f__JAGPMH01 g__JAGPMH01 s__JAGPMH01 sp018052945 ERR2592268_bin.23\nd__Bacteria p__Actinobacteriota c__Acidimicrobiia   o__Acidimicrobiales f__Microtrichaceae  g__Microthrix   s__ ERR2592268_bin.25\nd__Bacteria p__Firmicutes_A c__Clostridia   o__Oscillospirales  f__Acutalibacteraceae   g__Fimenecus    s__Fimenecus sp000432435    ERR2592268_bin.26\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Weeksellaceae    g__Cloacibacterium  s__Cloacibacterium caeni_A  ERR2592268_bin.27\nd__Bacteria p__Firmicutes_A c__Clostridia   o__Tissierellales   f__Peptoniphilaceae g__JAAYEL01 s__ ERR2592268_bin.28\nd__Bacteria p__Campylobacterota c__Campylobacteria  o__Campylobacterales    f__Arcobacteraceae  g__Aliarcobacter    s__ ERR2592268_bin.29\nd__Bacteria p__Actinobacteriota c__Actinomycetia    o__Actinomycetales  f__Bifidobacteriaceae   g__Bifidobacterium  s__Bifidobacterium adolescentis ERR2592268_bin.3\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Bacteroidales    f__Dysgonomonadaceae    g__UBA5287  s__ ERR2592268_bin.30\nd__Bacteria p__Firmicutes_A c__Clostridia   o__Peptostreptococcales f__Filifactoraceae  g__Proteocatella    s__Proteocatella sp009929415    ERR2592268_bin.31\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Bacteroidales    f__Paludibacteraceae    g__UPXZ01   s__ ERR2592268_bin.32\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Acidaminococcales    f__Acidaminococcaceae   g__ s__ ERR2592268_bin.33\nd__Bacteria p__Firmicutes_A c__Clostridia   o__Lachnospirales   f__Lachnospiraceae  g__JAGOGN01 s__ ERR2592268_bin.34\nd__Bacteria p__Firmicutes_A c__Clostridia   o__Oscillospirales  f__Ruminococcaceae  g__Gemmiger s__Gemmiger qucibialis  ERR2592268_bin.35\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Psychrobacter    s__Psychrobacter faecalis   ERR2592268_bin.36\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Veillonellales   f__Dialisteraceae   g__Dialister    s__Dialister invisus    ERR2592268_bin.37\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Bacteroidales    f__UBA4417  g__UBA4417  s__ ERR2592268_bin.38\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Comamonas    s__Comamonas denitrificans_A    ERR2592268_bin.39\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Ottowia  s__Ottowia sp018060485  ERR2592268_bin.40\nd__Bacteria p__Patescibacteria  c__ABY1 o__BM507    f__UBA917   g__UBA1359  s__ ERR2592268_bin.41\nd__Bacteria p__Firmicutes   c__Bacilli  o__Lactobacillales  f__Enterococcaceae  g__Enterococcus_I   s__Enterococcus_I aquimarinus   ERR2592268_bin.42\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Brachymonas  s__Brachymonas sp017993135  ERR2592268_bin.43\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Veillonellales   f__Dialisteraceae   g__Dialister    s__Dialister invisus    ERR2592268_bin.44\nd__Bacteria p__Firmicutes   c__Bacilli  o__Lactobacillales  f__Aerococcaceae    g__Trichococcus s__Trichococcus flocculiformis  ERR2592268_bin.45\nd__Bacteria p__Patescibacteria  c__Dojkabacteria    o__SC72 f__SC72 g__ s__ ERR2592268_bin.46\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Acidovorax   s__Acidovorax defluvii  ERR2592268_bin.47\nd__Bacteria p__Patescibacteria  c__Paceibacteria    o__UBA6257  f__ g__ s__ ERR2592268_bin.48\nd__Bacteria p__Patescibacteria  c__Saccharimonadia  o__Saccharimonadales    f__UBA7683  g__ s__ ERR2592268_bin.49\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Neisseriaceae    g__Neisseria    s__Neisseria suis   ERR2592268_bin.5\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Acinetobacter    s__Acinetobacter johnsonii  ERR2592268_bin.50\nd__Bacteria p__Fusobacteriota   c__Fusobacteriia    o__Fusobacteriales  f__Leptotrichiaceae g__JAGOYM01 s__JAGOYM01 sp018059165 ERR2592268_bin.6\nd__Bacteria p__Firmicutes_A c__Clostridia   o__Peptostreptococcales f__Filifactoraceae  g__Proteocatella    s__ ERR2592268_bin.7\nd__Bacteria p__Firmicutes_A c__Clostridia   o__Tissierellales   f__Peptoniphilaceae g__JAAYEL01 s__ ERR2592268_bin.8\nd__Bacteria p__Firmicutes   c__Bacilli  o__Lactobacillales  f__Streptococcaceae g__Lactococcus_A    s__Lactococcus_A raffinolactis  ERR2592268_bin.9\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__CTSOIL-112   s__ ERR2592269_bin.10\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Neisseriaceae    g__Neisseria    s__ ERR2592269_bin.11\nd__Bacteria p__Caldisericota    c__Caldisericia o__Caldisericales   f__Caldisericaceae  g__ s__ ERR2592269_bin.12\nd__Bacteria p__Chloroflexota    c__Anaerolineae o__Anaerolineales   f__Anaerolineaceae  g__ s__ ERR2592269_bin.13\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Enterobacterales f__Pasteurellaceae  g__Glaesserella s__ ERR2592269_bin.14\nd__Bacteria p__Desulfobacterota c__Syntrophia   o__Syntrophales f__UBA5619  g__UBA5619  s__ ERR2592269_bin.16\nd__Bacteria p__Verrucomicrobiota    c__Kiritimatiellae  o__LD1-PB3  f__Lenti-01 g__Lenti-01 s__ ERR2592269_bin.17\nd__Bacteria p__Bacteroidota c__Ignavibacteria   o__SJA-28   f__B-1AR    g__CAIKZJ01 s__ ERR2592269_bin.18\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Bacteroidales    f__VadinHA17    g__SR-FBR-E99   s__SR-FBR-E99 sp018262715   ERR2592269_bin.19\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Weeksellaceae    g__Cloacibacterium  s__ ERR2592269_bin.2\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__CTSOIL-112   s__ ERR2592269_bin.20\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Enterobacterales f__Enterobacteriaceae   g__Klebsiella   s__Klebsiella quasipneumoniae   ERR2592269_bin.22\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__JAABTG01 f__JAABTG01 g__JAGNLM01 s__ ERR2592269_bin.23\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Xanthomonadales  f__Rhodanobacteraceae   g__ s__ ERR2592269_bin.24\nd__Bacteria p__Desulfobacterota c__Syntrophia   o__Syntrophales f__Fen-1087 g__Fen-1087 s__ ERR2592269_bin.25\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Xanthomonadales  f__Xanthomonadaceae g__Thermomonas  s__ ERR2592269_bin.26\nd__Bacteria p__Firmicutes   c__Bacilli  o__Lactobacillales  f__Streptococcaceae g__Streptococcus    s__ ERR2592269_bin.28\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Casimicrobiaceae g__VBCG01   s__ ERR2592269_bin.29\nd__Bacteria p__UBA1439  c__UBA1439  o__UBA1439  f__UBA1439  g__UBA1439  s__UBA1439 sp002329605  ERR2592269_bin.3\nd__Bacteria p__Spirochaetota    c__JAAYUW01 o__JAAYUW01 f__JAAYUW01 g__Exilispira   s__ ERR2592269_bin.30\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Neisseriaceae    g__Neisseria    s__ ERR2592269_bin.32\nd__Bacteria p__Desulfobacterota c__Syntrophia   o__Syntrophales f__Smithellaceae    g__UBA8904  s__UBA8904 sp002070455  ERR2592269_bin.35\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Brachymonas  s__Brachymonas sp018054915  ERR2592269_bin.4\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Acinetobacter    s__Acinetobacter harbinensis    ERR2592269_bin.5\nd__Bacteria p__Firmicutes   c__Bacilli  o__Lactobacillales  f__Streptococcaceae g__Streptococcus    s__ ERR2592269_bin.7\nd__Bacteria p__Firmicutes_A c__Clostridia   o__Tissierellales   f__Sedimentibacteraceae g__Sedimentibacter  s__Sedimentibacter sp017995755  ERR2592269_bin.8\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Acinetobacter    s__Acinetobacter parvus ERR2592269_bin.9\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Flavobacteriaceae    g__Flavobacterium   s__Flavobacterium sp017997335   ERR2592273_bin.1\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Acinetobacter    s__Acinetobacter pullicarnis    ERR2592273_bin.10\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Acinetobacter    s__ ERR2592273_bin.12\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Pseudomonadaceae g__Pseudomonas_E    s__ ERR2592273_bin.13\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Acinetobacter    s__Acinetobacter harbinensis    ERR2592273_bin.14\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Acidovorax   s__Acidovorax defluvii  ERR2592273_bin.15\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Flavobacteriaceae    g__Flavobacterium   s__ ERR2592273_bin.16\nd__Bacteria p__Campylobacterota c__Campylobacteria  o__Campylobacterales    f__Arcobacteraceae  g__Aliarcobacter    s__Aliarcobacter acticola   ERR2592273_bin.17\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Acinetobacter    s__ ERR2592273_bin.19\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Comamonas    s__ ERR2592273_bin.2\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Acinetobacter    s__Acinetobacter celticus   ERR2592273_bin.20\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Flavobacteriaceae    g__Flavobacterium   s__ ERR2592273_bin.21\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Flavobacteriaceae    g__Flavobacterium   s__ ERR2592273_bin.22\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Pseudomonadaceae g__Pseudomonas_E    s__ ERR2592273_bin.26\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Flavobacteriaceae    g__Flavobacterium   s__ ERR2592273_bin.27\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Evansiales   f__ g__ s__ ERR2592273_bin.28\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Flavobacteriaceae    g__Flavobacterium   s__ ERR2592273_bin.3\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Weeksellaceae    g__Bergeyella_A s__ ERR2592273_bin.4\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Comamonas    s__ ERR2592273_bin.5\nd__Bacteria p__Actinobacteriota c__Actinomycetia    o__Actinomycetales  f__Microbacteriaceae    g__ s__ ERR2592273_bin.6\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Flavobacteriaceae    g__Flavobacterium   s__Flavobacterium sp017989775   ERR2592273_bin.7\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Weeksellaceae    g__Kaistella    s__Kaistella chaponensis    ERR2592273_bin.8\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Psychrobacter    s__Psychrobacter faecalis   ERR2592273_bin.9\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Ottowia  s__ ERR2592274_bin.1\nd__Bacteria p__Actinobacteriota c__Actinomycetia    o__Mycobacteriales  f__Mycobacteriaceae g__Rhodococcus  s__Rhodococcus qingshengii  ERR2592274_bin.11\nd__Bacteria p__Actinobacteriota c__Actinomycetia    o__Actinomycetales  f__Dermatophilaceae g__Ornithinibacter  s__Ornithinibacter sp017989115  ERR2592274_bin.12\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Formosimonas s__ ERR2592274_bin.13\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Acidovorax   s__Acidovorax defluvii  ERR2592274_bin.14\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Acidovorax   s__Acidovorax temperans ERR2592274_bin.15\nd__Bacteria p__Firmicutes   c__Bacilli  o__Lactobacillales  f__Streptococcaceae g__Lactococcus_A    s__Lactococcus_A raffinolactis  ERR2592274_bin.16\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__JAGOEY01 s__ ERR2592274_bin.17\nd__Bacteria p__Fusobacteriota   c__Fusobacteriia    o__Fusobacteriales  f__Leptotrichiaceae g__JAGOYM01 s__JAGOYM01 sp018059165 ERR2592274_bin.18\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Moraxella_A  s__Moraxella_A sp002478835  ERR2592274_bin.19\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Acinetobacter    s__ ERR2592274_bin.2\nd__Bacteria p__Chloroflexota    c__Anaerolineae o__Anaerolineales   f__Anaerolineaceae  g__Brevefilum   s__Brevefilum fermentans    ERR2592274_bin.20\nd__Bacteria p__Firmicutes_A c__Clostridia   o__Oscillospirales  f__Ruminococcaceae  g__Faecalibacterium s__Faecalibacterium prausnitzii_G   ERR2592274_bin.21\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Weeksellaceae    g__Kaistella    s__Kaistella chaponensis    ERR2592274_bin.22\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Flavobacteriaceae    g__Flavobacterium   s__Flavobacterium sp017997335   ERR2592274_bin.23\nd__Bacteria p__Firmicutes   c__Bacilli  o__Lactobacillales  f__Aerococcaceae    g__Trichococcus s__Trichococcus flocculiformis  ERR2592274_bin.25\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Veillonellales   f__Dialisteraceae   g__Dialister    s__ ERR2592274_bin.26\nd__Bacteria p__Firmicutes_A c__Clostridia   o__TANB77   f__CAG-508  g__CAG-273  s__ ERR2592274_bin.27\nd__Bacteria p__Firmicutes_A c__Clostridia   o__Oscillospirales  f__Acutalibacteraceae   g__Ruminococcus_E   s__Ruminococcus_E sp003526955   ERR2592274_bin.3\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Veillonellales   f__Dialisteraceae   g__Dialister    s__Dialister invisus    ERR2592274_bin.4\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Hydromonas   s__ ERR2592274_bin.5\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Acinetobacter    s__ ERR2592274_bin.6\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Veillonellales   f__Dialisteraceae   g__Dialister    s__Dialister invisus    ERR2592274_bin.9\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Veillonellales   f__Dialisteraceae   g__Dialister    s__ ERR2592276_bin.1\nd__Bacteria p__Firmicutes   c__Bacilli  o__Lactobacillales  f__Streptococcaceae g__Streptococcus    s__Streptococcus parasuis   ERR2592276_bin.10\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Neisseriaceae    g__Neisseria    s__Neisseria suis   ERR2592276_bin.11\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Brachymonas  s__ ERR2592276_bin.12\nd__Bacteria p__Fusobacteriota   c__Fusobacteriia    o__Fusobacteriales  f__Leptotrichiaceae g__Sebaldella   s__Sebaldella termitidis    ERR2592276_bin.13\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Comamonas    s__Comamonas denitrificans_A    ERR2592276_bin.14\nd__Bacteria p__Firmicutes_A c__Clostridia   o__Oscillospirales  f__Ruminococcaceae  g__Gemmiger s__Gemmiger qucibialis  ERR2592276_bin.16\nd__Bacteria p__Fusobacteriota   c__Fusobacteriia    o__Fusobacteriales  f__Leptotrichiaceae g__JAGOYM01 s__ ERR2592276_bin.17\nd__Bacteria p__Firmicutes_A c__Clostridia   o__Peptostreptococcales f__Filifactoraceae  g__Proteocatella    s__Proteocatella sp009929415    ERR2592276_bin.18\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Neisseriaceae    g__Neisseria    s__ ERR2592276_bin.19\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Acidovorax   s__Acidovorax temperans ERR2592276_bin.2\nd__Bacteria p__Firmicutes_A c__Clostridia   o__Lachnospirales   f__Lachnospiraceae  g__Agathobacter s__Agathobacter rectalis    ERR2592276_bin.20\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Enterobacterales f__Pasteurellaceae  g__Chelonobacter    s__ ERR2592276_bin.21\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Veillonellales   f__Dialisteraceae   g__Dialister    s__Dialister invisus    ERR2592276_bin.22\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Pseudomonadales  f__Moraxellaceae    g__Moraxella    s__ ERR2592276_bin.23\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Brachymonas  s__Brachymonas sp017996515  ERR2592276_bin.24\nd__Bacteria p__Bacteroidota c__Bacteroidia  o__Flavobacteriales f__Weeksellaceae    g__Cloacibacterium  s__Cloacibacterium caeni_A  ERR2592276_bin.27\nd__Bacteria p__Campylobacterota c__Campylobacteria  o__Campylobacterales    f__Arcobacteraceae  g__Aliarcobacter    s__Aliarcobacter cryaerophilus_A    ERR2592276_bin.28\nd__Bacteria p__Firmicutes_A c__Clostridia   o__Lachnospirales   f__Lachnospiraceae  g__Blautia_A    s__Blautia_A wexlerae   ERR2592276_bin.29\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Burkholderiaceae g__Ottowia  s__Ottowia beijingensis ERR2592276_bin.3\nd__Bacteria p__Firmicutes_C c__Negativicutes    o__Selenomonadales  f__JAGPMH01 g__JAGPMH01 s__JAGPMH01 sp018052945 ERR2592276_bin.30\nd__Bacteria p__Firmicutes_A c__Clostridia   o__Lachnospirales   f__Lachnospiraceae  g__Eisenbergiella   s__Eisenbergiella sp900066775   ERR2592276_bin.31\nd__Bacteria p__Firmicutes_A c__Clostridia   o__Oscillospirales  f__Ruminococcaceae  g__Faecalibacterium s__Faecalibacterium prausnitzii_C   ERR2592276_bin.4\nd__Bacteria p__Proteobacteria   c__Gammaproteobacteria  o__Burkholderiales  f__Neisseriaceae    g__Neisseria    s__ ERR2592276_bin.5\nd__Bacteria p__Firmicutes   c__Bacilli  o__Lactobacillales  f__Streptococcaceae g__Streptococcus    s__ ERR2592276_bin.6\nd__Bacteria p__Fusobacteriota   c__Fusobacteriia    o__Fusobacteriales  f__Leptotrichiaceae g__JAGOYM01 s__ ERR2592276_bin.7\nd__Bacteria p__Firmicutes_A c__Clostridia   o__Peptostreptococcales f__Filifactoraceae  g__Acetoanaerobium  s__Acetoanaerobium noterae  ERR2592276_bin.8\nd__Bacteria p__Desulfobacterota c__Desulfobulbia    o__Desulfobulbales  f__Desulfobulbaceae g__Desulfobulbus    s__ ERR2592276_bin.9\n</code></pre> GTDB assignment of all samples that were produced by the magAttributes module.</p> <pre><code>BIN_ID  PATH\nERR2592244_bin.27   https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592244_bin.27.fa.xml\nERR2592244_bin.33   https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592244_bin.33.fa.xml\nERR2592245_bin.18   https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592245_bin.18.fa.xml\nERR2592249_bin.19   https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592249_bin.19.fa.xml\nERR2592249_bin.25   https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592249_bin.25.fa.xml\nERR2592249_bin.27   https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592249_bin.27.fa.xml\nERR2592249_bin.29   https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592249_bin.29.fa.xml\nERR2592249_bin.31   https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592249_bin.31.fa.xml\nERR2592249_bin.38   https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592249_bin.38.fa.xml\nERR2592251_bin.11   https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592251_bin.11.fa.xml\nERR2592251_bin.12   https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592251_bin.12.fa.xml\nERR2592251_bin.13   https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592251_bin.13.fa.xml\nERR2592252_bin.11   https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592252_bin.11.fa.xml\nERR2592252_bin.16   https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592252_bin.16.fa.xml\nERR2592252_bin.19   https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592252_bin.19.fa.xml\nERR2592252_bin.25   https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592252_bin.25.fa.xml\nERR2592252_bin.35   https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592252_bin.35.fa.xml\nERR2592252_bin.44   https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592252_bin.44.fa.xml\nERR2592252_bin.5    https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592252_bin.5.fa.xml\nERR2592252_bin.6    https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592252_bin.6.fa.xml\nERR2592253_bin.11   https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592253_bin.11.fa.xml\nERR2592253_bin.13   https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592253_bin.13.fa.xml\nERR2592253_bin.23   https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592253_bin.23.fa.xml\nERR2592253_bin.35   https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592253_bin.35.fa.xml\nERR2592253_bin.37   https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592253_bin.37.fa.xml\nERR2592253_bin.4    https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592253_bin.4.fa.xml\nERR2592253_bin.6    https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592253_bin.6.fa.xml\nERR2592253_bin.9    https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592253_bin.9.fa.xml\nERR2592258_bin.2    https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592258_bin.2.fa.xml\nERR2592258_bin.21   https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592258_bin.21.fa.xml\nERR2592258_bin.3    https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592258_bin.3.fa.xml\nERR2592258_bin.5    https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592258_bin.5.fa.xml\nERR2592258_bin.56   https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592258_bin.56.fa.xml\nERR2592260_bin.16   https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592260_bin.16.fa.xml\nERR2592262_bin.5    https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592262_bin.5.fa.xml\nERR2592262_bin.6    https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592262_bin.6.fa.xml\nERR2592264_bin.1    https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592264_bin.1.fa.xml\nERR2592265_bin.11   https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592265_bin.11.fa.xml\nERR2592265_bin.13   https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592265_bin.13.fa.xml\nERR2592265_bin.7    https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592265_bin.7.fa.xml\nERR2592265_bin.9    https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592265_bin.9.fa.xml\nERR2592268_bin.23   https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592268_bin.23.fa.xml\nERR2592268_bin.38   https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592268_bin.38.fa.xml\nERR2592268_bin.45   https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592268_bin.45.fa.xml\nERR2592269_bin.12   https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592269_bin.12.fa.xml\nERR2592269_bin.19   https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592269_bin.19.fa.xml\nERR2592269_bin.4    https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592269_bin.4.fa.xml\nERR2592274_bin.18   https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592274_bin.18.fa.xml\nERR2592274_bin.3    https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592274_bin.3.fa.xml\nERR2592276_bin.23   https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592276_bin.23.fa.xml\nERR2592276_bin.28   https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592276_bin.28.fa.xml\nERR2592276_bin.5    https://openstack.cebitec.uni-bielefeld.de:8080/meta_test/medium/cooccurrence/ERR2592276_bin.5.fa.xml\n</code></pre> <p>The following parameters can be configured:</p> <ul> <li> <p>metabolicEdgeBatches: Batches of edges that are provided as input to SMETANA.</p> </li> <li> <p>metabolicEdgeReplicates: Number of replicates per edge that should be computed.</p> </li> </ul>"},{"location":"modules/cooccurrence/#output","title":"Output","text":"<ul> <li> <p>output_raw.graphml: Cooccurrence network unfiltered in graphml format</p> </li> <li> <p>output.graphml: Filtered cooccurrence network in graphml format</p> </li> <li> <p>edges_index.tsv: Edges of the graph</p> </li> <li> <p>edgeAttributes.tsv: Edge attributes of the graph containing metrics computed by SMETANA.</p> </li> </ul>"},{"location":"modules/cooccurrence/#spiec-easi","title":"SPIEC-EASI","text":"<ul> <li>stability.txt: Network stability estimation</li> </ul>"},{"location":"modules/dereplication/","title":"Dereplication","text":""},{"location":"modules/dereplication/#input","title":"Input","text":"CommandConfiguration FileTSV Table <pre><code>-entry wDereplication -params-file example_params/dereplication.yml\n</code></pre> <p>Warning</p> <p>The configuration file shown here is for demonstration and testing purposes only.    Parameters that should be used in production can be viewed in the dereplication section    of one of the yaml files located in the <code>default</code> folder of the Toolkit's Github repository.</p> <pre><code>output: \"output\"\ns3SignIn: false\nrunid: 1\nlogLevel: 1\nlogDir: log\nscratch: \"/vol/scratch\"\npublishDirMode: \"symlink\"\nsteps:\n  dereplication:\n    bottomUpClustering:\n      input: \"test_data/dereplication/input.tsv\"\n      minimumCompleteness: 0\n      maximumContamination: 5000\n      ANIBuffer: 20\n      mashBuffer: 2000\n      method: 'ANI'\n      additionalParams:\n        mash_sketch: \"\"\n        mash_dist: \"\"\n        #  cluster cutoff\n        cluster: \" -c 0.05 \"\n        pyani: \" -m ANIb \"\n        representativeAniCutoff: 0.95\n    sans:\n      additionalParams: \" -k 15 -f strict -w 25  -t 400 \" \nresources:\n  highmemLarge:\n    cpus: 28\n    memory: 230\n  highmemMedium:\n    cpus: 14\n    memory: 113\n  large:\n    cpus: 28\n    memory: 58\n  medium:\n    cpus: 14\n    memory: 29\n  small:\n    cpus: 7\n    memory: 14\n  tiny:\n    cpus: 1\n    memory: 1\n</code></pre> <p><pre><code>DATASET BIN_ID  PATH    COMPLETENESS    CONTAMINATION   COVERAGE    N50 HETEROGENEITY\ntest3   test3_bin.1 https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/bins/bin.1.fa    100 0   10  5000    10\ntest1   test1_bin.2 https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/bins/bin.2.fa    100 0   10  5000    10\ntest1   test1_bin.8 https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/bins/bin.8.fasta 100 0   10  5000    10\ntest2   test2_bin.9 https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/bins/bin.9.fasta 100 0   10  5000    10\ntest3   test2_bin.10    https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/bins/bin.9.fasta 100 0   10  5000    10\ntest2   test2_bin.32    https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/bins/bin.32.fa   100 0   10  5000    10\n</code></pre> Must include the columns <code>DATASET</code>, <code>BIN_ID</code>, <code>PATH</code>, <code>COMPLETENESS</code>, <code>CONTAMINATION</code>, <code>COVERAGE</code>, <code>N50</code> and <code>HETEROGENEITY</code>.  Completeness and contamination can be used for filtering (see <code>params-file</code>). <code>N50</code>, <code>COVERAGE</code> and <code>HETEROGENEITY</code> are used for selecting the representative of every cluster. You can set values of these columns to zero if data is not available or if you don't want the representative selection to be influenced by theses columns. Make sure that <code>BIN_ID</code> is a unique identifier.</p>"},{"location":"modules/dereplication/#output","title":"Output","text":"<p>The output tsv file (<code>clusters.tsv</code>in the cluster's folder) contains the columns <code>CLUSTER</code>, <code>GENOME</code> and <code>REPRESENTATIVE</code> where <code>CLUSTER</code> identifies a group of genomes, <code>GENOME</code> represents the path or link of a genome and <code>REPRESENTATIVE</code> is either 0 or 1 (selected as representative). If <code>sans</code> is specified in the configuration file (see examples folder), then SANS is used to dereplicate the genomes of every cluster that was reported by the previous step.  The SANS output can be found in the <code>sans</code> folder.</p>"},{"location":"modules/export/","title":"Exploratory Metagenome Browser (EMGB)","text":"<p>The output generated by the Metagenomics-Toolkit can be imported into EMGB. EMGB allows you to easily explore your metagenomic samples in terms of MAGs, genes and their function.</p> <p>You can either directly specify the export to EMGB in your config file when you analyse your samples, or you can execute the export afterwards by running the Toolkit again with the export entry point. </p> <p>Currently, the export for EMGB needs the results of the following tools:</p> <ol> <li>Assembly</li> <li>Binning</li> <li>Checkm (v1 or v2)</li> <li>Prokka output</li> <li>GTDB-Tk </li> <li>MMseqs Taxonomy (Database: GTDB)</li> <li>MMseqs (Database: UniRef90) </li> </ol>"},{"location":"modules/export/#input","title":"Input","text":"CommandConfiguration File <pre><code>-entry wExportPipeline -params-file example_params/export.yml\n</code></pre> <p>Warning</p> <p>The configuration file shown here is for demonstration and testing purposes only.    Parameters that should be used in production can be viewed in the read mapping section    of one of the yaml files located in the <code>default</code> folder of the Toolkit's Github repository.</p> <pre><code>tempdir: \"tmp\"\ns3SignIn: false\ninput: \"output\"\noutput: \"output\"\nlogDir: log\nrunid: 1\ndatabases: \"/mnt/databases\"\nlogLevel: 1\nscratch: \"/vol/scratch\"\npublishDirMode: \"symlink\"\nsteps:\n  export:\n    emgb:\n      additionalParams:\n              blastDB: \"bacmet20_predicted\"\n              taxonomyDB: \"gtdb\"\n      titles:\n        database:\n          download:\n            source: \"https://openstack.cebitec.uni-bielefeld.de:8080/databases/uniref90.titles.202506.tsv.gz\"\n            md5sum: 65437094361e84756e156c54d3a6e56b\n      kegg:\n        database:\n          download:\n            source: s3://databases_internal/annotatedgenes2json_db_kegg-mirror-2025-05.tar.zst\n            md5sum: 94169a92d453f55553aa7edf4546d00d\n            s5cmd:\n              params: '--retry-count 30 --no-verify-ssl --endpoint-url https://openstack.cebitec.uni-bielefeld.de:8080'\nresources:\n  highmemLarge:\n    cpus: 28\n    memory: 230\n  highmemMedium:\n    cpus: 14\n    memory: 113\n  large:\n    cpus: 28\n    memory: 58\n  medium:\n    cpus: 14\n    memory: 29\n  small:\n    cpus: 7\n    memory: 14\n  tiny:\n    cpus: 1\n    memory: 1\n</code></pre>"},{"location":"modules/export/#additional-parameters","title":"Additional Parameters","text":"<ul> <li> <p>blastDB: The toolkit runs MMseqs against multiple databases. You can specify here, which BLAST output should be used. (Default: UniRef90)</p> </li> <li> <p>taxonomyDB: MMseqs is executed against a specific taxonomy database. (Default: GTDB)   </p> </li> </ul>"},{"location":"modules/export/#output","title":"Output","text":"<p>The following files are produced as output:</p> <ul> <li>SAMPLE.bins.json.gz  </li> <li>SAMPLE.contigs.json.gz</li> <li>SAMPLE.genes.json.gz</li> </ul> <p>where <code>SAMPLE</code> is the name of the sample.</p> <p>You can read more here about how to  start EMGB and use these files to import a dataset. </p>"},{"location":"modules/fragment_recruitment/","title":"Run Fragment Recruitment","text":"<p>The fragment recruitment module can be used to find genomes in a set of read datasets.</p> <p>In case the fragment recruitment module is part of the full pipeline pr per-sample pipeline configuration then reads that could not be mapped back to a contig are screened for a user provided list of MAGs. Detected genomes are included in all other parts of the remaining pipeline. Look out for their specific headers to differentiate results based on real assembled genomes and the reference genomes.</p> <p>Note: This module currently only supports illumina data. </p>"},{"location":"modules/fragment_recruitment/#input","title":"Input","text":"CommandConfiguration file for fragment recruitment via mash screen and BWAConfiguration file for fragment recruitment via mash screen and BowtieInput TSV file for genomesInput TSV file for paired end readsInput TSV file for single end reads <pre><code>-entry wFragmentRecruitment -params-file example_params/fragmentRecruitment.yml\n</code></pre> <p>Warning</p> <p>The configuration file shown here is for demonstration and testing purposes only.    Parameters that should be used in production can be viewed in the fragment recruitment section    of one of the yaml files located in the <code>default</code> folder of the Toolkit's Github repository.</p> <pre><code>tempdir: \"tmp\"\ns3SignIn: false\noutput: \"output\"\nlogDir: log\nrunid: 1\nlogLevel: 1\nscratch: \"/vol/scratch\"\npublishDirMode: \"symlink\"\nsteps:\n  fragmentRecruitment:\n    mashScreen:\n      samples:\n        paired: test_data/fragmentRecruitment/paired.tsv\n        single: test_data/fragmentRecruitment/single.tsv\n      genomes: test_data/fragmentRecruitment/mags.tsv\n      unzip:\n        timeLimit: \"AUTO\"\n      additionalParams:\n        mashSketch: \" \"\n        mashScreen: \" \"\n        bwa2: \" \"\n        minimap: \" \"\n        coverm: \"  --min-covered-fraction 0  \"\n        covermONT: \"  --min-covered-fraction 0 \"\n        samtoolsViewBwa2: \" -F 3584 \" \n        samtoolsViewMinimap: \" \" \n      mashDistCutoff: 0.70\n      coveredBasesCutoff: 0.2\n      mashHashCutoff: 2\n    genomeCoverage:\n      additionalParams: \"\"\n    contigsCoverage:\n      additionalParams: \"\"\nresources:\n  highmemLarge:\n    cpus: 28\n    memory: 230\n  highmemMedium:\n    cpus: 14\n    memory: 113\n  large:\n    cpus: 28\n    memory: 58\n  medium:\n    cpus: 14\n    memory: 29\n  small:\n    cpus: 7\n    memory: 14\n  tiny:\n    cpus: 1\n    memory: 1\n</code></pre> <p>Warning</p> <p>The configuration file shown here is for demonstration and testing purposes only.    Parameters that should be used in production can be viewed in the fragment recruitment section    of one of the yaml files located in the <code>default</code> folder of the Toolkit's Github repository.</p> <pre><code>tempdir: \"tmp\"\ns3SignIn: false\noutput: \"output\"\nlogDir: log\nrunid: 1\nlogLevel: 1\nscratch: \"/vol/scratch\"\nsteps:\n  fragmentRecruitment:\n    mashScreen:\n      samples:\n        paired: test_data/fragmentRecruitment/paired.tsv\n        single: test_data/fragmentRecruitment/single.tsv\n      genomes: test_data/fragmentRecruitment/mags.tsv\n      unzip:\n        timeLimit: \"AUTO\"\n      additionalParams:\n        mashSketch: \" \"\n        mashScreen: \" \"\n        bowtie: \" \"\n        minimap: \" \"\n        coverm: \"  --min-covered-fraction 0  \"\n        covermONT: \"  --min-covered-fraction 0  \"\n        samtoolsViewBowtie: \" -F 3584 \" \n        samtoolsViewMinimap: \" \" \n      mashDistCutoff: 0.70\n      coveredBasesCutoff: 0.2\n      mashHashCutoff: 2\n    genomeCoverage:\n      additionalParams: \"\"\n    contigsCoverage:\n      additionalParams: \"\"\n\nresources:\n  highmemLarge:\n    cpus: 28\n    memory: 230\n  highmemMedium:\n    cpus: 14\n    memory: 113\n  large:\n    cpus: 28\n    memory: 58\n  medium:\n    cpus: 14\n    memory: 29\n  small:\n    cpus: 7\n    memory: 14\n  tiny:\n    cpus: 1\n    memory: 1\n</code></pre> <pre><code>PATH\nhttps://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/bins/bin.1.fa\nhttps://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/bins/bin.2.fa\nhttps://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/bins/bin.8.fasta\nhttps://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/bins/bin.9.fasta\nhttps://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/bins/test234.fa\nhttps://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/bins/bin.32.fa\n</code></pre> <pre><code>SAMPLE  READS\ntest1   https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/small/interleaved.fq.gz\ntest2   https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/small/interleaved.fq.gz\n</code></pre> <pre><code>SAMPLE  READS\ntest1   https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/small/read1_1.fq.gz\n</code></pre> <p>NOTE! The file names of all provided genomes must be unique.</p> <p>The following parameters can be configured:</p> <ul> <li> <p>mashDistCutoff: All hits below this threshold are  discarded.</p> </li> <li> <p>mashHashCutoff: All hits that have a lower count of matched minimum hashes are discarded.</p> </li> <li> <p>coveredBasesCutoff: Number of bases that must be covered by at least one read. By how many reads     the bases must be covered can be configured via the coverm setting (coverm: \"  --min-covered-fraction 0  \").</p> </li> </ul>"},{"location":"modules/fragment_recruitment/#output","title":"Output","text":"<p>The module outputs mash screen and bowtie alignment statistics.  Furthermore, the module provides a coverm output which basically reports all metrics about the found genomes (e.g covered bases,length, tpm, ...).</p>"},{"location":"modules/introduction/","title":"Introduction","text":"<p>Every module of the Metagenomics-Toolkit can be executed independent of any other module. This section defines the input YAML, input TSV file and the files that are output. The explanation of how to use the Toolkit and any configuration parameters can be found in the Getting Started and the Configuration section.</p>"},{"location":"modules/magAttributes/","title":"MagAttributes","text":""},{"location":"modules/magAttributes/#input","title":"Input","text":"CommandConfiguration FileMAGs TSV Table <pre><code>-entry wMagAttributes -params-file example_params/magAttributes.yml \n</code></pre> <p>Warning</p> <p>The configuration file shown here is for demonstration and testing purposes only.    Parameters that should be used in production can be viewed in the magAttributes section    of one of the yaml files located in the <code>default</code> folder of the Toolkit's Github repository.</p> <pre><code>tempdir: \"tmp\"\noutput: \"output\"\nlogDir: log\nrunid: 1\ns3SignIn: false\nlogLevel: 1\nscratch: \"/vol/scratch\"\npublishDirMode: \"symlink\"\nsteps:\n  magAttributes:\n    input: \"test_data/magAttributes/input.tsv\"\n    gtdb:\n      buffer: 1000\n      database:\n        download:\n          source: https://openstack.cebitec.uni-bielefeld.de:8080/databases/gtdbtk_r226_v2_data.tar.gz\n          md5sum: 0777ea44f9d7d96f9ca6ea43ed016799\n      additionalParams: \" --min_af 0.65 --scratch_dir . \"\n    checkm2:\n      database:\n        download:\n          source: \"https://openstack.cebitec.uni-bielefeld.de:8080/databases/checkm2_v2.tar.gz\"\n          md5sum: a634cb3d31a1f56f2912b74005f25f09\n      additionalParams: \"  \"\n    checkm:\n      database:\n        download:\n          source: \"https://openstack.cebitec.uni-bielefeld.de:8080/databases/checkm_data_2015_01_16.tar.gz\"\n          md5sum: 0963b301dfe9345ea4be1246e32f6728\n      buffer: 200\n      additionalParams:\n        tree: \" --reduced_tree \"\n        lineage_set: \" \" \n        qa: \"  \"\n    prokka:\n      defaultKingdom: false\n      additionalParams: \" --mincontiglen 200 \"\n\nresources:\n  highmemLarge:\n    cpus: 28\n    memory: 230\n  highmemMedium:\n    cpus: 14\n    memory: 113\n  large:\n    cpus: 28\n    memory: 58\n  medium:\n    cpus: 14\n    memory: 29\n  small:\n    cpus: 7\n    memory: 14\n  tiny:\n    cpus: 1\n    memory: 1\n</code></pre> <p><pre><code>DATASET BIN_ID  PATH    COMPLETENESS    CONTAMINATION   COVERAGE    N50 HETEROGENEITY\ntest1   bin.1   https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/bins/bin.1.fa    100 0   10  5000    10\ntest1   bin.2   https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/bins/bin.2.fa    100 0   10  5000    10\ntest1   bin.8   https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/bins/bin.8.fasta 100 0   10  5000    10\ntest2   bin.9   https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/bins/bin.9.fasta 100 0   10  5000    10\ntest2   bin.32  https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/bins/bin.32.fa   100 0   10  5000    10\n</code></pre> Must include at least <code>DATASET</code> identifier and mag specific <code>PATH</code> and <code>BIN_ID</code> column.</p>"},{"location":"modules/magAttributes/#databases","title":"Databases","text":"<p>CheckM and GTDB need their databases as input. See database section for possibly download strategies. The GTDB and CheckM compressed databases must consist of tar.gz files. </p> <p>If you need credentials to access your files via S3 then please use the following command:</p> <p>For GTDB: <pre><code>nextflow secrets set S3_gtdb_ACCESS XXXXXXX\nnextflow secrets set S3_gtdb_SECRET XXXXXXX\n</code></pre></p> <p>For Checkm: <pre><code>nextflow secrets set S3_checkm_ACCESS XXXXXXX\nnextflow secrets set S3_checkm_SECRET XXXXXXX\n</code></pre></p>"},{"location":"modules/magAttributes/#gtdb-tk","title":"GTDB-Tk","text":"<p>If you provide the extracted version of GTDB using the <code>extractedDBPath</code> parameter, please specify the path to the <code>releasesXXX</code> directory (e.g. \"/vol/spool/gtdb/release202\").</p> <p>To create a new GTDB database, optionally add the <code>metadata_genomes</code> directory to the GTDB release directory. This directory must contain the <code>ar53_metadata.tsv.gz</code> and <code>bac120_metadata.tsv.gz</code> metadata files. These can be downloaded from the GTDB release page. In addition, if a MASH directory containing a MASH index of the GTDB reference genomes is found, it is provided as input to GTDB-tk. Otherwise, GTDB-tk will build an index on the fly.</p>"},{"location":"modules/magAttributes/#output","title":"Output","text":""},{"location":"modules/magAttributes/#gtdb-tk_1","title":"GTDB-Tk","text":"<p>All GTDB files include the GTDB specific columns in addition to a <code>SAMPLE</code> column (<code>SAMPLE_gtdbtk.bac120.summary.tsv</code>, <code>SAMPLE_gtdbtk.ar122.summary.tsv</code>). In addition, this module produces a file <code>SAMPLE_gtdbtk_CHUNK.tsv</code> that combines both files and adds a <code>BIN_ID</code> column that adheres to the magAttributes specification. The raw output from each GTDB-tk run can also be found in the raw_output_CHUNK directories.</p> <p>The <code>gtdb_to_ncbi_majority_vote.py</code> script is executed on every GTDB-tk output directory. The output is saved in the <code>chunk_CHUNK_SAMPLE_gtdb_to_ncbi_majority_vote.tsv</code> file. </p>"},{"location":"modules/magAttributes/#checkm-and-checkm2","title":"CheckM and CheckM2","text":"<p>The Checkm and Checkm2 output adheres to the magAttributes specification and adds a <code>BIN_ID</code> and <code>SAMPLE</code> column to the output file. If Checkm2 and Checkm are both specified in the config file then only the Checkm2 results are used for downstream pipeline steps.</p>"},{"location":"modules/metabolomics/","title":"Metabolomics","text":"<p>The metabolomics module runs genome scale metabolic modeling analysis based on a supplied genome or directly on its proteins. The module is able to use gapseq and carveme for analysing genomes and carveme for analysing predicted proteins which depends on the configuration you provide as input.</p> <p>Note: If carvem is specificed in fullPipeline mode then carveme is executed with proteins as input.</p> <p>All generated models are used for further downstream analysis such as the \"Minimum Resource Overlap\" computation by smetana.</p>"},{"location":"modules/metabolomics/#input","title":"Input","text":"CommandConfiguration file for providing genomes <pre><code>-entry wMetabolomics -params-file example_params/metabolomics\n</code></pre> <p>Warning</p> <p>The configuration file shown here is for demonstration and testing purposes only.    Parameters that should be used in production can be viewed in the metabolomics section    of one of the yaml files located in the <code>default</code> folder of the Toolkit's Github repository.</p> <pre><code>tempdir: \"tmp\"\ns3SignIn: false\ninput:\n  paired:\n    path: \"test_data/fullPipeline/reads_split.tsv\"\n    watch: false\noutput: \"output\"\nlogDir: log\nrunid: 1\ndatabases: \"/mnt/databases\"\nlogLevel: 1\nscratch: \"/vol/scratch\"\nsteps:\n  metabolomics:\n     input:\n       bins: \"test_data/metabolomics/input.tsv\" \n     filter:\n       minCompleteness: 49\n       maxContamination: 5\n     carveme:\n       additionalParams: \" --solver scip \"\n     smetana:\n       beforeProcessScript: /vol/spool/dockerPull.sh\n       global: true\n       detailed: true\n       additionalParams:\n         detailed: \"\"\n         global: \"\"\n     memote:\n       beforeProcessScript: /vol/spool/dockerPull.sh\n       additionalParams:\n         run: \"\"\n         report: \"\"\nresources:\n  highmemLarge:\n    cpus: 28\n    memory: 230\n  highmemMedium:\n    cpus: 14\n    memory: 113\n  large:\n    cpus: 28\n    memory: 58\n  medium:\n    cpus: 14\n    memory: 29\n  small:\n    cpus: 7\n    memory: 14\n  tiny:\n    cpus: 1\n    memory: 1\n</code></pre> <p>Almost all tools of this module are using linear programming solvers. The tool developers are recommending the use of the cplex solver that is included in the IBM ILOG CPLEX Optimization Studio which is free for students and academics through the IBM Academic Initiative programm.  Since the toolkit uses docker images that are downloaded from public Docker Hub repositories and the cplex license is not allowed to be distributed, we prepared a Dockerfile (see <code>cplex/docker/Dockerfile</code> in the Github repository) that allows you to build your own local docker image with all metabolomics specific tools installed. Just copy your cplex binary to the cplex docker folder and build your own docker image. You can override all existing images via the command line.</p> <p>In the following example your the image name is metabolomics:0.1.0:</p> <ul> <li><code>--gapseq_image=metabolomics:0.1.0</code> (Optional)</li> <li><code>--smetana_image=metabolomics:0.1.0</code> (Required)</li> <li><code>--carveme_image=metabolomics:0.1.0</code> (Optional. Carveme is not able to detect the solver automatically. Please specify <code>--solver</code> in the configuration file if you want to use the scip solver.)</li> <li><code>--memote_image=metabolomics:0.1.0</code> (Optional. Memote is not able to detect the solver automatically. Please specify <code>--solver</code> in the configuration file if you are not using the glpk solver.)</li> </ul> <p>For gapseq and memote we are using a publicly available docker image that uses the freely available glkp solver which means that you don't have to provide this parameter. If you want to build your own image, please use the <code>beforeProcessScript</code> parameter. This parameter expects a bash script that accepts the docker image name as a parameter. The script is executed right before the actual docker image is started.  You could for example provide a script that builds the actual image right before running the tool on the VM.  It would be also possible to push the docker image to a private dockerhub repository and login to your docker account via this script. We provide two example template scripts in the cplex folder. Please note that in both cases you distribute the docker image with your cplex binary on all machines where you run the toolkit. If you login to dockerhub then your credentials will saved on the VM. If you are not the only docker user on the machine we do not recommend this approach! </p>"},{"location":"modules/metabolomics/#output","title":"Output","text":""},{"location":"modules/metabolomics/#gapseq-carveme","title":"GapSeq / CarveMe","text":"<p>Both tools are generating genome scale metabolic reconstruction models (<code>*.xml</code>).  All models are translated to json format and substrats, products and reactions are saved in distinct files.</p>"},{"location":"modules/metabolomics/#memote","title":"Memote","text":"<p>Memote tests metabolic reconstruction models and therefore produces a machine readable json file  (<code>*_report.json.gz</code>) and a human readable tsv (<code>*_metrics.tsv</code>) and html (<code>*_report.html</code>) file.</p>"},{"location":"modules/metabolomics/#smetana","title":"Smetana","text":"<p>Smetana is used for analysing possible interactions in microbial communities. Smetana's global and detailed modes  are executed per sample. The Smetana output is saved in <code>*_detailed.tsv</code> and <code>*_global.tsv</code>.</p>"},{"location":"modules/plasmids/","title":"Plasmids","text":"<p>The plasmid module is able to identify contigs as plasmids and also to assemble plasmids from the samples fastq data. The module is executed in two parts. In the first part contigs of a metagenome assembler are scanned for plasmids. In the second part a plasmid assembler is used to assemble circular plasmids out of raw reads. All plasmid detection tools are executed on the circular assembly result and on the contigs of the metagenome assembler. Just the filtered sequences are used for downstream analysis. </p> <p>The identification of plasmids is based on the combined result of tools which have a <code>filter</code> property assigned. Results of all tools that have the <code>filter</code> property set to true are combined either by a logical <code>OR</code> or by a logical <code>AND</code>. </p> <p>Example for the <code>OR</code> and <code>AND</code> operations:  Let's assume that we have three plasmid detection tools (t1, t2, t3) that have four contigs (c1, c2, c3, c4) as input. Let's further assume that c1 and c2 are detected by all tools as contigs and c3 and c4 are only detected by t1 and t2. By using an <code>AND</code> only c1 and c2 are finally reported by the module as plasmids. By using an <code>OR</code> all contigs would be annotated as plasmids. </p> <p>It is also possible to simply run a tool without using its result as filter by setting <code>filter</code> to <code>false</code>. If a tool should not be executed then the tool section should be removed. Only the detected plasmids will be used for downstream analysis.</p> <p>For running a plasmid assembly we suggest running the full pipeline mode with the enabled plasmids module. See input example configuration files. The read mapper can either be Bowtie or Bwa for Illumina and minimap for long reads.  </p>"},{"location":"modules/plasmids/#input","title":"Input","text":"CommandConfiguration file for full pipeline mode with plasmids detectionsConfiguration file for plasmids module onlyTSV Table <pre><code>-entry wPlasmidsPath -params-file example_params/plasmids.yml\n</code></pre> <p>Warning</p> <p>The configuration file shown here is for demonstration and testing purposes only.    Parameters that should be used in production can be viewed in the plasmids section    of one of the yaml files located in the <code>default</code> folder of the Toolkit's Github repository.</p> <pre><code>tempdir: \"tmp\"\ns3SignIn: true\ninput:\n  paired:\n    path: \"test_data/fullPipeline/reads_split.tsv\"\n    watch: false\noutput: \"output\"\nrunid: 1\nscratch: \"/vol/scratch\"\ndatabases: \"/mnt/databases/\"\nlogDir: log\npublishDirMode: \"symlink\"\nsteps:\n  qc:\n    interleaved: false\n    fastp:\n       # Example params: \" --cut_front --cut_tail --detect_adapter_for_pe  \"\n       additionalParams:\n         fastp: \"  \"\n         reportOnly: false\n       timeLimit: \"AUTO\"\n  assembly:\n    megahit:\n      fastg: false\n      additionalParams: \" --min-contig-len 200 \"\n      resources:\n         RAM: \n            mode: 'DEFAULT'\n            predictMinLabel: 'AUTO' \n  binning:\n    bowtie:\n      additionalParams: \n        bowtie: \" --quiet --very-sensitive \"\n        samtoolsView: \" -F 3584 \" \n    contigsCoverage:\n      additionalParams: \"\"\n    genomeCoverage:\n      additionalParams: \" \"\n    metabat:\n      additionalParams: \"   \"\n  plasmid:\n    SCAPP:\n      additionalParams: \n        SCAPP: \"  \"\n        bowtie: \"  \"\n        coverm: \"  \"\n        covermONT: \"  \"\n        minimap: \" \"\n        samtoolsViewBowtie: \" -F 3584 \" \n        samtoolsViewMinimap: \" \" \n    ViralVerifyPlasmid:\n      filter: true\n      filterString: \"Uncertain - plasmid or chromosomal|Plasmid\"\n      database:\n        download:\n          source: https://openstack.cebitec.uni-bielefeld.de:8080/databases/Pfam-A_37.4.hmm.gz\n          md5sum: 9ca99e4396bb7c123e884057e91eff8d\n      additionalParams: \" --thr 7 \"\n    MobTyper:\n      filter: true\n      minLength: 5000\n      additionalParams: \" --min_length 9000  \"\n      database:\n        download:\n          source: https://openstack.cebitec.uni-bielefeld.de:8080/databases/mob_20250620.tar.gz\n          md5sum: d73fce2d4fc03ca8466b8b1baf1e5634\n    Platon:\n      filter: false\n      database:\n        download:\n          source: https://openstack.cebitec.uni-bielefeld.de:8080/databases/platon_20220929.tar.gz\n          md5sum: f6d1701704396182c6c9daca053eb9d6\n      additionalParams: \"   \"\n    PlasClass:\n      filter: true\n      threshold: 0.5 \n      additionalParams: \"   \"\n    Filter:\n      method: \"AND\"\n      minLength: 0 \n    PLSDB:\n      sharedKmerThreshold: 30\n      database:\n        download:\n          source: https://openstack.cebitec.uni-bielefeld.de:8080/databases/plasmids_plsdb_20250618.tar.bz2\n          md5sum: c860e17ce5c0e87d7f21e57fbecdca91\n      additionalParams:\n        mashSketch: \" -S 42 -k 21 -s 1000 \"\n        mashDist: \" -v 0.2 -d 0.2 \"\nresources:\n  highmemLarge:\n    cpus: 28\n    memory: 230\n  highmemMedium:\n    cpus: 14\n    memory: 113\n  large:\n    cpus: 28\n    memory: 58\n  medium:\n    cpus: 14\n    memory: 29\n  small:\n    cpus: 7\n    memory: 14\n  tiny:\n    cpus: 1\n    memory: 1\n</code></pre> <p>Warning</p> <p>The configuration file shown here is for demonstration and testing purposes only.    Parameters that should be used in production can be viewed in the plasmids section    of one of the yaml files located in the <code>default</code> folder of the Toolkit's Github repository.</p> <pre><code>tempdir: \"tmp\"\ns3SignIn: false\noutput: \"output\"\nlogDir: log\nrunid: 1\ndatabases: \"/vol/scratch/databases/\"\nlogLevel: 1\nscratch: \"/vol/scratch\"\npublishDirMode: \"symlink\"\nsteps:\n  plasmid:\n    input: \"test_data/plasmid/input_contigs.tsv\"\n    ViralVerifyPlasmid:\n      filter: true\n      filterString: \"Uncertain - plasmid or chromosomal|Plasmid\"\n      database:\n        download:\n          source: https://openstack.cebitec.uni-bielefeld.de:8080/databases/pfam-A_35.0.hmm.gz\n          md5sum: c80b75bd48ec41760bbca19c70616e36\n      additionalParams: \" --thr 7 \"\n    MobTyper:\n      filter: true\n      minLength: 5000\n      additionalParams: \" --min_length 9000  \"\n      database:\n        download:\n          source: https://openstack.cebitec.uni-bielefeld.de:8080/databases/mob_20250620.tar.gz\n          md5sum: d73fce2d4fc03ca8466b8b1baf1e5634\n    Platon:\n      filter: false\n      database:\n        download:\n          source: https://openstack.cebitec.uni-bielefeld.de:8080/databases/platon_20220929.tar.gz\n          md5sum: f6d1701704396182c6c9daca053eb9d6\n      additionalParams: \"   \"\n    PlasClass:\n      filter: true\n      threshold: 0.5 \n      additionalParams: \"   \"\n    Filter:\n      method: \"AND\"\n      minLength: 0 \n    PLSDB:\n      sharedKmerThreshold: 30\n      database:\n        download:\n          source: https://openstack.cebitec.uni-bielefeld.de:8080/databases/plasmids_plsdb_20250618.tar.bz2\n          md5sum: c860e17ce5c0e87d7f21e57fbecdca91\n      additionalParams:\n        mashSketch: \" -S 42 -k 21 -s 1000 \"\n        mashDist: \" -v 0.2 -d 0.2 \"\n\nresources:\n  highmemLarge:\n    cpus: 28\n    memory: 230\n  highmemMedium:\n    cpus: 14\n    memory: 113\n  large:\n    cpus: 28\n    memory: 58\n  medium:\n    cpus: 14\n    memory: 29\n  small:\n    cpus: 7\n    memory: 14\n  tiny:\n    cpus: 1\n    memory: 1\n</code></pre> <pre><code>DATASET BIN_ID  PATH\ntest3   bin.1   https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/bins/bin.1.fa\ntest1   bin.2   https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/bins/bin.2.fa\ntest1   bin.8   https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/bins/bin.8.fasta\ntest2   bin.9   https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/bins/bin.9.fasta\ntest2   bin.32  https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/bins/bin.32.fa\n</code></pre>"},{"location":"modules/plasmids/#databases","title":"Databases","text":"<p>The plasmid module needs the following compressed database file formats: </p>"},{"location":"modules/plasmids/#viralverifyplasmid","title":"ViralVerifyPlasmid","text":"<p>ViralVerifyPlasmid needs a recent pfam-A database in .gz format. See database section for possible download strategies. If you need credentials to access your files via S3 then please use the following command:</p> <pre><code>nextflow secrets set S3_ViralVerifyPlasmid_ACCESS XXXXXXX\nnextflow secrets set S3_ViralVerifyPlasmid_SECRET XXXXXXX\n</code></pre>"},{"location":"modules/plasmids/#mobtyper","title":"MobTyper","text":"<p>Database was generated by gzipping the output of mob_init. See database section for possible download strategies. If you need credentials to access your files via S3 then please use the following command:</p> <pre><code>nextflow secrets set S3_MobTyper_ACCESS XXXXXXX\nnextflow secrets set S3_MobTyper_SECRET XXXXXXX\n</code></pre>"},{"location":"modules/plasmids/#platon","title":"Platon","text":"<p>The tar gzipped database for running platon can be fetched from the Platon github page. See database section for possible download strategies. If you need credentials to access your files via S3 then please use the following command:</p> <pre><code>nextflow secrets set S3_Platon_ACCESS XXXXXXX\nnextflow secrets set S3_Platon_SECRET XXXXXXX\n</code></pre>"},{"location":"modules/plasmids/#plsdb","title":"PLSDB","text":"<p>PLSDB Database is available via this link: https://ccb-microbe.cs.uni-saarland.de/plsdb/plasmids/download/plasmids_meta.tar.bz2. All files except .tsv and .msh were deleted from the compressed package. See database section for possible download strategies. The compressed database must be a tar.bz2 file.  If you need credentials to access your files via S3 then please use the following command:</p> <pre><code>nextflow secrets set S3_PLSDB_ACCESS XXXXXXX\nnextflow secrets set S3_PLSDB_SECRET XXXXXXX\n</code></pre>"},{"location":"modules/plasmids/#output","title":"Output","text":""},{"location":"modules/plasmids/#scapp","title":"SCAPP","text":"<p>SCAPP detects plasmid sequences out of the samples assembly graph. It reports sequences as gzipped fasta files (<code>*_plasmids.fasta.gz</code>). A basic statistic (<code>*_plasmids_stats.tsv</code>) per plasmid and a summary statistic (<code>*_plasmids_summary_stats.tsv</code>) over all plasmids is also generated. Coverm coverage metrics are generated for all plasmids. Gene coverage values are generated as part of the annotation module output.</p>"},{"location":"modules/plasmids/#plasclass","title":"PlasClass","text":"<p>PlasClass is able to identify plasmids by using a statistical model that was build using kmer frequencies. It reports gzipped fata files and their probabilities (<code>*_plasclass.tsv</code>).</p>"},{"location":"modules/plasmids/#mobtyper-and-platon","title":"MobTyper and Platon","text":"<p>MobTyper and Platon are using both replicon typing for plasmid detection. (<code>*_mobtyper_results.tsv</code>, <code>*_platon.tsv</code>)</p>"},{"location":"modules/plasmids/#viralverifyplasmid_1","title":"ViralVerifyPlasmid","text":"<p>ViralVerfiy is applying a Naive Bayes classifier (<code>*_viralverifyplasmid.tsv</code>).</p>"},{"location":"modules/plasmids/#plsdb_1","title":"PLSDB","text":"<p>PLSDB includes a curated set of plasmid sequences that were extracted from databases like refseq. The metadata of found sequences are reported in <code>*.tsv</code> and the metadata of the filtered sequences in <code>*_kmerThreshold_X.tsv</code>.</p>"},{"location":"modules/qualityControl/","title":"Quality Control","text":"<p>The quality control module removes adapters, trims and filters short and long-read data. Since quality control is typically the first step in the processing of sequencing data, the Toolkit offers a way to directly download the sequencing data (See <code>download</code> flag.). This allows the data to be downloaded in parallel on multiple machines, as opposed to the usual Nextflow mechanism of downloading input data only on the VM running Nextflow. In addition, the quality control module enables the filtering of human reads and, with Nonpareil, provides diversity estimation of input sequences. </p>"},{"location":"modules/qualityControl/#short-reads","title":"Short Reads","text":"<p>For short reads, we offer a way to generate only a quality report using fastp. This approach eliminates the need for additional disk space to store quality-controlled reads.  (See <code>reportOnly</code> flag in the configuration file below.)</p>"},{"location":"modules/qualityControl/#input","title":"Input","text":"Command for short read dataConfiguration FileTSV Table short read <pre><code>-entry wShortReadQualityControl -params-file example_params/qc.yml\n</code></pre> <p>Warning</p> <p>The configuration file shown here is for demonstration and testing purposes only.    Parameters that should be used in production can be viewed in the quality control section    of one of the yaml files located in the <code>default</code> folder of the Toolkit's Github repository.</p> <pre><code>tempdir: \"tmp\"\ns3SignIn: false\noutput: \"output\"\nlogDir: log\nrunid: 1\ndatabases: \"/mnt/databases\"\nlogLevel: 1\nscratch: \"/vol/scratch\"\nsteps:\n  qc:\n    input: \"test_data/qc/reads_split.tsv\"\n    fastp:\n       # Example params: \" --cut_front --cut_tail --detect_adapter_for_pe  \"\n       additionalParams:\n         fastp: \"  \"\n         reportOnly: false\n    filterHuman:\n      additionalParams: \"  \"\n      database:\n        download:\n          source: https://openstack.cebitec.uni-bielefeld.de:8080/databases/human_filter.db.20231218v2.gz\n          md5sum: cc92c0f926656565b1156d66a0db5a3c\nresources:\n  highmemLarge:\n    cpus: 28\n    memory: 230\n  highmemMedium:\n    cpus: 14\n    memory: 113\n  large:\n    cpus: 28\n    memory: 58\n  medium:\n    cpus: 14\n    memory: 29\n  small:\n    cpus: 7\n    memory: 14\n  tiny:\n    cpus: 1\n    memory: 1\n</code></pre> <pre><code>SAMPLE  READS1  READS2\ntest1   https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/small/read1_1.fq.gz  https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/small/read2_1.fq.gz\ntest2   https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/small/read1_1.fq.gz  https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/small/read2_1.fq.gz\n</code></pre>"},{"location":"modules/qualityControl/#output","title":"Output","text":""},{"location":"modules/qualityControl/#fastp","title":"Fastp","text":"<code>SAMPLE_fastp.json</code> <p>Contains quality statistics about the raw reads and the quality controlled reads in JSON format. </p> <code>SAMPLE_fastp_summary_after.tsv</code> <p>Contains quality statistics about the quality controlled reads in TSV format. </p> <code>SAMPLE_fastp_summary_before.tsv</code> <p>Contains quality statistics about the raw reads in TSV format. </p> <code>SAMPLE_interleaved.qc.fq.gz</code> <p>Quality controlled reads</p> <code>SAMPLE_report.html</code> <p>HTML report with plots summarizing the quality of the raw and quality controlled reads.</p> <code>test1_unpaired.qc.fq.gz</code> <p>Unpaired reads where the other pair was filtered out due to quality control. </p> <code>test1_unpaired_summary.tsv</code> <p>TSV file that contains quality statistics about single reads where the other pair was filtered out due to quality control. </p>"},{"location":"modules/qualityControl/#kmc","title":"KMC","text":"<p><code>SAMPLE.[13|21|71].kmc.json</code></p> <p>K-mer statistics for k-mers of length 13, 21 and 71.</p> <p><code>SAMPLE.[13|21|71].histo.tsv</code></p> <p>K-mer frequency table with the columns <code>FREQUENCY</code>, <code>COUNT</code> and <code>SAMPLE</code>. <code>FREQUENCY</code> is the number of times a specific k-mer appears. <code>COUNT</code> is the number of different k-mers that occur a number of times described by <code>FREQUENCY</code>.</p>"},{"location":"modules/qualityControl/#nanopore-reads","title":"Nanopore Reads","text":""},{"location":"modules/qualityControl/#input_1","title":"Input","text":"Command for nanopore dataConfiguration FileTSV Table nanopore <pre><code>-entry wOntQualityControl -params-file example_params/qcONT.yml\n</code></pre> <p>Warning</p> <p>The configuration file shown here is for demonstration and testing purposes only.    Parameters that should be used in production can be viewed in the quality control section    of one of the YAML files located in the <code>default</code> folder of the Toolkit's GitHub repository.</p> <pre><code>tempdir: \"tmp\"\ns3SignIn: false\noutput: \"output\"\nlogDir: log\nrunid: 1\ndatabases: \"/mnt/databases\"\nlogLevel: 1\nscratch: \"/vol/scratch\"\nsteps:\n  qcONT:\n    input: \"test_data/qcONT/ont.tsv\"\n    porechop:\n      additionalParams:\n        chunkSize: 450000\n        porechop: \"\"\n        filtlong: \" --min_length 1000 --keep_percent 90  \"\n    filterHumanONT:\n      additionalParams: \"  \"\n      database:\n        download:\n          source: https://openstack.cebitec.uni-bielefeld.de:8080/databases/human_filter.db.20231218v2.gz\n          md5sum: cc92c0f926656565b1156d66a0db5a3c\n    nanoplot:\n      additionalParams: \"\"\nresources:\n  highmemLarge:\n    cpus: 28\n    memory: 230\n  highmemMedium:\n    cpus: 14\n    memory: 113\n  large:\n    cpus: 28\n    memory: 58\n  medium:\n    cpus: 14\n    memory: 29\n  small:\n    cpus: 7\n    memory: 14\n  tiny:\n    cpus: 1\n    memory: 1\n</code></pre> <pre><code>SAMPLE  READS\nnano    https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/SRR16328449_qc.fq.gz\n</code></pre>"},{"location":"modules/qualityControl/#output_1","title":"Output","text":""},{"location":"modules/qualityControl/#porechop","title":"Porechop","text":"<code>SAMPLE_qc.fq.gz</code> <p>Gzipped quality controlled reads of the format <code>SAMPLE_qc.fq.gz</code>.</p>"},{"location":"modules/qualityControl/#nanoplot","title":"Nanoplot","text":"<code>NanoStats.tsv</code> <p>Statistics of the output reads, such as quality scores and read length.</p> <code>plots</code> <p>NanoPlot offers a variety of plots that show the quality, length and quantity of the reads.</p>"},{"location":"modules/qualityControl/#output_2","title":"Output","text":"<p>The following output is produced for short and long reads. </p>"},{"location":"modules/qualityControl/#nonpareil","title":"Nonpareil","text":"<code>SAMPLE.npa</code> <p>You can read more here</p> <code>SAMPLE.npc</code> <p>You can read more here</p> <code>SAMPLE.npl</code> <p>You can read more here</p> <code>SAMPLE_nonpareil_curves.pdf</code> <p>Nonpareil curves visualize the estimated average coverage for the current sequencing effort.</p> <code>SAMPLE_nonpareil_index.tsv</code> <p>Nonpareil statistics including the Nonpareil diversity index. </p> <p>Columns: </p> <ul> <li> <p><code>SAMPLE</code> sample name</p> </li> <li> <p><code>C</code> Average coverage of the entire dataset.</p> </li> <li> <p><code>diversity</code> is the Nonpareil diversity index. </p> </li> <li> <p><code>LR</code> Actual sequencing effort of the dataset. </p> </li> <li> <p><code>LRstar</code> is the sequencing effort for nearly complete coverage.</p> </li> <li> <p><code>modelR</code> Pearson\u2019s R coefficient betweeen the rarefied data and the projected model.</p> </li> <li> <p><code>kappa</code> \"Redundancy\" value of the entire dataset. </p> </li> </ul>"},{"location":"modules/qualityControl/#filtered-out-human-sequences","title":"Filtered out Human Sequences","text":"<code>SAMPLE_filtered.fq.gz</code> <p>Sequences without human DNA.</p> <code>SAMPLE_removed.fq.gz</code> <p>Sequences that were classified as human DNA. </p> <code>SAMPLE_summary_[after|before].tsv</code> <p>Statistics of reads before and after quality control. </p>"},{"location":"modules/readMapping/","title":"Read Mapping","text":"<p>Note: This module only supports illumina data. </p>"},{"location":"modules/readMapping/#input","title":"Input","text":"CommandConfiguration FileMAGs TSV TableSamples TSV Table <pre><code>-entry wReadMapping -params-file example_params/readMapping.yml\n</code></pre> <p>Warning</p> <p>The configuration file shown here is for demonstration and testing purposes only.    Parameters that should be used in production can be viewed in the read mapping section    of one of the yaml files located in the <code>default</code> folder of the Toolkit's Github repository.</p> <pre><code>tempdir: \"tmp\"\ns3SignIn: false\noutput: \"output\"\nlogDir: log\nrunid: 1\nlogLevel: 1\nscratch: \"/vol/scratch\"\npublishDirMode: \"symlink\"\nsteps:\n  readMapping:\n    samples: \n       paired: test_data/readMapping/samples.tsv\n       single: test_data/readMapping/single.tsv\n       ont: test_data/readMapping/ont.tsv\n    mags: test_data/readMapping/mags.tsv\n    bwa2:\n      additionalParams:\n        bwa2_index: \"\"\n        bwa2_mem: \"\"\n    minimap:\n      additionalParams:\n        minimap_index: \"\"\n        minimap: \"\"\n\n    coverm: \" --min-covered-fraction 0 \"\n    covermONT: \" --min-covered-fraction 0 \"\nresources:\n  highmemLarge:\n    cpus: 28\n    memory: 230\n  highmemMedium:\n    cpus: 14\n    memory: 113\n  large:\n    cpus: 28\n    memory: 58\n  medium:\n    cpus: 14\n    memory: 29\n  small:\n    cpus: 7\n    memory: 14\n  tiny:\n    cpus: 1\n    memory: 1\n</code></pre> <pre><code>BINS\nhttps://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/bins/bin.1.fa\nhttps://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/bins/bin.2.fa\nhttps://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/bins/bin.8.fasta\nhttps://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/bins/bin.9.fasta\nhttps://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/bins/bin.32.fa\n</code></pre> <pre><code>SAMPLE  READS\ntest1   https://openstack.cebitec.uni-bielefeld.de:8080/swift/v1/meta_test/small/interleaved.fq.gz\n</code></pre>"},{"location":"modules/readMapping/#output","title":"Output","text":"<p>The produced output files are the following: count.tsv, mean.tsv, mean_mincov10.tsv, rpkm.tsv, tpm.tsv, trimmed_mean.tsv. The content of the files are produced by coverm. All metrics are explained on the coverm GitHub page: https://github.com/wwood/CoverM .</p>"},{"location":"tutorials/tutorial1/annotation/","title":"Annotation","text":"<p>Annotation is the process of identifying features of interest in a set of genomic DNA sequences and labeling them with information such as their function. The Metagenomics-Toolkit provides several tools for annotating genes, based on the gene prediction using Prodigal. Prodigal is also part of Prokka, which provides a fast functional annotation of our data in addition to gene prediction. </p> <p>Annotation with larger databases is beyond the scope of this workshop due to runtime limitations, but we will explore it within EMGB.</p>"},{"location":"tutorials/tutorial1/annotation/#prokka","title":"Prokka","text":"<p>Prokka is an efficient, user-friendly and open source bioinformatics tool designed for the annotation of bacterial genomes. It automates the prediction of genes, tRNAs, rRNAs, and other genomic features, utilizing various databases and algorithms to ensure accurate annotations. Prokka supports standard output formats such as GenBank and GFF, facilitating further analysis with compatible tools. </p> <p>See Prokka homepage for more information.</p> <p>The following snippet represents the Toolkit contiguration for the annotion module, just running Prokka: Annotation Configuration File Snippet 1<pre><code>  annotation:\n    prokka:\n      defaultKingdom: false\n      additionalParams: \" --mincontiglen 500 \"\n</code></pre></p> <p>Task 1</p> <p>Run the following command to for the annotation: <pre><code>cd ~/mgcourse/\n\nNXF_VER=25.04.2 nextflow run metagenomics/metagenomics-tk \\\n      -profile standard \\\n      -params-file https://raw.githubusercontent.com/metagenomics/metagenomics-tk/refs/heads/master/default/tutorials/tutorial1/fullpipeline_annotation.yml \\\n      -ansi-log false \\\n      -entry wFullPipeline \\\n      -resume \\\n      --input.paired.path https://raw.githubusercontent.com/metagenomics/metagenomics-tk/refs/heads/master/test_data/tutorials/tutorial1/reads.tsv \\\n          --databases $(readlink -f databases) \\\n      --logDir logs_annotation \n</code></pre></p> <p>For reference, your complete parameter file should look like this:</p> Parameter-file <pre><code>tempdir: \"tmp\"\nsummary: false\ns3SignIn: false \ninput:\n  paired:\n    path: \"test_data/tutorials/tutorial1/reads.tsv\"\n    watch: false\noutput: output\nlogDir: log\nrunid: 1\ndatabases: \"/vol/scratch/databases\"\npublishDirMode: \"symlink\"\nlogLevel: 1\nscratch: false\nsteps:\n  qc:\n    fastp:\n       # For PE data, the adapter sequence auto-detection is disabled by default since the adapters can be trimmed by overlap analysis. However, you can specify --detect_adapter_for_pe to enable it.\n       # For PE data, fastp will run a little slower if you specify the sequence adapters or enable adapter auto-detection, but usually result in a slightly cleaner output, since the overlap analysis may fail due to sequencing errors or adapter dimers.\n       # -q, --qualified_quality_phred       the quality value that a base is qualified. Default 15 means phred quality &gt;=Q15 is qualified.\n       # --cut_front move a sliding window from front (5') to tail, drop the bases in the window if its mean quality is below cut_mean_quality, stop otherwise.\n       # --length_required  reads shorter than length_required will be discarded, default is 15. (int [=15])\n       # PE data, the front/tail trimming settings are given with -f, --trim_front1 and -t, --trim_tail1\n       additionalParams:\n         fastp: \" --detect_adapter_for_pe -q 20 --cut_front --trim_front1 3 --cut_tail --trim_tail1 3 --cut_mean_quality 10 --length_required 50 \"\n         reportOnly: false\n       timeLimit: \"AUTO\"\n    nonpareil:\n      additionalParams: \" -v 10 -r 1234 \"\n    filterHuman:\n      additionalParams: \"  \"\n      database:\n        download:\n          source: https://openstack.cebitec.uni-bielefeld.de:8080/databases/human_filter.db.20231218v2.gz\n          md5sum: cc92c0f926656565b1156d66a0db5a3c\n  assembly:\n    megahit:\n      # --mem-flag 0 to use minimum memory, --mem-flag 1 (default) moderate memory and --mem-flag 2 all memory.\n      # meta-sensitive: '--min-count 1 --k-list 21,29,39,49,...,129,141' \n      # meta-large:  '--k-min  27  --k-max 127 --k-step 10' (large &amp; complex metagenomes, like soil)\n      additionalParams: \" --min-contig-len 500 --presets meta-sensitive \"\n      fastg: true\n      resources:\n        RAM:\n          mode: 'PREDICT'\n          predictMinLabel: 'highmemLarge'\n  binning:\n    bwa2:\n      additionalParams: \n        bwa2: \" \"\n        # samtools flags are used to filter resulting bam file\n        samtoolsView: \" -F 3584 \" \n    contigsCoverage:\n      additionalParams: \" --min-covered-fraction 0 --min-read-percent-identity 100 --min-read-aligned-percent 100 \"\n    genomeCoverage:\n      additionalParams: \" --min-covered-fraction 0 --min-read-percent-identity 100 --min-read-aligned-percent 100 \"\n    # Primary binning tool\n    metabat:\n      # Set --seed positive numbers to reproduce the result exactly. Otherwise, random seed will be set each time.\n      additionalParams: \" --seed 234234  \"\n  magAttributes:\n    checkm2:\n      database:\n        download:\n          source: \"https://openstack.cebitec.uni-bielefeld.de:8080/databases/checkm2_v2.tar.gz\"\n          md5sum: a634cb3d31a1f56f2912b74005f25f09\n      additionalParams: \"  \"\n    gtdb:\n      buffer: 1000\n      database:\n        download:\n          source: https://openstack.cebitec.uni-bielefeld.de:8080/databases/gtdbtk_r226_v2_data.tar.gz\n          md5sum: 0777ea44f9d7d96f9ca6ea43ed016799\n      additionalParams: \" --min_af 0.65 --scratch_dir . \"\n  annotation:\n    prokka:\n      defaultKingdom: false\n      additionalParams: \" --mincontiglen 500 \"\nresources:\n  highmemLarge:\n    cpus: 28\n    memory: 60\n  highmemMedium:\n    cpus: 14\n    memory: 30\n  large:\n    cpus: 28\n    memory: 58\n  medium:\n    cpus: 14\n    memory: 29\n  small:\n    cpus: 7\n    memory: 14\n  tiny:\n    cpus: 1\n    memory: 1\n</code></pre> <p>Task 2</p> <p>Locate the annotation files in the <code>output</code> directory and look for <code>*.txt</code> files that summarize the number of genes per category. Can you report a bin that contains CRISPR sequences?</p> Solution <p><pre><code>cd ~/mgcourse\n</code></pre> In the following case a bin with the number 3 contains CRISPR sequences. In your case maybe another bin contains CRISPR sequences. <pre><code>cat output/data/1/annotation/1.0.0/prokka/data_bin.3.txt\n</code></pre> The above command leads to the following output for the pre computed results:  <pre><code>organism: Genus species strain \ncontigs: 150\nbases: 520975\nCDS: 853\nCRISPR: 3\ntRNA: 12\n</code></pre></p> <p>If you want to look into the descriptions of the annotated genes then you have to look into the GFF (General Feature Format) files. </p> <p>A GFF file is a tab-delimited text format used in bioinformatics to describe genomic features such as genes, exons, introns, and more. It consists of multiple columns that provide specific details about each feature:</p> <ul> <li> <p>Sequence Name: The identifier for the sequence or chromosome.</p> </li> <li> <p>Source: The source or database providing the annotation.</p> </li> <li> <p>Feature Type: The type of feature (e.g., gene, exon, CDS).</p> </li> <li> <p>Start and End Positions: The coordinates where the feature begins and ends.</p> </li> <li> <p>Score: A confidence score for the feature prediction.</p> </li> <li> <p>Strand: Indicates the directionality (forward or reverse) of the feature.</p> </li> <li> <p>Phase: For features like CDS, it indicates reading frame offsets.</p> </li> <li> <p>Attributes: Additional information such as gene IDs or product names.</p> </li> </ul> <p>Optional header lines starting with '##' provide metadata about the file, adding context to the annotations. </p> <p>Task 3</p> <p>Find a gff file of a bin that contains tRNAs the same way as you did in the previous taks. Search with <code>zgrep -P \"\\ttRNA\\t\"</code> for tRNAs and the corresponding annotations file (gff). Can you tell the products of two tRNAs. </p> Solution <p>In the our precomputed case bin3 contains two rRNAs. <pre><code>zgrep tRNA output/data/1/annotation/1.0.0/prokka/data_bin.3.gff.gz\n</code></pre> For example, there the tRNA products <code>tRNA-Thr(cgt)</code> and <code>tRNA-His(gtg)</code>.  </p>"},{"location":"tutorials/tutorial1/annotation/#emgb","title":"EMGB","text":"<p>Using the Toolkit, you have now predicted and annotated hundreds of genes. If you were to use all the tools in the annotation module, you could annotate even more genes using databases such as KEGG. While you can of course use the command line to search for genes, MAGs, and pathways in this mass of data, you could also explore your dataset with EMGB.</p> <p>Task 4</p> <p>Lets imagine you search for MAGs that contain genes that encode for the enzyme <code>5.3.1.9</code>. Hint: The enzyme is involved the glycolysis pathway. </p> <p>Click on the enzyme and a filter will be automatically created. Now if you click on the \"MAGs\" tab, you should see MAGs that contain that gene. How many MAGs contain that gene? Which MAG has the highest completeness?</p> <p>In the next step, you might also interested in neighboring genes of the MAG with the highest completeness. What is the name of the gene to the left of it?</p> Solution <p>In the precomputed output the results look the following way. In your case it might another bin id might have the highest completeness. Five MAGs contain the gene that encodes the enzyme <code>5.3.1.9</code>. The MAG with the highest completeness is <code>Bin 6</code>. To the left of the gene encoding the enzyme is the gene for <code>Proline--tRNA ligase</code>.</p>"},{"location":"tutorials/tutorial1/assembly/","title":"Assembly","text":"<p>Once your raw reads have been quality-controlled, we will now perform an assembly using the Metagenomics-Toolkit. The Toolkit supports several assemblers, one of which is the MEGAHIT assembler, which we will run in this section.</p>"},{"location":"tutorials/tutorial1/assembly/#metagenomics-toolkit","title":"Metagenomics-Toolkit","text":"<p>The following lines represent the part of the configuration that tells the Toolkit to run the assembly:</p> Assembly Configuration File Snippet 1<pre><code>  assembly:\n    megahit:\n      # --mem-flag 0 to use minimum memory, --mem-flag 1 (default) moderate memory and --mem-flag 2 all memory.\n      # meta-sensitive: '--min-count 1 --k-list 21,29,39,49,...,129,141' \n      # meta-large:  '--k-min  27  --k-max 127 --k-step 10' (large &amp; complex metagenomes, like soil)\n      additionalParams: \" --min-contig-len 500 --presets meta-sensitive \"\n      fastg: true\n      resources:\n        RAM:\n          mode: 'PREDICT'\n          predictMinLabel: 'highmemLarge'\n</code></pre> <p>Task 1</p> <p>Which tool is used for the assembly? What additional parameters are used?</p> Solution <p>MEGAHIT with the additional parameters: minimum contig length of <code>500 bp</code> and the preset <code>meta-sensitive</code>. <code>meta-sensitive</code> sets the following parameters: <code>--min-count 1 --k-list 21,29,39,49,...,129,141</code>, which causes  MEGAHIT to use a longer list of k-mers. MEGAHIT is a single-node assembler for large and complex metagenomics NGS reads, such as soil. It makes use of a succinct de Bruijn graph (SdBG) to achieve a low memory assembly. See the MEGAHIT home page for more info.</p> <p>Task 2</p> <p>Copy the following command to run the quality control and assembly module of the Toolkit on your machine</p> <pre><code>cd ~/mgcourse/\n\nNXF_VER=25.04.2 nextflow run metagenomics/metagenomics-tk \\\n      -profile standard \\\n      -params-file https://raw.githubusercontent.com/metagenomics/metagenomics-tk/refs/heads/master/default/tutorials/tutorial1/fullpipeline_assembly.yml \\\n      -ansi-log false \\\n      -entry wFullPipeline \\\n      -resume \\\n      --input.paired.path https://raw.githubusercontent.com/metagenomics/metagenomics-tk/refs/heads/master/test_data/tutorials/tutorial1/reads.tsv \\\n      --logDir logs_assembly\n</code></pre> <p>Note that you have used the <code>resume</code> flag this time, which causes Nextflow to reuse the results of the QC analysis.  That's why you'll see several processes labeled Cache process on the Toolkit execution screen.</p> <p>For reference, your complete parameter file looks like this:</p> Parameter-file <pre><code>tempdir: \"tmp\"\nsummary: false\ns3SignIn: false \ninput:\n  paired:\n    path: \"test_data/tutorials/tutorial1/reads.tsv\"\n    watch: false\noutput: output\nlogDir: log\nrunid: 1\ndatabases: \"/vol/scratch/databases\"\npublishDirMode: \"symlink\"\nlogLevel: 1\nscratch: false \nsteps:\n  qc:\n    fastp:\n       # For PE data, the adapter sequence auto-detection is disabled by default since the adapters can be trimmed by overlap analysis. However, you can specify --detect_adapter_for_pe to enable it.\n       # For PE data, fastp will run a little slower if you specify the sequence adapters or enable adapter auto-detection, but usually result in a slightly cleaner output, since the overlap analysis may fail due to sequencing errors or adapter dimers.\n       # -q, --qualified_quality_phred       the quality value that a base is qualified. Default 15 means phred quality &gt;=Q15 is qualified.\n       # --cut_front move a sliding window from front (5') to tail, drop the bases in the window if its mean quality is below cut_mean_quality, stop otherwise.\n       # --length_required  reads shorter than length_required will be discarded, default is 15. (int [=15])\n       # PE data, the front/tail trimming settings are given with -f, --trim_front1 and -t, --trim_tail1\n       additionalParams:\n         fastp: \" --detect_adapter_for_pe -q 20 --cut_front --trim_front1 3 --cut_tail --trim_tail1 3 --cut_mean_quality 10 --length_required 50 \"\n         reportOnly: false\n       timeLimit: \"AUTO\"\n    nonpareil:\n      additionalParams: \" -v 10 -r 1234 \"\n    filterHuman:\n      additionalParams: \"  \"\n      database:\n        download:\n          source: https://openstack.cebitec.uni-bielefeld.de:8080/databases/human_filter.db.20231218v2.gz\n          md5sum: cc92c0f926656565b1156d66a0db5a3c\n  assembly:\n    megahit:\n      # --mem-flag 0 to use minimum memory, --mem-flag 1 (default) moderate memory and --mem-flag 2 all memory.\n      # meta-sensitive: '--min-count 1 --k-list 21,29,39,49,...,129,141' \n      # meta-large:  '--k-min  27  --k-max 127 --k-step 10' (large &amp; complex metagenomes, like soil)\n      additionalParams: \" --min-contig-len 500 --presets meta-sensitive \"\n      fastg: true\n      resources:\n        RAM:\n          mode: 'PREDICT'\n          predictMinLabel: 'highmemLarge'\nresources:\n  highmemLarge:\n    cpus: 28\n    memory: 60\n  highmemMedium:\n    cpus: 14\n    memory: 30\n  large:\n    cpus: 28\n    memory: 58\n  medium:\n    cpus: 14\n    memory: 29\n  small:\n    cpus: 7\n    memory: 14\n  tiny:\n    cpus: 1\n    memory: 1\n</code></pre>"},{"location":"tutorials/tutorial1/assembly/#assembly-results","title":"Assembly Results","text":"<p>We will now have a first look at some assembly statistics. First of all, locate your assembly results somewhere in your <code>output</code> directory.</p> <p>Task 3</p> <p>Where are the assembly results of MEGAHIT stored? And what files are generated?</p> Solution <p>The assembly results are stored in <code>output/data/1/assembly/1.2.1/megahit/</code> <pre><code>cd ~/mgcourse/\n</code></pre> <pre><code>ls -l output/data/1/assembly/1.2.1/megahit/\n</code></pre> <pre><code>    data_contigs.fa.gz  # the assembled sequences as gzipped fasta\n    data_contigs.fastg  # the assembly graph for inspection for example with Bandage\n    data_contigs_stats.tsv  # some assembly statistics\n</code></pre></p> <p>Task 4</p> <p>Have a look at the <code>data_contigs_stats.tsv</code> file - how large is the assembly and what is the N50?</p> Solution <p><pre><code>cd ~/mgcourse/\n</code></pre> <pre><code>cat output/data/1/assembly/1.2.1/megahit/data_contigs_stats.tsv | column -s$'\\t' -t\n</code></pre> <pre><code>SAMPLE  file                format  type  num_seqs  sum_len   min_len  avg_len  max_len  Q1     Q2      Q3      sum_gap  N50   Q20(%)  Q30(%)  GC(%)\ndata    data_contigs.fa.gz  FASTA   DNA   16675     25442506  500      1525.8   35033    711.0  1087.0  1769.0  0        1897  0.00    0.00    42.84\n</code></pre> In this assembly run, the assembly length is 25,442,506 bp and the N50 is 1,897 bp. </p> <p>Task 5</p> <p>Open the results in the EMGB-Browser, find out, how many genes are annotated on the largest contig!</p> Solution <p>There are 41 genes annotated on the largest contig. Open the dataset <code>data</code> go to \"Contigs\", filter for contigs longer than 30kb and inspect the larger one.</p> <p>\u27a1\ufe0f Continue to: Assembly Evaluation </p>"},{"location":"tutorials/tutorial1/assembly_evaluation/","title":"Assembly Evaluation","text":"<p>We will now compare the results of our assembly with those of other assemblers and inspect mappings of reads to the corresponding assemblies.</p>"},{"location":"tutorials/tutorial1/assembly_evaluation/#metaquast","title":"MetaQUAST","text":"<p>QUAST stands for QUality ASsessment Tool. The tool evaluates genome assemblies by computing various metrics.  You can find all project news and the latest version of the tool at SourceForge.  QUAST is a comprehensive quality assessment tool for genome assemblies that employs several bioinformatics utilities to evaluate assembly accuracy and quality. It utilizes MUMmer (for genome alignment), GeneMarkS (self-training gene prediction in prokaryotes), GeneMark-ES (gene prediction in eukaryotes), GlimmerHMM (gene finding using Hidden Markov Models), and GAGE (a pipeline for comparing gene predictions across multiple annotations).</p> <p>Additionally, MetaQUAST extends QUAST's capabilities to metagenomic assemblies by incorporating tools such as MetaGeneMark (specialized for gene prediction in metagenomes), Krona Tools (for taxonomic classification visualization), BLAST (for sequence similarity searches against reference databases), and the SILVA 16S rRNA database (for identifying microbial communities).</p> <p>Task 1</p> <p>Copy the pre-computed assembly results to your local directory. <pre><code>cd ~/mgcourse\n</code></pre> <pre><code>wget https://s3.bi.denbi.de/cmg/mgcourses/mg2025/assembly_results.tar.gz\ntar -xzvf assembly_results.tar.gz\n</code></pre></p> <p>Task 2</p> <p>Then copy your assembly results from the toolkit run to the assembly_results directory: <pre><code>cd ~/mgcourse/\n</code></pre> <pre><code>cp output/data/1/assembly/1.2.1/megahit/data_contigs.fa.gz assembly_results/megahit_out/final.contigs.fa.gz\ngunzip -fd assembly_results/megahit_out/final.contigs.fa.gz\n</code></pre></p> <p>Task 3</p> <p>In addition, we need to download some references in order to compare them to our assemblies: <pre><code>cd ~/mgcourse/\n</code></pre> <pre><code>wget https://s3.bi.denbi.de/cmg/mgcourses/mg2025/genomes.tar.gz\ntar -xzvf genomes.tar.gz\n</code></pre></p> <p>To call the <code>metaquast.py</code> script, we have to provide reference genomes which are used to calculate a variety of different metrics for evaluating the assembly. In real-world metagenomics, these references are not available, of course.</p> <p>Task 4</p> <p>Run metaquast: <pre><code>cd ~/mgcourse/\n</code></pre> <pre><code>metaquast.py --threads 28 --gene-finding \\\n-R genomes/Aquifex_aeolicus_VF5.fna,\\\ngenomes/Bdellovibrio_bacteriovorus_HD100.fna,\\\ngenomes/Chlamydia_psittaci_MN.fna,\\\ngenomes/Chlamydophila_pneumoniae_CWL029.fna,\\\ngenomes/Chlamydophila_pneumoniae_J138.fna,\\\ngenomes/Chlamydophila_pneumoniae_LPCoLN.fna,\\\ngenomes/Chlamydophila_pneumoniae_TW_183.fna,\\\ngenomes/Chlamydophila_psittaci_C19_98.fna,\\\ngenomes/Finegoldia_magna_ATCC_29328.fna,\\\ngenomes/Fusobacterium_nucleatum_ATCC_25586.fna,\\\ngenomes/Helicobacter_pylori_26695.fna,\\\ngenomes/Lawsonia_intracellularis_PHE_MN1_00.fna,\\\ngenomes/Mycobacterium_leprae_TN.fna,\\\ngenomes/Porphyromonas_gingivalis_W83.fna,\\\ngenomes/Wigglesworthia_glossinidia.fna \\\n-o quast \\\n-l MegaHit,metaSPAdes,Ray_31,Ray_51,velvet_31,velvet_51,idba_ud \\\nassembly_results/megahit_out/final.contigs.fa \\\nassembly_results/metaspades_out/contigs.fasta \\\nassembly_results/ray_31/Contigs.fasta \\\nassembly_results/ray_51/Contigs.fasta \\\nassembly_results/velvet_31/contigs.fa \\\nassembly_results/velvet_51/contigs.fa \\\nassembly_results/idba_ud_out/contig.fa\n</code></pre></p> <p>We will now inspect the QUAST reports.</p> <p>Task 5</p> <p>QUAST generates HTML reports including a number of interactive graphics. To access these reports, locate the html report in the <code>quast</code> directory and load the reports in your web browser via the following command: <pre><code>firefox quast/report.html\n</code></pre></p> <p>Task 6</p> <p>Which of the assemblers performs best in terms of coverage of genome fraction of the reference genomes? Which assembly result would you prefer among them?</p> Solution <p>MEGAHIT, metaSPAdes and idba_ud perform best in terms of covered genome fraction. MEGAHIT tends to generate larger contigs along with misassemblies, while metaSPAdes creates shorter contigs with fewer misassemblies, idba_ud is quite similar to metaSPades.</p> <p>\u27a1\ufe0f Continue to: Binning</p>"},{"location":"tutorials/tutorial1/bin_quality/","title":"Assessing Bin Quality","text":"<p>To get an impression of the quality of our bins, we compute the completeness and contamination values for our bins. </p>"},{"location":"tutorials/tutorial1/bin_quality/#computing-completeness-and-contamination-using-checkm2","title":"Computing Completeness and Contamination using CheckM2","text":"<p>CheckM 1 and 2 provide a set of tools to assess the quality of genomes recovered from isolates, single cells, or metagenomes. While the first version relies on collocated sets of genes that are ubiquitous and single-copy within a phylogenetic lineage, the second version uses a machine-learning-based approach that was trained on all available genomic information such as multi-copy genes, biological pathways and modules. Both versions are supported by the Metagenomics-Toolkit and we will use the second version for this part of the tutorial.</p> <p>The following lines represent the part of the configuration that tells the Toolkit to run the MagAttributes module that also includes the CheckM2 tool:</p> MagAttributes Configuration File Snippet 1<pre><code>  magAttributes:\n    checkm2:\n      database:\n        download:\n          source: \"https://openstack.cebitec.uni-bielefeld.de:8080/databases/checkm2_v2.tar.gz\"\n          md5sum: a634cb3d31a1f56f2912b74005f25f09\n      additionalParams: \"  \"\n</code></pre> <p>For reference, your complete parameter file looks like this:</p> Parameter-file <pre><code>tempdir: \"tmp\"\nsummary: false\ns3SignIn: false\ninput:\n  paired:\n    path: \"test_data/tutorials/tutorial1/reads.tsv\"\n    watch: false\noutput: output\nlogDir: log\nrunid: 1\ndatabases: \"/vol/scratch/databases\"\npublishDirMode: \"symlink\"\nlogLevel: 1\nscratch: false\nsteps:\n  qc:\n    fastp:\n       # For PE data, the adapter sequence auto-detection is disabled by default since the adapters can be trimmed by overlap analysis. However, you can specify --detect_adapter_for_pe to enable it.\n       # For PE data, fastp will run a little slower if you specify the sequence adapters or enable adapter auto-detection, but usually result in a slightly cleaner output, since the overlap analysis may fail due to sequencing errors or adapter dimers.\n       # -q, --qualified_quality_phred       the quality value that a base is qualified. Default 15 means phred quality &gt;=Q15 is qualified.\n       # --cut_front move a sliding window from front (5') to tail, drop the bases in the window if its mean quality is below cut_mean_quality, stop otherwise.\n       # --length_required  reads shorter than length_required will be discarded, default is 15. (int [=15])\n       # PE data, the front/tail trimming settings are given with -f, --trim_front1 and -t, --trim_tail1\n       additionalParams:\n         fastp: \" --detect_adapter_for_pe -q 20 --cut_front --trim_front1 3 --cut_tail --trim_tail1 3 --cut_mean_quality 10 --length_required 50 \"\n         reportOnly: false\n       timeLimit: \"AUTO\"\n    nonpareil:\n      additionalParams: \" -v 10 -r 1234 \"\n    filterHuman:\n      additionalParams: \"  \"\n      database:\n        download:\n          source: https://openstack.cebitec.uni-bielefeld.de:8080/databases/human_filter.db.20231218v2.gz\n          md5sum: cc92c0f926656565b1156d66a0db5a3c\n  assembly:\n    megahit:\n      # --mem-flag 0 to use minimum memory, --mem-flag 1 (default) moderate memory and --mem-flag 2 all memory.\n      # meta-sensitive: '--min-count 1 --k-list 21,29,39,49,...,129,141' \n      # meta-large:  '--k-min  27  --k-max 127 --k-step 10' (large &amp; complex metagenomes, like soil)\n      additionalParams: \" --min-contig-len 500 --presets meta-sensitive \"\n      fastg: true\n      resources:\n        RAM:\n          mode: 'PREDICT'\n          predictMinLabel: 'highmemLarge'\n  binning:\n    bwa2:\n      additionalParams: \n        bwa2: \" \"\n        # samtools flags are used to filter resulting bam file\n        samtoolsView: \" -F 3584 \" \n    contigsCoverage:\n      additionalParams: \" --min-covered-fraction 0 --min-read-percent-identity 100 --min-read-aligned-percent 100 \"\n    genomeCoverage:\n      additionalParams: \" --min-covered-fraction 0 --min-read-percent-identity 100 --min-read-aligned-percent 100 \"\n    # Primary binning tool\n    metabat:\n      # Set --seed positive numbers to reproduce the result exactly. Otherwise, random seed will be set each time.\n      additionalParams: \" --seed 234234  \"\n  magAttributes:\n    checkm2:\n      database:\n        download:\n          source: \"https://openstack.cebitec.uni-bielefeld.de:8080/databases/checkm2_v2.tar.gz\"\n          md5sum: a634cb3d31a1f56f2912b74005f25f09\n      additionalParams: \"  \"\nresources:\n  highmemLarge:\n    cpus: 28\n    memory: 60\n  highmemMedium:\n    cpus: 14\n    memory: 30\n  large:\n    cpus: 28\n    memory: 58\n  medium:\n    cpus: 14\n    memory: 29\n  small:\n    cpus: 7\n    memory: 14\n  tiny:\n    cpus: 1\n    memory: 1\n</code></pre> <p>Task 2</p> <p>Run CheckM2 with the Metagenomics-Toolkit using the following command: <pre><code>cd ~/mgcourse/\n\nNXF_VER=25.04.2 nextflow run metagenomics/metagenomics-tk \\\n      -profile standard \\\n      -params-file https://raw.githubusercontent.com/metagenomics/metagenomics-tk/refs/heads/master/default/tutorials/tutorial1/fullpipeline_bin_quality.yml \\\n      -ansi-log false \\\n      -entry wFullPipeline \\\n      -resume \\\n      --databases $(readlink -f databases) \\\n      --input.paired.path https://raw.githubusercontent.com/metagenomics/metagenomics-tk/refs/heads/master/test_data/tutorials/tutorial1/reads.tsv \\\n      --logDir logs_bin_quality \n</code></pre></p> <p>Task 3</p> <p>Locate the CheckM results inside the <code>output</code> directory and find out the completeness and contamination values for all of our bins.</p> Solution <p><pre><code>cd ~/mgcourse\n</code></pre> <pre><code>cut -f2,3,4 output/data/1/magAttributes/*/checkm2/data_checkm2_generated.tsv\n</code></pre> <pre><code>BIN_ID  COMPLETENESS    CONTAMINATION\ndata_bin.1.fa   16.88   0.0\ndata_bin.10.fa  24.04   0.04\ndata_bin.2.fa   12.81   0.12\ndata_bin.3.fa   20.57   0.01\ndata_bin.4.fa   52.73   0.3\ndata_bin.5.fa   11.09   0.0\ndata_bin.6.fa   81.91   0.46\ndata_bin.7.fa   20.68   0.75\ndata_bin.8.fa   85.4    0.28\ndata_bin.9.fa   39.66   0.62\n</code></pre></p> <p>Task 4</p> <p>Compare the CheckM2 completeness results with the genome fraction results from QUAST. Do they match your expectations?</p> Solution <p>No, completeness is much lower than expected in comparison to the genome fraction we observed in the QUAST results (see Assembly evaluation part).</p> <p>We will now run QUAST again, but this time we specify the bins and the unbinned contigs instead of the assemblies as input. By doing this, QUAST can tell us which reference genome the bin belongs to.</p> <p>Task 5</p> <p>Run metaquast with the individual bins as input:</p> <pre><code>cd ~/mgcourse/\n</code></pre> <pre><code>metaquast.py --threads 28 --gene-finding \\\n-R genomes/Aquifex_aeolicus_VF5.fna,\\\ngenomes/Bdellovibrio_bacteriovorus_HD100.fna,\\\ngenomes/Chlamydia_psittaci_MN.fna,\\\ngenomes/Chlamydophila_pneumoniae_CWL029.fna,\\\ngenomes/Chlamydophila_pneumoniae_J138.fna,\\\ngenomes/Chlamydophila_pneumoniae_LPCoLN.fna,\\\ngenomes/Chlamydophila_pneumoniae_TW_183.fna,\\\ngenomes/Chlamydophila_psittaci_C19_98.fna,\\\ngenomes/Finegoldia_magna_ATCC_29328.fna,\\\ngenomes/Fusobacterium_nucleatum_ATCC_25586.fna,\\\ngenomes/Helicobacter_pylori_26695.fna,\\\ngenomes/Lawsonia_intracellularis_PHE_MN1_00.fna,\\\ngenomes/Mycobacterium_leprae_TN.fna,\\\ngenomes/Porphyromonas_gingivalis_W83.fna,\\\ngenomes/Wigglesworthia_glossinidia.fna \\\n-o quast_bins \\\n-l BIN1,BIN2,BIN3,BIN4,BIN5,BIN6,BIN7,BIN8,BIN9,BIN10,UNBINNED \\\noutput/data/1/binning/*/metabat/data_bin.1.fa \\\noutput/data/1/binning/*/metabat/data_bin.2.fa \\\noutput/data/1/binning/*/metabat/data_bin.3.fa \\\noutput/data/1/binning/*/metabat/data_bin.4.fa \\\noutput/data/1/binning/*/metabat/data_bin.5.fa \\\noutput/data/1/binning/*/metabat/data_bin.6.fa \\\noutput/data/1/binning/*/metabat/data_bin.7.fa \\\noutput/data/1/binning/*/metabat/data_bin.8.fa \\\noutput/data/1/binning/*/metabat/data_bin.9.fa \\\noutput/data/1/binning/*/metabat/data_bin.10.fa \\\noutput/data/1/binning/*/metabat/data_notBinned.fa\n</code></pre> <p>Task 6</p> <p>Now inspect the QUAST report with: <pre><code>firefox quast_bins/report.html\n</code></pre> Can you identify, where large part of the genome fractions are assigned to? What might be the reason? Have a close look on the contig statistics of each genome bin and the unbinned fraction</p> Solution <p>Large parts of the contigs have been assigned to the unbinned fraction. The reason for that is the small contig size. Metabat2 uses 2500 bp as default cutoff for contigs that can be assigned to genome bins, other contigs are automatically assigned to the unbinned fraction. This cutoff can be lowered to 1500 bp but would not improve the results very much here, since so many contigs are smaller than that.</p> <p>In summary, three of your bins meet at least the completeness and contamination criteria to be considered as medium quality (Completeness &gt; 50% and Contamination &lt; 10%)  according to the Minimum Information about a Metagenome-Assembled Genome (MIMAG) standard.  In the next section we will examine their taxonomy.</p> <p>\u27a1\ufe0f Continue to: Classification </p>"},{"location":"tutorials/tutorial1/binning/","title":"Binning","text":"<p>We will now perform binning of our assembly.</p> <p>Binning is a critical process in metagenomics that involves grouping DNA fragments, known as contigs, into bins that likely originate from the same organism or genome. This technique is essential for reconstructing genomes from mixed microbial communities, where DNA from various organisms is intermingled, and there is often no prior knowledge of the species present.</p> <p>The binning process primarily utilizes two types of information: composition and coverage of the assembled contigs.</p> <ul> <li> <p>Composition: This refers to the analysis of k-mer frequency profiles within the contigs. K-mers are short DNA sequences of a fixed length (commonly tetranucleotides, or sequences of four nucleotides). These k-mer frequencies are generally conserved within a single genome, meaning that different regions of the same genome will share similar sequence composition.</p> </li> <li> <p>Coverage: This reflects the abundance of each contig in the assembly. Organisms that are more abundant in the environment contribute more DNA to the metagenomic sample, resulting in their sequences being represented more frequently in the data.</p> </li> </ul> <p>During binning, binning tools look for DNA fragments that, despite not being assembled together, exhibit similar composition and occur at approximately equal abundances. These similarities suggest that such contigs likely originate from the same genome, enabling accurate grouping into bins.</p> <p>By integrating both composition and coverage information,  binning provides a powerful approach to distinguish between different organisms within a mixed sample. This method is particularly valuable as it allows researchers to reconstruct genomes without prior knowledge of the microbial community, making it an indispensable tool in metagenomic studies.</p>"},{"location":"tutorials/tutorial1/binning/#metabat","title":"MetaBAT","text":"<p>MetaBAT is an automated metagenome binning software which integrates empirical probabilistic distances of genome abundance and tetranucleotide frequency. See the MetaBAT home page for more information.</p> <p>We will now perform a binning of our assembly with MetaBAT using the Metagenomics-Toolkit. The following snippet represents the Toolkit configuration for the binning module:</p> Binning Configuration File Snippet 1<pre><code>  binning:\n    bwa2:\n      additionalParams: \n        bwa2: \" \"\n        # samtools flags are used to filter resulting bam file\n        samtoolsView: \" -F 3584 \" \n    contigsCoverage:\n      additionalParams: \" --min-covered-fraction 0 --min-read-percent-identity 100 --min-read-aligned-percent 100 \"\n    genomeCoverage:\n      additionalParams: \" --min-covered-fraction 0 --min-read-percent-identity 100 --min-read-aligned-percent 100 \"\n    # Primary binning tool\n    metabat:\n      # Set --seed positive numbers to reproduce the result exactly. Otherwise, random seed will be set each time.\n      additionalParams: \" --seed 234234  \"\n</code></pre> <p>Task 1</p> <p>We can run it directly with: <pre><code>cd ~/mgcourse/\n\nNXF_VER=25.04.2 nextflow run metagenomics/metagenomics-tk \\\n      -profile standard \\\n      -params-file https://raw.githubusercontent.com/metagenomics/metagenomics-tk/refs/heads/master/default/tutorials/tutorial1/fullpipeline_binning.yml \\\n      -ansi-log false \\\n      -entry wFullPipeline \\\n      -resume \\\n      --input.paired.path https://raw.githubusercontent.com/metagenomics/metagenomics-tk/refs/heads/master/test_data/tutorials/tutorial1/reads.tsv \\\n      --logDir logs_binning \n</code></pre></p> <p>For reference, your complete parameter file looks like this:</p> Parameter-file <pre><code>tempdir: \"tmp\"\nsummary: false\ns3SignIn: false\ninput:\n  paired:\n    path: \"test_data/tutorials/tutorial1/reads.tsv\"\n    watch: false\noutput: output\nlogDir: log\nrunid: 1\ndatabases: \"/vol/scratch/databases\"\npublishDirMode: \"symlink\"\nlogLevel: 1\nscratch: false\nsteps:\n  qc:\n    fastp:\n       # For PE data, the adapter sequence auto-detection is disabled by default since the adapters can be trimmed by overlap analysis. However, you can specify --detect_adapter_for_pe to enable it.\n       # For PE data, fastp will run a little slower if you specify the sequence adapters or enable adapter auto-detection, but usually result in a slightly cleaner output, since the overlap analysis may fail due to sequencing errors or adapter dimers.\n       # -q, --qualified_quality_phred       the quality value that a base is qualified. Default 15 means phred quality &gt;=Q15 is qualified.\n       # --cut_front move a sliding window from front (5') to tail, drop the bases in the window if its mean quality is below cut_mean_quality, stop otherwise.\n       # --length_required  reads shorter than length_required will be discarded, default is 15. (int [=15])\n       # PE data, the front/tail trimming settings are given with -f, --trim_front1 and -t, --trim_tail1\n       additionalParams:\n         fastp: \" --detect_adapter_for_pe -q 20 --cut_front --trim_front1 3 --cut_tail --trim_tail1 3 --cut_mean_quality 10 --length_required 50 \"\n         reportOnly: false\n       timeLimit: \"AUTO\"\n    nonpareil:\n      additionalParams: \" -v 10 -r 1234 \"\n    filterHuman:\n      additionalParams: \"  \"\n      database:\n        download:\n          source: https://openstack.cebitec.uni-bielefeld.de:8080/databases/human_filter.db.20231218v2.gz\n          md5sum: cc92c0f926656565b1156d66a0db5a3c\n  assembly:\n    megahit:\n      # --mem-flag 0 to use minimum memory, --mem-flag 1 (default) moderate memory and --mem-flag 2 all memory.\n      # meta-sensitive: '--min-count 1 --k-list 21,29,39,49,...,129,141' \n      # meta-large:  '--k-min  27  --k-max 127 --k-step 10' (large &amp; complex metagenomes, like soil)\n      additionalParams: \" --min-contig-len 500 --presets meta-sensitive \"\n      fastg: true\n      resources:\n        RAM:\n          mode: 'PREDICT'\n          predictMinLabel: 'highmemLarge'\n  binning:\n    bwa2:\n      additionalParams: \n        bwa2: \" \"\n        # samtools flags are used to filter resulting bam file\n        samtoolsView: \" -F 3584 \" \n    contigsCoverage:\n      additionalParams: \" --min-covered-fraction 0 --min-read-percent-identity 100 --min-read-aligned-percent 100 \"\n    genomeCoverage:\n      additionalParams: \" --min-covered-fraction 0 --min-read-percent-identity 100 --min-read-aligned-percent 100 \"\n    # Primary binning tool\n    metabat:\n      # Set --seed positive numbers to reproduce the result exactly. Otherwise, random seed will be set each time.\n      additionalParams: \" --seed 234234  \"\nresources:\n  highmemLarge:\n    cpus: 28\n    memory: 60\n  highmemMedium:\n    cpus: 14\n    memory: 30\n  large:\n    cpus: 28\n    memory: 58\n  medium:\n    cpus: 14\n    memory: 29\n  small:\n    cpus: 7\n    memory: 14\n  tiny:\n    cpus: 1\n    memory: 1\n</code></pre>"},{"location":"tutorials/tutorial1/binning/#assembly-evaluation-by-read-mapping","title":"Assembly Evaluation by Read Mapping","text":"<p>Before we take a closer look at the binning results,  let us first check the mapping of the reads to the assembly (alignment) that was computed as part of the binning process and saved in the file with the <code>.bam</code> prefix. The number of reads mapped to the contigs of the assembly is called the contig abundance and is also used as input to a binning tool to separate contigs. From this alignment we get the contig coverage (read depth) which refers to the number of times each nucleotide position in a contig is covered by sequencing reads on average. It quantifies how many reads align to each base pair in the assembled sequence.</p> <p>Task 2</p> <p>First we have to index the sorted BAM file: <pre><code>cd ~/mgcourse/\n</code></pre> <pre><code>samtools index output/data/1/binning/*/contigMapping/data.bam\n</code></pre></p> <p>Usually a bam file has to be sorted before it can be indexed. The sorting has already been done for you by the Toolkit.</p> <p>Task 3</p> <p>To look at the BAM file you can use the following command (You can quit less with <code>q</code>): <pre><code>cd ~/mgcourse/\n</code></pre> <pre><code>samtools view output/data/1/binning/*/contigMapping/data.bam | less\n</code></pre></p> <p>IGV, or Integrated Genome Viewer, is an open-source bioinformatics tool designed for the interactive visualization and exploration of genomic data. It primarily serves as a platform to view alignments of sequencing reads against a reference genome, aiding in the identification of genetic variants such as SNPs (Single Nucleotide Polymorphisms) and indels (insertions and deletions). Beyond read alignment, IGV supports various data types, including gene annotations and expression levels,  offering a comprehensive view that enhances understanding of the genomic context. In our case, we are only interested in the alignment.</p> <p>Task 4</p> <p>Now copy/link everything you need for igv in a separate folder:</p> <p><pre><code>cd ~/mgcourse/\n</code></pre> <pre><code>mkdir igv_data\ncd igv_data\ncp ../output/data/1/assembly/*/megahit/data_contigs.fa.gz .\ngunzip data_contigs.fa.gz\nln -s ../output/data/1/binning/*/contigMapping/data.bam\nln -s ../output/data/1/binning/*/contigMapping/data.bam.bai\n</code></pre></p> <p>Task 5</p> <p>We will use the IGV genome browser to look at the mappings. <pre><code>igv\n</code></pre></p> <p>Now let's look at the mapped reads:</p> <ol> <li>Load the contig sequences into IGV. Use the menu <code>Genomes-&gt;Load Genome from File...</code> and search for <code>home|ubuntu/mgcourse/igv_data/data_contigs.fa</code></li> <li>Load the BAM file into IGV. Use menu <code>File-&gt;Load from File...</code> and search for <code>home|ubuntu/mgcourse/igv_data/data.bam</code></li> </ol> <p>Task 6</p> <p>Look for errors in the mappings - are all those error sequencing errors?</p> Solution <p>Some errors are due to merging multiple strains into one contig. This can be clearly seen  when there are a large number of errors at one position with different possibilities for that base.</p>"},{"location":"tutorials/tutorial1/binning/#metabat-results","title":"MetaBAT Results","text":"<p>Let's now inspect the bins created by MetaBAT:</p> <p>Task 7</p> <p>How many bins did metabat generate? Locate the metabat results and the fasta files for each bin in the output folder.</p> Solution <p><pre><code>cd ~/mgcourse/\n</code></pre> <pre><code>ls -l output/data/1/binning/*/metabat/\n</code></pre> <pre><code>data_bin.1.fa\ndata_bin.10.fa\ndata_bin.2.fa\ndata_bin.3.fa\ndata_bin.4.fa\ndata_bin.5.fa\ndata_bin.6.fa\ndata_bin.7.fa\ndata_bin.8.fa\ndata_bin.9.fa\ndata_bin_contig_mapping.tsv\ndata_bins_stats.tsv\ndata_contigs_depth.tsv\ndata_notBinned.fa\n</code></pre> MetaBAT generated 10 bins.</p> <p>You might have noticed that there is also a <code>data_notBinned.fa</code> file that represents contigs that could not be binned. We will investigate these contigs in the next section of this tutorial. The next question you might want to ask is whether you can trust these bins and which organism they represent according to a taxonomy. Before that, let's have a look at what other information the Metagenomics-Toolkit provides as part of the binning output.</p> <p>Task 8</p> <p>There is a file containing some statistics on the generated bins <code>data_bins_stats.tsv</code>.  Find out, which bin has the highest coverage and which one has the highest N50.</p> Solution <p>You can have a look on some bin statistics with: <pre><code>cd ~/mgcourse/\n</code></pre> <pre><code>less output/data/1/binning/0.5.0/metabat/data_bins_stats.tsv\n</code></pre> <pre><code>SAMPLE  BIN_ID  format  type    num_seqs        sum_len min_len avg_len max_len Q1      Q2      Q3      sum_gap N50     Q20(%)  Q30(%)  GC(%)   COVERAGE\ndata    data_bin.1.fa   FASTA   DNA     150     507900  2504    3386.0  7616    2757.0  3127.5  3704.0  0       3321    0.00    0.00    33.29   6.15009\ndata    data_bin.10.fa  FASTA   DNA     177     641723  2504    3625.6  8372    2855.0  3213.0  4036.0  0       3542    0.00    0.00    49.49   7.7348\ndata    data_bin.2.fa   FASTA   DNA     224     717735  2500    3204.2  6592    2692.0  2958.5  3442.0  0       3109    0.00    0.00    50.72   5.05946\ndata    data_bin.3.fa   FASTA   DNA     150     520975  2500    3473.2  9084    2739.0  3127.0  3909.0  0       3328    0.00    0.00    43.93   7.0744\ndata    data_bin.4.fa   FASTA   DNA     264     1340562 2505    5077.9  17160   3213.0  4328.5  6132.5  0       5540    0.00    0.00    32.36   10.8599\ndata    data_bin.5.fa   FASTA   DNA     88      280467  2509    3187.1  5229    2746.5  2998.5  3429.5  0       3093    0.00    0.00    39.02   5.12779\ndata    data_bin.6.fa   FASTA   DNA     139     1149027 2523    8266.4  35033   4367.0  6419.0  10607.0 0       10241   0.00    0.00    40.52   14.8691\ndata    data_bin.7.fa   FASTA   DNA     280     944933  2505    3374.8  6899    2786.5  3165.0  3690.5  0       3346    0.00    0.00    57.96   6.31446\ndata    data_bin.8.fa   FASTA   DNA     66      682946  2837    10347.7 31045   6083.0  8277.5  13778.0 0       13397   0.00    0.00    25.03   15.588\ndata    data_bin.9.fa   FASTA   DNA     374     1625795 2501    4347.0  11747   3019.0  3725.5  5239.0  0       4532    0.00    0.00    27.53   9.6491\n</code></pre> In this result, bin 8 has the highest coverage (15.588) and highest N50 (13397). Also note, that some of the bins highly differ in their GC content.</p> <p>Task 9</p> <p>Let's say we want to take a closer look at these bins. Switch back to EMGB and try to find the bin with the largest contig, this one: <pre><code>SAMPLE  BIN_ID  format  type    num_seqs        sum_len min_len avg_len max_len Q1      Q2      Q3      sum_gap N50     Q20(%)  Q30(%)  GC(%)   COVERAGE\ndata    data_bin.4.fa   FASTA   DNA     139     1149027 2523    8266.4  35033   4367.0  6419.0  10607.0 0       10241   0.00    0.00    40.52   14.9085\n</code></pre> How many genes are found on this contig and how long is the longest one on it?</p> Solution <p>One way to get to the desired contig is to click on the \"Contigs\" tab in the upper middle part of EMGB. It is located between \"Genes\" and \"GO Stats\". Here we activate the filter \"Show only contigs longer than x\" and set the filter to 30000. Two contigs remain, the longer one at the top is the one we are looking for.  The table shows us the first solution, right next to the contig name and length it says that there are 41 genes on this contig.        </p> <p>If you look to the right of our contig, you will see the \"Actions\" tab with two buttons. A colorful one representing small contigs and a Download \".fna\" button. Click on the first one. A popup will open, showing this contig with all predicted genes and their positions on this contig. If you look at the top of the popup,  you will find the solution to your second problem. There are some basic statistics for this contig: <pre><code>Overall length (bp)     Number of genes     Longest gene (bp)   Dataset     Current gene\n35033   41  2756    data    -   \n</code></pre> The longest gene is 2756 bp in size.</p> <p>Task 10</p> <p>We now know what the longest contig looks like, but what about its bin? Go back to EMGB again and try to find the name of the bin our contig belongs to. How many contigs are in that bin?</p> Solution <p>An easy way to find our contig is to use filters again. Simply search for the longest contig again, as we did in the task above. This time, we don't click on the \"full contigs\"  button, but on the little filter icon right next to the contig name, you might see a hovering text \"create a filter for this contig\". After clicking it, we see a new filter at the top left \"Contig ID is exactly X\", which means that only information that this contig is part of will be shown. We now click on the \"MAGs\" (Metagenome-assembled genomes) tab, three tabs to the right of the \"Contigs\" tab we are currently in. Since only information about this contig is shown,  thanks to the filter set, and the contig can only be part of one bin/MAG, there is only one left. The one we are looking for, our contig, is part of \"bin 7\". Lets do the same trick again. Click on the filter icon next to the bin name. There should now be another active filter at the top left, \"Bins is exactly X\". Go back to the Contigs tab. Only or first contig in the list? Sure, because there is still our first filter that lets us see only information about this contig. Go to the top filter section and remove the first filter \"Contig ID is exactly X\" by clicking the little X in the top right corner of its box. With the filter removed,  there is only \"Bins is exactly X\" left, which means we see all information about this bin. If not, is the \"Show only contigs longer than X\" box still checked? If so, uncheck it.  With this done, the information should update, we see \"Showing 139 contigs that contain predicted genes matching the filters\", which is the answer to our question. </p> <p>\u27a1\ufe0f Continue to: Assessing Bin Quality</p>"},{"location":"tutorials/tutorial1/classification/","title":"Classification","text":"<p>Taxonomic classification tools assign taxonomic labels to reads or assembled contigs of metagenomic datasets. We will perform taxonomic classification of our genome bins using GTDB-Tk.</p>"},{"location":"tutorials/tutorial1/classification/#genome-taxonomy-database-gtdb","title":"Genome Taxonomy Database (GTDB)","text":"<p>GTDB is an open-access database that provides a comprehensive taxonomy for bacterial and archaeal genomes. It uses genome-scale phylogenetic analysis to classify microbial genomes into a standardized taxonomic framework. The database is regularly updated with new genomes, ensuring it reflects the latest advancements in microbial taxonomy. GTDB-Tk is a software toolkit that enables users to classify their own genomic or metagenomic data against the GTDB taxonomy. It uses metrics like average nucleotide identity (ANI) and other phylogenetic markers to determine the taxonomic placement of query genomes. This tool is particularly useful for researchers who want to assign taxonomic labels to their sequences within a consistent and widely-accepted framework, facilitating better understanding of microbial diversity and evolution.</p> <p>See the GTDB-Tk homepage for more information.</p> <p>Next, let's assign taxonomic labels to our binning results using GTDB-Tk.  The following snippet represents the Toolkit configuration for the classification part of the MagAttributes module: Classification Configuration File Snippet 1<pre><code>    gtdb:\n      buffer: 1000\n      database:\n        download:\n          source: https://openstack.cebitec.uni-bielefeld.de:8080/databases/gtdbtk_r226_v2_data.tar.gz\n          md5sum: 0777ea44f9d7d96f9ca6ea43ed016799\n      additionalParams: \" --min_af 0.65 --scratch_dir . \"\n</code></pre></p> <p>Task 1</p> <p>Run the following command for the classification: <pre><code>cd ~/mgcourse/\n\nNXF_VER=25.04.2 nextflow run metagenomics/metagenomics-tk \\\n      -profile standard \\\n      -params-file https://raw.githubusercontent.com/metagenomics/metagenomics-tk/refs/heads/master/default/tutorials/tutorial1/fullpipeline_classification.yml \\\n      -ansi-log false \\\n      -entry wFullPipeline \\\n      -resume \\\n      --databases $(readlink -f databases) \\\n      --input.paired.path https://raw.githubusercontent.com/metagenomics/metagenomics-tk/refs/heads/master/test_data/tutorials/tutorial1/reads.tsv \\\n      --logDir logs_classification\n</code></pre></p> <p>For reference, your complete parameter file should look like this:</p> Parameter-file <pre><code>tempdir: \"tmp\"\nsummary: false\ns3SignIn: false\ninput:\n  paired:\n    path: \"test_data/tutorials/tutorial1/reads.tsv\"\n    watch: false\noutput: output\nlogDir: log\nrunid: 1\ndatabases: \"/vol/scratch/databases\"\npublishDirMode: \"symlink\"\nlogLevel: 1\nscratch: false\nsteps:\n  qc:\n    fastp:\n       # For PE data, the adapter sequence auto-detection is disabled by default since the adapters can be trimmed by overlap analysis. However, you can specify --detect_adapter_for_pe to enable it.\n       # For PE data, fastp will run a little slower if you specify the sequence adapters or enable adapter auto-detection, but usually result in a slightly cleaner output, since the overlap analysis may fail due to sequencing errors or adapter dimers.\n       # -q, --qualified_quality_phred       the quality value that a base is qualified. Default 15 means phred quality &gt;=Q15 is qualified.\n       # --cut_front move a sliding window from front (5') to tail, drop the bases in the window if its mean quality is below cut_mean_quality, stop otherwise.\n       # --length_required  reads shorter than length_required will be discarded, default is 15. (int [=15])\n       # PE data, the front/tail trimming settings are given with -f, --trim_front1 and -t, --trim_tail1\n       additionalParams:\n         fastp: \" --detect_adapter_for_pe -q 20 --cut_front --trim_front1 3 --cut_tail --trim_tail1 3 --cut_mean_quality 10 --length_required 50 \"\n         reportOnly: false\n       timeLimit: \"AUTO\"\n    nonpareil:\n      additionalParams: \" -v 10 -r 1234 \"\n    filterHuman:\n      additionalParams: \"  \"\n      database:\n        download:\n          source: https://openstack.cebitec.uni-bielefeld.de:8080/databases/human_filter.db.20231218v2.gz\n          md5sum: cc92c0f926656565b1156d66a0db5a3c\n  assembly:\n    megahit:\n      # --mem-flag 0 to use minimum memory, --mem-flag 1 (default) moderate memory and --mem-flag 2 all memory.\n      # meta-sensitive: '--min-count 1 --k-list 21,29,39,49,...,129,141' \n      # meta-large:  '--k-min  27  --k-max 127 --k-step 10' (large &amp; complex metagenomes, like soil)\n      additionalParams: \" --min-contig-len 500 --presets meta-sensitive \"\n      fastg: true\n      resources:\n        RAM:\n          mode: 'PREDICT'\n          predictMinLabel: 'highmemLarge'\n  binning:\n    bwa2:\n      additionalParams: \n        bwa2: \" \"\n        # samtools flags are used to filter resulting bam file\n        samtoolsView: \" -F 3584 \" \n    contigsCoverage:\n      additionalParams: \" --min-covered-fraction 0 --min-read-percent-identity 100 --min-read-aligned-percent 100 \"\n    genomeCoverage:\n      additionalParams: \" --min-covered-fraction 0 --min-read-percent-identity 100 --min-read-aligned-percent 100 \"\n    # Primary binning tool\n    metabat:\n      # Set --seed positive numbers to reproduce the result exactly. Otherwise, random seed will be set each time.\n      additionalParams: \" --seed 234234  \"\n  magAttributes:\n    checkm2:\n      database:\n        download:\n          source: \"https://openstack.cebitec.uni-bielefeld.de:8080/databases/checkm2_v2.tar.gz\"\n          md5sum: a634cb3d31a1f56f2912b74005f25f09\n      additionalParams: \"  \"\n    gtdb:\n      buffer: 1000\n      database:\n        download:\n          source: https://openstack.cebitec.uni-bielefeld.de:8080/databases/gtdbtk_r226_v2_data.tar.gz\n          md5sum: 0777ea44f9d7d96f9ca6ea43ed016799\n      additionalParams: \" --min_af 0.65 --scratch_dir . \"\nresources:\n  highmemLarge:\n    cpus: 28\n    memory: 60\n  highmemMedium:\n    cpus: 14\n    memory: 30\n  large:\n    cpus: 28\n    memory: 58\n  medium:\n    cpus: 14\n    memory: 29\n  small:\n    cpus: 7\n    memory: 14\n  tiny:\n    cpus: 1\n    memory: 1\n</code></pre> <p>The classification took some time, most of it is due to a mash database, that needs to be built before the actual classification. After that, the classification itself was quite fast. The fast execution can be explained by the fast identification of a genome, and thus a taxonomic label, in the GTDB via Mash and ANI. Classification based on marker genes was almost not necessary. In a real metagenome sample, the classification would usually take longer. You can read here about the different steps of the GTDB-Tk classification.</p> <p>Task 2</p> <p>Inspect the different columns of the GTDB-Tk output file with the following command to see how many genomes were processed by the ANI screening and how many were identified by the marker gene approach.</p> <pre><code>cd ~/mgcourse/\n</code></pre> <pre><code>cat output/data/1/magAttributes/*/gtdb/data_gtdbtk_generated_combined.tsv  | column -s$'\\t' -t | less -S\n</code></pre> Solution <p>For example, the column <code>fastani_ani</code> reports that 9 out of 10 genomes were processed using ANI.  <pre><code>fastani_ani\n98.52\n97.43\n98.4\n98.7\n99.23\nN/A\n99.51\n98.58\n99.67\n98.82\n</code></pre></p> <p>Now, of course, you are interested in the classification of your genomes. The results can be affected by missing marker genes or contamination, as indicated on the GTDB-Tk website. You already have an estimate of the completeness and contamination of your genomes from the bin quality sections and can use this information for the next task.</p> <p>Task 3</p> <p>Check the classification and BIN_ID column of the gtdb output. What is the classification of genomes that are at least 50% complete and are at most 10% contaminated?</p> Solution <pre><code>cd ~/mgcourse/\n</code></pre> <p>By executing the following two commands you get the classification and the quality values: <pre><code>cut -f 1,5 output/data/1/magAttributes/*/gtdb/data_gtdbtk_generated_combined.tsv | column -s$'\\t' -t\n</code></pre></p> <pre><code>cut -f2,3,4 output/data/1/magAttributes/*/checkm2/data_checkm2_generated.tsv | column -s$'\\t' -t\n</code></pre> <p>The output is the following: <pre><code>BIN_ID          classification\ndata_bin.1.fa   d__Bacteria;p__Desulfobacterota;c__Desulfovibrionia;o__Desulfovibrionales;f__Desulfovibrionaceae;g__Lawsonia;s__Lawsonia intracellularis\ndata_bin.10.fa  d__Bacteria;p__Bacteroidota;c__Bacteroidia;o__Bacteroidales;f__Porphyromonadaceae;g__Porphyromonas;s__Porphyromonas gingivalis\ndata_bin.2.fa   d__Bacteria;p__Bdellovibrionota;c__Bdellovibrionia;o__Bdellovibrionales;f__Bdellovibrionaceae;g__Bdellovibrio;s__Bdellovibrio bacteriovorus\ndata_bin.3.fa   d__Bacteria;p__Aquificota;c__Aquificae;o__Aquificales;f__Aquificaceae;g__Aquifex;s__Aquifex aeolicus\ndata_bin.4.fa   d__Bacteria;p__Bacillota_A;c__Clostridia;o__Tissierellales;f__Peptoniphilaceae;g__Finegoldia;s__Finegoldia magna_H\ndata_bin.5.fa   d__Bacteria;p__Campylobacterota;c__Campylobacteria;o__Campylobacterales;f__Helicobacteraceae;g__Helicobacter;s__\ndata_bin.6.fa   d__Bacteria;p__Chlamydiota;c__Chlamydiia;o__Chlamydiales;f__Chlamydiaceae;g__Chlamydophila;s__Chlamydophila pneumoniae\ndata_bin.7.fa   d__Bacteria;p__Actinomycetota;c__Actinomycetia;o__Mycobacteriales;f__Mycobacteriaceae;g__Mycobacterium;s__Mycobacterium leprae\ndata_bin.8.fa   d__Bacteria;p__Pseudomonadota;c__Gammaproteobacteria;o__Enterobacterales;f__Enterobacteriaceae;g__Wigglesworthia;s__Wigglesworthia glossinidia_B\ndata_bin.9.fa   d__Bacteria;p__Fusobacteriota;c__Fusobacteriia;o__Fusobacteriales;f__Fusobacteriaceae;g__Fusobacterium;s__Fusobacterium nucleatum\n</code></pre></p> <pre><code>BIN_ID          COMPLETENESS  CONTAMINATION\ndata_bin.1.fa   16.88         0.0\ndata_bin.10.fa  24.04         0.04\ndata_bin.2.fa   12.81         0.12\ndata_bin.3.fa   20.57         0.01\ndata_bin.4.fa   52.73         0.3\ndata_bin.5.fa   11.09         0.0\ndata_bin.6.fa   81.91         0.46\ndata_bin.7.fa   20.68         0.75\ndata_bin.8.fa   85.4          0.28\ndata_bin.9.fa   39.66         0.62\n</code></pre> <p>Based on the output and the comparison of the BIN_ID columns, we can say that the following species could be detected that belong to bins that are at least 50% complete and at most 10% contaminated: <pre><code>d__Bacteria;p__Bacillota_A;c__Clostridia;o__Tissierellales;f__Peptoniphilaceae;g__Finegoldia;s__Finegoldia magna_H\nd__Bacteria;p__Chlamydiota;c__Chlamydiia;o__Chlamydiales;f__Chlamydiaceae;g__Chlamydophila;s__Chlamydophila pneumoniae\nd__Bacteria;p__Pseudomonadota;c__Gammaproteobacteria;o__Enterobacterales;f__Enterobacteriaceae;g__Wigglesworthia;s__Wigglesworthia glossinidia_B\n</code></pre></p> <p>Task 4</p> <p>Inspect the results in EMGB - (1) filter for a specific species in the tree on the left side then inspect the MAG tab. (2) Do the same by filtering for a specific MAG (using the filter symbol near the bin id).  What do you observe? </p> Solution <p>Not all genes are classified the same way as the corresponding MAG. Genes are classified using an LCA with mmseqs taxonomy, MAGs are classified using GTDBtk. If in doubt, the GTDBtk classification is probably more accurate.</p> <p>\u27a1\ufe0f Continue to: Annotation</p>"},{"location":"tutorials/tutorial1/introduction/","title":"Introduction","text":"<p>This workshop demonstrates typical steps of a short-read metagenomic analysis using the Metagenomics-Toolkit and is divided into eight parts. In this part you will learn how to configure and run the Toolkit and what the output of a Toolkit run looks like.</p>"},{"location":"tutorials/tutorial1/introduction/#tutorial-scope-and-requirements","title":"Tutorial Scope and Requirements","text":"<p>The Metagenomics-Toolkit allows you to run either the full pipeline of assembly, binning and many other downstream analysis tasks or the individual analyses separately.  In this tutorial you will only use the full pipeline mode. The full pipeline mode itself is structured into two parts. The first part runs the Toolkit on each sample separately (per-sample), and the second part runs a combined downstream analysis on the output of the per-sample part, called aggregation.  In this tutorial, you will only run the per-sample part. While there are several optimizations for running the Toolkit on a cloud-based setup,  during this workshop you will run the Toolkit on a single machine.</p>"},{"location":"tutorials/tutorial1/introduction/#requirements","title":"Requirements","text":"<p>Course Participants</p> <p>In case you are a course participant, it is very likely that a machine has been prepared for you and you can ignore this section. If in doubt, ask the course organizers. </p> <ul> <li> <p>Basic Linux command line usage</p> </li> <li> <p>This tutorial has been tested on a machine with 28 CPUs and 64 GBs of RAM with Ubuntu installed on it.</p> </li> <li> <p>Docker: Install Docker by following the official Docker installation instructions.</p> </li> <li> <p>Java: In order to run Nextflow, you need to install Java on your machine, which can be achieved via <code>sudo apt install default-jre</code>.</p> </li> <li> <p>Nextflow should be installed. Please check the official Nextflow instructions.</p> </li> </ul>"},{"location":"tutorials/tutorial1/introduction/#preparations","title":"Preparations","text":"<p>For this tutorial, we need some preparations on our machine. Please note that the following instructions can be customized for your specific machine. </p>"},{"location":"tutorials/tutorial1/introduction/#link-to-your-volume","title":"Link to your volume","text":"<p>We are using the location <code>~/mgcourse/</code> as the directory, where all the analysis is stored. We assume that there is a larger storage volume mounted at <code>/vol/mgcourse</code>.  If your preferred volume is at another location, change <code>/vol/mgcourse</code> to a different path accordingly. If you don't have another volume with more storage available you can also simply leave out the following command and create the directory with <code>mkdir ~/mgcourse</code>.</p> <p>Course Participants</p> <p>In case you are a course participant, a volume has been prepared for you, so please run the following commands. </p> <p>Task 1</p> <p>We will link the volume to <code>~/mgcourse</code>, so we can use that link for the rest of this tutorial: <pre><code>ln -s /vol/mgcourse/ ~/mgcourse\n</code></pre></p> <p>Create a database directory</p> <pre><code>mkdir -p ~/mgcourse/databases\n</code></pre>"},{"location":"tutorials/tutorial1/introduction/#metagenomics-toolkit-introduction","title":"Metagenomics-Toolkit Introduction","text":""},{"location":"tutorials/tutorial1/introduction/#execution","title":"Execution","text":"<p>The Toolkit is based on Nextflow and you can execute the Toolkit based on the following command line schema:</p> <pre><code>NXF_VER=NEXTFLOW_VERSION nextflow run metagenomics/metagenomics-tk NEXTFLOW_OPTIONS TOOLKIT_OPTIONS \n</code></pre> <ul> <li> <p><code>NEXTFLOW_VERSION</code> is the Nextflow version that is supported by Nextflow. Every code snippet in this tutorial has a hard coded version number.    If you ever choose the wrong version, then the Toolkit will print out the versions that are supported.</p> </li> <li> <p><code>NEXTFLOW_OPTIONS</code> are options that are implemented by Nextflow:</p> <ul> <li> <p><code>-profile</code> determines the technology which is used to execute the Toolkit. Here we support standard for running the workflow on a single machine and              slurm for running the Toolkit on a cluster which uses SLURM to distribute jobs. </p> </li> <li> <p><code>-params-file</code> points to a configuration file that tells the Toolkit which analyses to run and which resources it should use. An example configuration file will be explained in the next section.</p> </li> <li> <p><code>-resume</code> In some cases, you may want to resume the workflow execution, such as when you add an analysis.               Resuming the workflow forces Nextflow to reuse the results of the previous analyses that the new analysis depends on, rather than starting from scratch. </p> </li> <li> <p><code>-ansi-log</code> accepts a boolean (default: true) that tells Nextflow on true to print every update as a new line on the terminal. If false then Nextflow               prints a line for every process and updates the specific line on an update. We recommend to setting -ansi-log to false because it is not possible to               print all possible processes on a terminal at once when running the Toolkit.</p> </li> <li> <p><code>-entry</code> specifies which entrypoint Nextflow should use to run the workflow. For running the full pipeline, that you will use in this workshop, you use the wFullPipeline entrypoint.             If you ever want to run separate modules, you can check on the modules specific page (e.g. assembly).</p> </li> </ul> </li> <li> <p><code>TOOLKIT_OPTIONS</code> are options that are provided by the Toolkit. All Toolkit options are either in a configuration file or can be provided on the command line which will be explained in the following section. </p> </li> </ul> <p>Task 2</p> <p>Open the Metagenomics-Toolkit wiki on a second browser tab by a click on this link. Imagine you need to run the quality-control part separately. Can you tell the name of the entrypoint?  Use the wiki page you have opened on another tab to answer the question.</p> Solution <p>If you go to the quality control part, then you will find the wShortReadQualityControl entrypoint for short reads and the wOntQualityControl entrypoint for long reads.</p>"},{"location":"tutorials/tutorial1/introduction/#configuration","title":"Configuration","text":"<p>The Toolkit uses a YAML configuration file that specifies global parameters, the analyses that will be executed and the computational resources that can be used. </p> <p>The configuration file is divided into three parts:</p>"},{"location":"tutorials/tutorial1/introduction/#part-1-global-workflow-parameters","title":"Part 1: Global Workflow Parameters","text":"<p>The following snippet shows parameters that affect the whole execution of the  workflow. All parameters are explained in a dedicated Toolkit wiki section. </p> Example Configuration File Snippet 1<pre><code>summary: false\ns3SignIn: false \ninput:\n  paired:\n    path: \"test_data/tutorials/tutorial1/reads.tsv\"\n    watch: false\noutput: output\nlogDir: log\nrunid: 1\ndatabases: \"/vol/scratch/databases\"\npublishDirMode: \"symlink\"\nlogLevel: 1\nscratch: false \n</code></pre> <p>Computational Resources</p> <p>Please note that computational resources are also global parameters and will be handled in the third part of this configuration section. </p>"},{"location":"tutorials/tutorial1/introduction/#input-field","title":"Input Field","text":"<p>The input field (line 3, snippet 1) specifies the type of input data to process (Nanopore, Illumina, data hosted on SRA or a mirror) and you can find a dedicated wiki section here. Regardless of which input type is used, the user must provide a file containing a list of datasets to be processed. The list can be a list of remote or local files and in the case of SRA, a list of SRA run IDs.</p> <p>Since you will work with short read data in this tutorial, your input file looks like this:</p> Input File<pre><code>SAMPLE  READS1  READS2\ndata    https://s3.bi.denbi.de/cmg/mgcourses/mg2025/reads/read1.fq.gz   https://s3.bi.denbi.de/cmg/mgcourses/mg2025/reads/read2.fq.gz\n</code></pre> <p>The first column (SAMPLE) specifies the name of the dataset. The second (READS1) and third (READS2) column specify the files containing the forward and reverse reads.</p>"},{"location":"tutorials/tutorial1/introduction/#part-2-toolkit-analyses-steps","title":"Part 2: Toolkit Analyses Steps","text":"<p>Analyses, or sometimes called modules, that the Toolkit executes are placed directly under the steps attribute in the configuration file. In the example below, the modules qc and assembly are placed directly under the steps attribute. Any tools or methods that are used as part of the module can be considered as a property of the module. For example MEGAHIT is executed as part of the assembly module. The level below the tool names is for configuring the tools and methods. Each analysis is listed on the modules page. </p> Example Configuration File Snippet 2<pre><code>steps:\n  qc:\n    fastp:\n       # For PE data, the adapter sequence auto-detection is disabled by default since the adapters can be trimmed by overlap analysis. However, you can specify --detect_adapter_for_pe to enable it.\n       # For PE data, fastp will run a little slower if you specify the sequence adapters or enable adapter auto-detection, but usually result in a slightly cleaner output, since the overlap analysis may fail due to sequencing errors or adapter dimers.\n       # -q, --qualified_quality_phred       the quality value that a base is qualified. Default 15 means phred quality &gt;=Q15 is qualified.\n       # --cut_front move a sliding window from front (5') to tail, drop the bases in the window if its mean quality is below cut_mean_quality, stop otherwise.\n       # --length_required  reads shorter than length_required will be discarded, default is 15. (int [=15])\n       # PE data, the front/tail trimming settings are given with -f, --trim_front1 and -t, --trim_tail1\n       additionalParams:\n         fastp: \" --detect_adapter_for_pe -q 20 --cut_front --trim_front1 3 --cut_tail --trim_tail1 3 --cut_mean_quality 10 --length_required 50 \"\n         reportOnly: false\n       timeLimit: \"AUTO\"\n    nonpareil:\n      additionalParams: \" -v 10 -r 1234 \"\n    filterHuman:\n      additionalParams: \"  \"\n      database:\n        download:\n          source: https://openstack.cebitec.uni-bielefeld.de:8080/databases/human_filter.db.20231218v2.gz\n          md5sum: cc92c0f926656565b1156d66a0db5a3c\n  assembly:\n    megahit:\n      # --mem-flag 0 to use minimum memory, --mem-flag 1 (default) moderate memory and --mem-flag 2 all memory.\n      # meta-sensitive: '--min-count 1 --k-list 21,29,39,49,...,129,141' \n      # meta-large:  '--k-min  27  --k-max 127 --k-step 10' (large &amp; complex metagenomes, like soil)\n      additionalParams: \" --min-contig-len 500 --presets meta-sensitive \"\n      fastg: true\n      resources:\n        RAM:\n          mode: 'PREDICT'\n          predictMinLabel: 'highmemLarge'\n</code></pre>"},{"location":"tutorials/tutorial1/introduction/#part-3-computational-resources","title":"Part 3: Computational Resources","text":"<p>The third part of a Toolkit configuration file is the resources attribute. The resources attribute lists computational resource configurations, where each configuration has a label and consists of the number of CPUs and amount of RAM assigned to it. Predefined labels are listed in the following example snippet. These labels are assigned to the processes that run the workflow specific tools. You can read more  about resource parameters here.</p> Example Configuration File Snippet 3<pre><code>resources:\n  highmemLarge:\n    cpus: 28\n    memory: 60\n  highmemMedium:\n    cpus: 14\n    memory: 30\n  large:\n    cpus: 28\n    memory: 58\n  medium:\n    cpus: 14\n    memory: 29\n  small:\n    cpus: 7\n    memory: 14\n  tiny:\n    cpus: 1\n    memory: 1\n</code></pre> <p>Task 3</p> <p>One of the first checks before running the Toolkit is to adjust the resource labels to the resources of your machine. You can run <code>nproc</code> to get the number of CPUs and <code>free -h --mega</code> to get the amount of RAM (row name: Mem, column name: total) available on your machine.  Is there enough RAM on your machine to run the Toolkit? </p> Solution <p>If you are using a machine as described in the \"Requirements\" section, then yes, there is enough RAM available for the workflow (more than 60 GBs). </p>"},{"location":"tutorials/tutorial1/introduction/#configuration-file-vs-commandline-parameters","title":"Configuration File vs. Commandline Parameters","text":"<p>All parameters defined in the YAML configuration file can also be supplied as command-line arguments. To do this, prefix each parameter with a double dash (--).  If a parameter is nested within the hierarchy of the YAML file, represent it as a command-line argument by connecting each level of the hierarchy using a dot (.).</p> <p>For example, consider the CPU count of the highmemLarge resource label in the previous snippet. The corresponding command-line argument would be <code>--resources.highmemLarge.cpus</code>.</p> <p>Task 4</p> <p>Let`s say you want to specify a path to a different input TSV file (see Example Configuration File Snippet 1) that contains a different set of input datasets.  How would you specify the parameter on the command line?</p> Solution <p><code>--input.paired.path</code></p> <p>Command line arguments supersede the configuration file. This is a quick, temporary way to change variables without touching files. </p>"},{"location":"tutorials/tutorial1/introduction/#output","title":"Output","text":"<p>The Toolkit output fulfills the following schema:</p> <pre><code>SAMPLE_NAME/RUN_ID/MODULE/MODULE_VERSION/TOOL\n</code></pre> <ul> <li> <p>RUN_ID: The run ID will be part of the output path and allows to distinguish between different pipeline configurations that were used for the same dataset.</p> </li> <li> <p>MODULE is the analysis that is executed by the Toolkit (e.g. qc, assembly, etc.). </p> </li> <li> <p>MODULE_VERSION is the version number of the module.</p> </li> <li> <p>TOOL is the tool or method that is executed by the Toolkit.</p> </li> </ul> <p>Below you can see an example output structure. Every output folder includes four log files:</p> <ul> <li> <p>.command.err: Contains the standard error.</p> </li> <li> <p>.command.out: Contains the standard output. </p> </li> <li> <p>.command.log: Contains the combined standard error and standard output.</p> </li> <li> <p>.command.sh: Contains the command that was executed. </p> </li> </ul> Example Output Directory<pre><code>output/\n\u2514\u2500\u2500 sample\n    \u2514\u2500\u2500 1\n        \u2514\u2500\u2500 qc\n            \u2514\u2500\u2500 0.3.0\n                \u251c\u2500\u2500 fastp\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 .command.err\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 .command.log\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 .command.out\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 .command.sh\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 sample_fastp.json\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 sample_fastp_summary_after.tsv\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 sample_fastp_summary_before.tsv\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 sample_interleaved.qc.fq.gz\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 sample_report.html\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 sample_unpaired.qc.fq.gz\n                \u2502\u00a0\u00a0 \u2514\u2500\u2500 sample_unpaired_summary.tsv\n                \u251c\u2500\u2500 filterHuman\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 .command.err\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 .command.log\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 .command.out\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 .command.sh\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 sample_interleaved.filtered.fq.gz\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 sample_interleaved.removed.fq.gz\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 sample_interleaved_summary_after.tsv\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 sample_interleaved_summary_before.tsv\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 sample_unpaired.filtered.fq.gz\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 sample_unpaired_summary_after.tsv\n                \u2502\u00a0\u00a0 \u2514\u2500\u2500 sample_unpaired_summary_before.tsv\n                \u251c\u2500\u2500 kmc\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 .command.err\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 .command.log\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 .command.out\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 .command.sh\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 sample.13.histo.tsv\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 sample.13.kmc.json\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 sample.21.histo.tsv\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 sample.21.kmc.json\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 sample.71.histo.tsv\n                \u2502\u00a0\u00a0 \u2514\u2500\u2500 sample.71.kmc.json\n                \u2514\u2500\u2500 nonpareil\n                    \u251c\u2500\u2500 .command.err\n                    \u251c\u2500\u2500 .command.log\n                    \u251c\u2500\u2500 .command.out\n                    \u251c\u2500\u2500 .command.sh\n                    \u251c\u2500\u2500 sample.npa\n                    \u251c\u2500\u2500 sample.npc\n                    \u251c\u2500\u2500 sample.npl\n                    \u251c\u2500\u2500 sample_nonpareil_curves.pdf\n                    \u2514\u2500\u2500 sample_nonpareil_index.tsv\n</code></pre> <p>\u27a1\ufe0f Continue to: Quality Control</p>"},{"location":"tutorials/tutorial1/quality_control/","title":"Quality Control","text":"<p>Quality control (QC) of metagenomic short reads is a series of procedures that check the accuracy, reliability, and suitability of data from metagenomic studies for further analysis. In the case of metagenomics, QC is very important for distinguishing real biological signals from technical problems.  Effective QC makes the results of later analyses, such as assembly and binning, more reliable.</p> <p>The key steps of quality control are</p> <ul> <li> <p>Trimming: Eliminate primer and adapter sequences and remove low-quality bases from the end of reads or entire low-quality reads.</p> </li> <li> <p>Length Filtering: Remove reads that are too short after trimming.</p> </li> <li> <p>Host DNA Removal: Remove contaminant sequences (e.g. mouse, human).</p> </li> </ul> <p>In addition, the Toolkit provides a way to estimate the sequencing effort necessary to theoretically retrieve all genomes, with Nonpareil. </p>"},{"location":"tutorials/tutorial1/quality_control/#metagenomics-toolkit","title":"Metagenomics-Toolkit","text":"<p>The Metagenomics-Toolkit offers a tool for each of the aforementioned categories. You will now execute the following Toolkit configuration:</p> QC Configuration File Snippet 1<pre><code>tempdir: \"tmp\"\nsummary: false\ns3SignIn: false\ninput:\n  paired:\n    path: \"test_data/tutorials/tutorial1/reads.tsv\"\n    watch: false\noutput: output\nlogDir: log\nrunid: 1\ndatabases: \"/vol/scratch/databases\"\npublishDirMode: \"symlink\"\nlogLevel: 1\nscratch: false \nsteps:\n  qc:\n    fastp:\n       # For PE data, the adapter sequence auto-detection is disabled by default since the adapters can be trimmed by overlap analysis. However, you can specify --detect_adapter_for_pe to enable it.\n       # For PE data, fastp will run a little slower if you specify the sequence adapters or enable adapter auto-detection, but usually result in a slightly cleaner output, since the overlap analysis may fail due to sequencing errors or adapter dimers.\n       # -q, --qualified_quality_phred       the quality value that a base is qualified. Default 15 means phred quality &gt;=Q15 is qualified.\n       # --cut_front move a sliding window from front (5') to tail, drop the bases in the window if its mean quality is below cut_mean_quality, stop otherwise.\n       # --length_required  reads shorter than length_required will be discarded, default is 15. (int [=15])\n       # PE data, the front/tail trimming settings are given with -f, --trim_front1 and -t, --trim_tail1\n       additionalParams:\n         fastp: \" --detect_adapter_for_pe -q 20 --cut_front --trim_front1 3 --cut_tail --trim_tail1 3 --cut_mean_quality 10 --length_required 50 \"\n         reportOnly: false\n       timeLimit: \"AUTO\"\n    nonpareil:\n      additionalParams: \" -v 10 -r 1234 \"\n    filterHuman:\n      additionalParams: \"  \"\n      database:\n        download:\n          source: https://openstack.cebitec.uni-bielefeld.de:8080/databases/human_filter.db.20231218v2.gz\n          md5sum: cc92c0f926656565b1156d66a0db5a3c\nresources:\n  highmemLarge:\n    cpus: 28\n    memory: 60\n  highmemMedium:\n    cpus: 14\n    memory: 30\n  large:\n    cpus: 28\n    memory: 58\n  medium:\n    cpus: 14\n    memory: 29\n  small:\n    cpus: 7\n    memory: 14\n  tiny:\n    cpus: 1\n    memory: 1\n</code></pre> <p>Task 1</p> <p>You saw the introduction part of the tutorial regarding the structure of the config file.  Can you tell which tools or methods are executed in the QC part?</p> Solution <p>Fastp, nonpareil, filterHuman </p> <p>The following configuration runs the tools </p> <p>Task 2</p> <p>Copy the following command to execute the Toolkit. The Toolkit will need about 3 to 5 minutes to complete.</p> <pre><code>cd ~/mgcourse/\n\nNXF_VER=25.04.2 nextflow run metagenomics/metagenomics-tk \\\n      -profile standard \\\n      -params-file https://raw.githubusercontent.com/metagenomics/metagenomics-tk/refs/heads/master/default/tutorials/tutorial1/fullpipeline_qc.yml \\\n      -ansi-log false \\\n      -entry wFullPipeline \\\n      --input.paired.path https://raw.githubusercontent.com/metagenomics/metagenomics-tk/refs/heads/master/test_data/tutorials/tutorial1/reads.tsv \\\n      --logDir logs_qc \n</code></pre> <p>In the following we will inspect the outputs of the mentioned tools.</p>"},{"location":"tutorials/tutorial1/quality_control/#output","title":"Output","text":""},{"location":"tutorials/tutorial1/quality_control/#fastp","title":"Fastp","text":"<p>Fastp is an efficient, versatile, open-source tool for preprocessing FASTQ files and quality control in next-generation sequencing (NGS) workflows. Written in C++, fastp provides high-speed performance without compromising accuracy, making it suitable for handling large sequencing data sets. For quality and adapter trimming fastp trims reads from 5\u2019 end to 3\u2019 end using a sliding window. </p> <p>If the mean quality of bases inside a window drops below a specific q-score, the remainder of the read will be trimmed. If a read gets too short during this trimming, it will be discarded.</p> <p>Quality Report</p> <p>The Metagenomics-Toolkit allows you to run fastp by only reporting the quality of the data. This way you can easily inspect the data before actually further processing it. You can do this by setting the <code>reportOnly</code> parameter to <code>true</code> in the config below. </p> <p>Task 3</p> <p>You can view the fastp output in the qc output directory: <pre><code>ls -1 output/data/1/qc/*/fastp/\n</code></pre></p> <p>which results in the following output:</p> <pre><code>data_fastp.json\ndata_fastp_summary_after.tsv\ndata_fastp_summary_before.tsv\ndata_interleaved.qc.fq.gz\ndata_report.html\ndata_unpaired.qc.fq.gz\ndata_unpaired_summary.tsv\n</code></pre> <p>The fastp output for your data can be viewed here. Since the data that you are using during this tutorial is simulated, we will inspect the fastp output of a real dataset.  The following fastp output belongs to a wastewater sequencing project: Wastewater Fastp Output.</p> <p>Task 4</p> <p>How many reads of the wastewater dataset passed the filter? Between which quality score ranges does the read1 FASTQ file fall before and after QC.</p> Solution <p>78.752339% of the reads passed the filter. The quality score for read1 ranges between 29 and 36 before filtering and after QC between 35 and 36. </p>"},{"location":"tutorials/tutorial1/quality_control/#host-dna-removal","title":"Host DNA Removal","text":"<p>The Toolkit provides the SRA human-scrubber tool that uses a k-mer based approach to search for human sequences against a k-mer database using human reference sequences.</p> <p>Task 5</p> <p>You can find the result of the tool in the filterHuman directory: The output of the tool are the filtered sequences, and it also includes statistics about the sequences before and after filtering.   </p> <pre><code>ls output/data/1/qc/*/filterHuman/\n</code></pre> <p>The following command tells you how many sequences are left after filtering:</p> <pre><code>column -t -s$'\\t' output/data/1/qc/*/filterHuman/data_interleaved_summary_after.tsv \n</code></pre> <p>The num_seqs column tells you the number of sequences.</p> <p>How many reads were removed after filtering?</p> Solution <p>Only 6 reads were removed, which are most likely false positives. </p>"},{"location":"tutorials/tutorial1/quality_control/#nonpareil","title":"Nonpareil","text":"<p>Nonpareil provides an estimate of what to expect from a dataset. Through a redundancy analysis of the reads, you get an estimate of the average genome coverage and  the sequencing effort required to theoretically retrieve all genomes. In addition, Nonpareil provides a diversity estimate based solely on the read data. </p> <p>Task 6</p> <p>You can view the Nonpareil output in the following directory: </p> <pre><code>ls -1 output/data/1/qc/*/nonpareil/\n</code></pre> <p>The output of ls is the following:</p> <pre><code>data.npa\ndata.npc\ndata.npl\ndata_nonpareil_curves.pdf\ndata_nonpareil_index.tsv\n</code></pre> <p>The data_nonpareil_curves.pdf contains the Nonpareil curve for the dataset:</p> <p></p> <p>The Nonpareil curves show the fit of coverage per sequencing effort to a sigmoidal model. The lines indicate coverage estimates from subsampling (solid) and Nonpareil projection curves (dashed), and the lower-end arrows indicate the sequence diversity. Horizontal red dashed lines indicate 95 and 99% coverage. You can obtain the estimated average coverage for a given sequencing effort by inspecting the rightmost point of the solid line.</p> <p>Sequencing Diversity Arrow</p> <p>The arrow that displays the sequencing diversity in the plot is only useful when comparing to other sequencing diversity arrows. </p> <p>Task 7</p> <p>From the plot, it is hard to get the exact numbers (e.g. average coverage for a given sequencing effort) and does not provide the diversity estimate.</p> <p>Instead of looking into the plot it is easier to check the data_nonpareil_index.tsv file. In the Toolkit wiki we provide an explanation for the columns provided by Nonpareil. View the file in order to get the following information:</p> <ul> <li> <p>The estimated genome coverage for the current sequencing effort</p> </li> <li> <p>The diversity estimate.</p> </li> </ul> Solution <p>You can view the file by using <code>cat</code> or <code>column</code></p> <pre><code>column -t -s$'\\t' output/data/1/qc/*/nonpareil/data_nonpareil_index.tsv\n</code></pre> <p>With the help of the wiki you can get the questioned values:</p> <ul> <li> <p>Estimated genome coverage for the current sequencing effort: 80% </p> </li> <li> <p>Diversity Estimate: 17.3</p> </li> </ul> <p>\u27a1\ufe0f Continue to: Assembly</p>"},{"location":"tutorials/tutorial2/assigning_lineage_and_function/","title":"Assessing Bin Quality, Lineage, and Gene Function","text":"<p>In this part of the tutorial we evaluate the recovered Bins for quality, classify them taxonomically and annotate their functional content. First, we run CheckM2, the machine\u2011learning based version of CheckM that assesses completeness and contamination by leveraging the full repertoire of genomic markers\u2014including multi\u2011copy genes and metabolic modules\u2014rather than relying solely on single\u2011copy core genes. Next, we use GTDB\u2011Tk to assign each Bin to the Genome Taxonomy Database (GTDB), a regularly updated, phylogenetic classification of bacteria and archaea that employs ANI and phylogenetic markers to place query genomes within a standardized taxonomy. We will then annotate the genomes with Prodigal (via Prokka) to predict genes and rapidly assign functions, providing a comprehensive view of each Bin\u2019s biological potential. Finally, we apply RGI to examine the Bins with regard to antibiotic resistance genes.</p> <p>Together, these steps deliver a complete assessment of Bin quality, lineage, and gene function within the Metagenomics\u2011Toolkit workflow.</p>"},{"location":"tutorials/tutorial2/assigning_lineage_and_function/#metagenomics-toolkit-execution","title":"Metagenomics-Toolkit Execution","text":"<p>The following lines represent the part of the configuration that tells the Toolkit to run the MagAttributes and Annotation module that include the CheckM2, GTDB-Tk, Prokka and Resistance Gene Identifier (RGI) tool:</p> MagAttributes Annotation Configuration File Snippet 1<pre><code>  magAttributes:\n    # gtdbtk classify_wf\n    # --min_af minimum alignment fraction to assign genome to a species cluster (0.5)\n    gtdb:\n      buffer: 1000\n      database:\n        extractedDBPath: /vol/volume/reference_databases/gtdbtk_r226_v2_data/release226/\n      additionalParams: \" --min_af 0.65 --scratch_dir . \"\n    checkm2:\n      database:\n        extractedDBPath: /vol/volume/reference_databases/checkm2/\n      additionalParams: \"  \"\n  annotation:\n    prokka:\n      defaultKingdom: false\n      additionalParams: \" --mincontiglen 500 \"\n    rgi:\n      # --include_loose includes matches of more distant homologs of AMR genes which may also report spurious partial matches\n      # --include_nudge Partial ORFs may do not pass curated bitscore cut-offs or novel samples may contain divergent alleles, so nudging \n      #                 95% identity Loose matches to Strict matches may aid resistome annotation \n      additionalParams: \" --include_loose --include_nudge \"\n      database:\n        download:\n          source: https://openstack.cebitec.uni-bielefeld.de:8080/databases/card_broadstreet-v4.0.1.tar.bz2\n          md5sum: 2396eaf95f2d35c58c135f7db2211dbb\n</code></pre> <p>For reference, your complete parameter file looks like this:</p> Parameter-file <pre><code>tempdir: \"tmp\"\nsummary: false\ns3SignIn: false\ninput:\n  paired:\n    path: \"test_data/tutorials/tutorial2/samples.tsv\"\n    watch: false\noutput: output\nlogDir: log\nrunid: 1\ndatabases: \"/vol/volume/sessions/metagenomics_metagenomics-tk/databases/\"\npublishDirMode: \"copy\"\nlogLevel: 1\nscratch: false\nsteps:\n  qc:\n    fastp:\n       # For PE data, the adapter sequence auto-detection is disabled by default since the adapters can be trimmed by overlap analysis. However, you can specify --detect_adapter_for_pe to enable it.\n       # For PE data, fastp will run a little slower if you specify the sequence adapters or enable adapter auto-detection, but usually result in a slightly cleaner output, since the overlap analysis may fail due to sequencing errors or adapter dimers.\n       # -q, --qualified_quality_phred       the quality value that a base is qualified. Default 15 means phred quality &gt;=Q15 is qualified.\n       # --cut_front move a sliding window from front (5') to tail, drop the bases in the window if its mean quality is below cut_mean_quality, stop otherwise.\n       # --length_required  reads shorter than length_required will be discarded, default is 15. (int [=15])\n       # PE data, the front/tail trimming settings are given with -f, --trim_front1 and -t, --trim_tail1\n       additionalParams:\n         fastp: \" --detect_adapter_for_pe -q 20 --cut_front --trim_front1 3 --cut_tail --trim_tail1 3 --cut_mean_quality 10 --length_required 50 \"\n         reportOnly: false \n       timeLimit: \"AUTO\"\n    nonpareil:\n      additionalParams: \" -v 10 -r 1234 \"\n    kmc:\n      timeLimit: \"AUTO\"\n      additionalParams:\n        # Computes k-mer distribution based on k-mer length 13 and 21\n        #  -sm - use strict memory mode (memory limit from -m&lt;n&gt; switch will not be exceeded)\n        #  -cs&lt;value&gt; - maximal value of a counter\n        count: \" -sm -cs10000 \"\n        histo: \" -cx50000 \"\n    filterHuman:\n      additionalParams: \"  \"\n      database:\n        download:\n          source: https://openstack.cebitec.uni-bielefeld.de:8080/databases/human_filter.db.20231218v2.gz\n          md5sum: cc92c0f926656565b1156d66a0db5a3c\n  assembly:\n    megahit:\n      # --mem-flag 0 to use minimum memory, --mem-flag 1 (default) moderate memory and --mem-flag 2 all memory.\n      # meta-sensitive: '--min-count 1 --k-list 21,29,39,49,...,129,141' \n      # meta-large:  '--k-min  27  --k-max 127 --k-step 10' (large &amp; complex metagenomes, like soil)\n      additionalParams: \" --min-contig-len 1000 --presets meta-sensitive \"\n      fastg: true\n      resources:\n        RAM:\n          mode: 'PREDICT'\n          predictMinLabel: 'medium'\n  binning:\n    bwa2:\n      additionalParams: \n        bwa2: \" \"\n        # samtools flags are used to filter resulting bam file\n        samtoolsView: \" -F 3584 \" \n    contigsCoverage:\n      additionalParams: \" --min-covered-fraction 0 --min-read-percent-identity 100 --min-read-aligned-percent 100 \"\n    genomeCoverage:\n      additionalParams: \" --min-covered-fraction 0 --min-read-percent-identity 100 --min-read-aligned-percent 100 \"\n    # Primary binning tool\n    metabat:\n      # Set --seed positive numbers to reproduce the result exactly. Otherwise, random seed will be set each time.\n      additionalParams: \" --seed 234234  \"\n  magAttributes:\n    # gtdbtk classify_wf\n    # --min_af minimum alignment fraction to assign genome to a species cluster (0.5)\n    gtdb:\n      buffer: 1000\n      database:\n        extractedDBPath: /vol/volume/reference_databases/gtdbtk_r226_v2_data/release226/\n      additionalParams: \" --min_af 0.65 --scratch_dir . \"\n    checkm2:\n      database:\n        extractedDBPath: /vol/volume/reference_databases/checkm2/\n      additionalParams: \"  \"\n  annotation:\n    prokka:\n      defaultKingdom: false\n      additionalParams: \" --mincontiglen 500 \"\n    rgi:\n      # --include_loose includes matches of more distant homologs of AMR genes which may also report spurious partial matches\n      # --include_nudge Partial ORFs may do not pass curated bitscore cut-offs or novel samples may contain divergent alleles, so nudging \n      #                 95% identity Loose matches to Strict matches may aid resistome annotation \n      additionalParams: \" --include_loose --include_nudge \"\n      database:\n        download:\n          source: https://openstack.cebitec.uni-bielefeld.de:8080/databases/card_broadstreet-v4.0.1.tar.bz2\n          md5sum: 2396eaf95f2d35c58c135f7db2211dbb\nresources:\n  highmemLarge:\n    cpus: 28\n    memory: 230\n  highmemMedium:\n    cpus: 14\n    memory: 113\n  large:\n    cpus: 28\n    memory: 58\n  medium:\n    cpus: 14\n    memory: 29\n  small:\n    cpus: 7\n    memory: 14\n  tiny:\n    cpus: 1\n    memory: 1\n</code></pre> <p>If you run the Toolkit again with the <code>-resume</code> flag, it will run all additional modules and tools you have specified.</p> <p>Task 1</p> <p>Change into the directory of the Toolkit session. <pre><code>cd /vol/volume/sessions/metagenomics_metagenomics-tk \n</code></pre></p> <p>Run the MagAttributes annotation module with the Metagenomics-Toolkit using the following command: <pre><code>NXF_VER=25.04.2 nextflow run metagenomics/metagenomics-tk \\\n      -profile standard \\\n      -params-file https://raw.githubusercontent.com/metagenomics/metagenomics-tk/refs/heads/doc/exhibition-day/tutorial/default/tutorials/tutorial2/fullPipeline_lineage_and_function.yml \\\n      -ansi-log false \\\n      -entry wFullPipeline \\\n      -resume \\\n      --input.paired.path https://raw.githubusercontent.com/metagenomics/metagenomics-tk/refs/heads/doc/exhibition-day/tutorial/test_data/tutorials/tutorial2/samples.tsv \\\n      --logDir logs_lineage_and_function \n</code></pre> This command will take between four and six minutes to run.</p>"},{"location":"tutorials/tutorial2/assigning_lineage_and_function/#output","title":"Output","text":""},{"location":"tutorials/tutorial2/assigning_lineage_and_function/#computing-completeness-and-contamination-using-checkm2","title":"Computing Completeness and Contamination using CheckM2","text":"<p>Task 2</p> <p>Locate the CheckM results inside the <code>output/SRR492065/</code> directory and find out the completeness and contamination values for all of our Bins.</p> Solution <p>Change into the directory of the Toolkit session. <pre><code>cd /vol/volume/sessions/metagenomics_metagenomics-tk \n</code></pre> View the Bin ID, Bin completeness and contamination: <pre><code>cut -f2,3,4 output/SRR492065/1/magAttributes/*/checkm2/SRR492065_checkm2_generated.tsv\n</code></pre> Output: <pre><code>BIN_ID  COMPLETENESS    CONTAMINATION\nSRR492065_bin.1.fa      54.16   0.31\nSRR492065_bin.2.fa      49.86   0.15\nSRR492065_bin.3.fa      94.75   0.09\nSRR492065_bin.4.fa      85.99   18.95\nSRR492065_bin.5.fa      96.32   0.14\n</code></pre></p> <p>According to Bowers et al. <sup>1</sup>, Bins can be grouped into three different quality categories. Part of this quality assessment involves evaluating Bin completeness and contamination.</p> <ul> <li> <p>High quality: &gt; 90% complete and &lt; 5% contamination </p> </li> <li> <p>Medium quality: &gt;= 50% complete and &lt; 10% contamination </p> </li> <li> <p>Low quality: &lt; 50% complete and &lt; 10% contamination </p> </li> </ul> <p>Based on this definition we can conclude that there are 2 high quality, 1 medium quality and 1 low quality Bin.</p>"},{"location":"tutorials/tutorial2/assigning_lineage_and_function/#genome-taxonomy-database-gtdb","title":"Genome Taxonomy Database (GTDB)","text":"<p>Next the Toolkit performed taxonomic classification of our genome Bins using GTDB-Tk. GTDB-Tk is a software toolkit that enables users to classify their own genomic or metagenomic data against the GTDB taxonomy.</p> <p>See the GTDB-Tk homepage for more information.</p> <p>The classification itself is quite fast. The fast execution can be explained by the fast identification of a genome, and thus a taxonomic label, in the GTDB via Mash and ANI. Classification based on marker genes was not necessary. In a real metagenome sample, the classification would usually take longer.</p> <p>Task 3</p> <p>Check the classification and BIN_ID column of the gtdb output for the SRR492065 sample. What is the classification of genomes that are at least 50% complete and are at most 10% contaminated?</p> Solution <p>Change into the directory of the Toolkit session.</p> <pre><code>cd /vol/volume/sessions/metagenomics_metagenomics-tk \n</code></pre> <p>By executing the following two commands you get the classification and the quality values: <pre><code>cut -f 1,5 output/SRR492065/1/magAttributes/*/gtdb/SRR492065_gtdbtk_generated_combined.tsv | column -s$'\\t' -t\n</code></pre></p> <p>To make a comparison with the CheckM values, list all the completeness and contamination values again. <pre><code>cut -f2,3,4 output/SRR492065/1/magAttributes/*/checkm2/SRR492065_checkm2_generated.tsv | column -s$'\\t' -t\n</code></pre></p> <p>The output for CheckM and GTDB-Tk is the following: <pre><code>BIN_ID              classification\nSRR492065_bin.1.fa  d__Bacteria;p__Bacillota;c__Bacilli;o__Lactobacillales;f__Enterococcaceae;g__Enterococcus;s__Enterococcus faecalis\nSRR492065_bin.2.fa  d__Bacteria;p__Bacillota;c__Bacilli;o__Lactobacillales;f__Enterococcaceae;g__Enterococcus;s__Enterococcus faecalis\nSRR492065_bin.3.fa  d__Bacteria;p__Actinomycetota;c__Actinomycetes;o__Propionibacteriales;f__Propionibacteriaceae;g__Cutibacterium;s__Cutibacterium avidum\nSRR492065_bin.4.fa  d__Bacteria;p__Bacillota;c__Bacilli;o__Staphylococcales;f__Staphylococcaceae;g__Staphylococcus;s__Staphylococcus aureus\nSRR492065_bin.5.fa  d__Bacteria;p__Bacillota;c__Clostridia;o__Tissierellales;f__Peptoniphilaceae;g__Peptoniphilus_A;s__Peptoniphilus_A lacydonensis\n</code></pre></p> <pre><code>BIN_ID  COMPLETENESS    CONTAMINATION\nSRR492065_bin.1.fa      54.16   0.31\nSRR492065_bin.2.fa      49.86   0.15\nSRR492065_bin.3.fa      94.75   0.09\nSRR492065_bin.4.fa      85.99   18.95\nSRR492065_bin.5.fa      96.32   0.14\n</code></pre> <p>Based on the output and the comparison of the BIN_ID columns, we can say that the following species could be detected that belong to bins that are at least 50% complete and at most 10% contaminated: <pre><code>d__Bacteria;p__Bacillota;c__Bacilli;o__Lactobacillales;f__Enterococcaceae;g__Enterococcus;s__Enterococcus faecalis\nd__Bacteria;p__Actinomycetota;c__Actinomycetes;o__Propionibacteriales;f__Propionibacteriaceae;g__Cutibacterium;s__Cutibacterium avidum\nd__Bacteria;p__Bacillota;c__Clostridia;o__Tissierellales;f__Peptoniphilaceae;g__Peptoniphilus_A;s__Peptoniphilus_A lacydonensis\n</code></pre></p> <p>You now have detected two pathogens and since many bacterial pathogens routinely carry antibiotic\u2011resistance genes we now will now investigate their genes using Prokka and RGI.</p>"},{"location":"tutorials/tutorial2/assigning_lineage_and_function/#annotate-genes-with-prokka-and-rgi","title":"Annotate Genes with Prokka and RGI","text":"<p>Prokka is an efficient, user-friendly and open source bioinformatics tool designed for the annotation of bacterial genomes. Prokka supports standard output formats such as GenBank and GFF, facilitating further analysis with compatible tools. </p> <p>See Prokka homepage for more information.</p> <p>We used the genes predicted by Prodigal to identify possible antibiotic resistance genes with the Resistance Gene Identifier (RGI) tool. You can inspect the output of RGI by running the following commands: </p> <p>Task 4</p> <p>Change into the directory of the Toolkit session. <pre><code>cd /vol/volume/sessions/metagenomics_metagenomics-tk \n</code></pre></p> <p>List all files that were produced for the SRR492065 sample. <pre><code>ls output/SRR492065/1/annotation/2.0.1/rgi/\n</code></pre></p> <p>The tool produces among other things <code>*.csv</code> files that contain a summary of all hits found according to the CARD database for every bin. </p> <p>Task 5</p> <p>Depending on the bin ID of your Enterococcus faecalis, count the number of antibiotic resistance genes that could be detected in that bin of the sample SRR492065.</p> Solution <p>Change into the directory of the Toolkit session. <pre><code>cd /vol/volume/sessions/metagenomics_metagenomics-tk \n</code></pre> In the following case the Enterococcus faecalis genome has the bin number 2. <pre><code>cat output/SRR492065/1/annotation/2.0.1/rgi/SRR492065_bin.2.fa.rgi-1.csv\n</code></pre></p> <p>In the output you can count the detected genes (6): <pre><code>gene,SRR492065_bin.2.fa.rgi\nIreK,1\ndfrE,1\nefrA,1\nefrB,1\nlsaA,1\nvanY gene in vanB cluster,1\n</code></pre></p> <p>\u27a1\ufe0f Continue to:Dereplication</p> <ol> <li> <p>https://www.nature.com/articles/nbt.3893\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorials/tutorial2/dereplication/","title":"Dereplication, and Coverage Inspection of MAGs","text":"<p>In this final part of the tutorial we aggregate the MAGs recovered from multiple samples, dereplicate them, and briefly inspect both the resulting clusters and the read-mapping depth (coverage) of some MAGs in the small example dataset.</p> <p>Dereplication reduces a large set of MAGs to a non-redundant genome catalogue by grouping nearly identical genomes (typically using Average Nucleotide Identity, ANI) and selecting the highest-quality MAG as the representative for each cluster. This avoids double-counting strains, simplifies downstream analyses and prevents ambiguous read mapping to many almost-identical genomes.</p> <p>Within the Metagenomics-Toolkit, dereplication is implemented as a separate module that:</p> <ol> <li>takes the previous run or a table describing all MAGs (including completeness, contamination and coverage),</li> <li>clusters similar genomes in a bottom-up fashion (using Mash and ANI distances), and</li> <li>selects the best representative per cluster based on quality and assembly statistics.</li> </ol>"},{"location":"tutorials/tutorial2/dereplication/#metagenomics-toolkit-execution","title":"Metagenomics-Toolkit Execution","text":"<p>The following lines represent the part of the configuration that tells the Toolkit to run the Dereplication module on your existing results.</p> Dereplication Configuration File Snippet<pre><code>output: \"output\"\ninput: \"output\"\nlogDir: log\nrunid: 1\nlogLevel: 1\nscratch: false\npublishDirMode: \"copy\"\nsteps:\n  dereplication:\n    bottomUpClustering:\n      # stricter MIMAG medium quality\n      minimumCompleteness: 50\n      maximumContamination: 5\n      ANIBuffer: 20\n      mashBuffer: 2000\n      method: 'ANI'\n      additionalParams:\n        mash_sketch: \"\"\n        mash_dist: \"\"\n        # cluster cutoff\n        cluster: \" -c 0.05 \"\n        pyani: \" -m ANIb \"\n        representativeAniCutoff: 0.95\n  readMapping:\n    bwa2:\n      additionalParams:\n        bwa2_index: \"\"\n        bwa2_mem: \"\"\n     # This module produces two abundance tables.\n     # One table is based on relative abundance and the second one on the trimmed mean.\n     # Just using relative abundance makes it difficult to tell if a genome is part of a dataset.\n     # Thats why it makes sense to set at leat a low min covered fraction parameter.\n    coverm: \" --min-covered-fraction 80  --min-read-percent-identity 95 --min-read-aligned-percent 95 \"\n    covermONT: \" --min-covered-fraction 80  --min-read-aligned-percent 95 \"\n    minimap:\n      additionalParams:\n        minimap_index: \"\"\n        minimap: \"\"\n</code></pre> <p>For reference, your complete parameter file for this final step looks like this:</p> Parameter-file <pre><code>tempdir: \"tmp\"\nsummary: false\ns3SignIn: false\noutput: \"output\"\ninput: \"output\"\nlogDir: log\nrunid: 1\nlogLevel: 1\nscratch: false\npublishDirMode: \"copy\"\nsteps:\n  dereplication:\n    bottomUpClustering:\n      # stricter MIMAG medium quality\n      minimumCompleteness: 50\n      maximumContamination: 5\n      ANIBuffer: 20\n      mashBuffer: 2000\n      method: 'ANI'\n      additionalParams:\n        mash_sketch: \"\"\n        mash_dist: \"\"\n        # cluster cutoff\n        cluster: \" -c 0.05 \"\n        pyani: \" -m ANIb \"\n        representativeAniCutoff: 0.95\n  readMapping:\n    bwa2:\n      additionalParams:\n        bwa2_index: \"\"\n        bwa2_mem: \"\"\n     # This module produces two abundance tables.\n     # One table is based on relative abundance and the second one on the trimmed mean.\n     # Just using relative abundance makes it difficult to tell if a genome is part of a dataset.\n     # Thats why it makes sense to set at leat a low min covered fraction parameter.\n    coverm: \" --min-covered-fraction 80  --min-read-percent-identity 95 --min-read-aligned-percent 95 \"\n    covermONT: \" --min-covered-fraction 80  --min-read-aligned-percent 95 \"\n    minimap:\n      additionalParams:\n        minimap_index: \"\"\n        minimap: \"\"\nresources:\n  highmemLarge:\n    cpus: 28\n    memory: 230\n  highmemMedium:\n    cpus: 14\n    memory: 113\n  large:\n    cpus: 28\n    memory: 58\n  medium:\n    cpus: 14\n    memory: 29\n  small:\n    cpus: 7\n    memory: 14\n  tiny:\n    cpus: 1\n    memory: 1\n</code></pre> <p>We assume that all required steps by the previous tutorials are done.</p> <p>If you run the Toolkit again with the <code>-resume</code> flag and the above dereplication configuration, it will reuse all previous results (QC, assembly, binning, MagAttributes) and only execute the new dereplication step.</p> <p>Task 1</p> <p>Change into the directory of the Toolkit session. <pre><code>cd /vol/volume/sessions/metagenomics_metagenomics-tk\n</code></pre></p> <p>Run the dereplication module on your results using the following command: <pre><code>NXF_VER=25.04.2 nextflow run metagenomics/metagenomics-tk \\\n      -profile standard \\\n      -params-file https://raw.githubusercontent.com/metagenomics/metagenomics-tk/refs/heads/doc/exhibition-day/tutorial/default/tutorials/tutorial2/fullPipeline_dereplication.yml \\\n      -ansi-log false \\\n      -entry wAggregatePipeline \\\n      -resume \\\n    --input output \\\n      --logDir dereplication \n</code></pre></p> <p>This will:</p> <ul> <li>collect all the necessary results from your MAGs to create the summary table needed as input,</li> <li>cluster similar genomes based on ANI, and</li> <li>write a <code>clusters.tsv</code> file describing the dereplicated genome clusters.</li> </ul>"},{"location":"tutorials/tutorial2/dereplication/#output","title":"Output","text":""},{"location":"tutorials/tutorial2/dereplication/#dereplication-clusters","title":"Dereplication Clusters","text":"<p>The example dereplication input table looks like this (one MAG per row):</p> <pre><code>BIN_ID  COMPLETENESS    COVERAGE        CONTAMINATION   HETEROGENEITY   PATH    N50\nSRR492065_bin.3.fa      36.29   171.259 0.39    0       metagenomics-tk/work/58/267ffbad749945cb7dfd98b23a7449/SRR492065_bin.3.fa      73239\nSRR492183_bin.2.fa      100.0   142.023 0.1     0       metagenomics-tk/work/90/b5b80ef93e064e7d723eca9bd92241/SRR492183_bin.2.fa      175163\nSRR492065_bin.1.fa      85.99   14.7729 18.95   0       metagenomics-tk/work/58/267ffbad749945cb7dfd98b23a7449/SRR492065_bin.1.fa      7308\nSRR492065_bin.5.fa      64.75   152.837 0.54    0       metagenomics-tk/work/58/267ffbad749945cb7dfd98b23a7449/SRR492065_bin.5.fa      161674\nSRR492065_bin.4.fa      96.32   70.5657 0.14    0       metagenomics-tk/work/58/267ffbad749945cb7dfd98b23a7449/SRR492065_bin.4.fa      25145\nSRR492065_bin.2.fa      94.64   23.2949 0.11    0       metagenomics-tk/work/58/267ffbad749945cb7dfd98b23a7449/SRR492065_bin.2.fa      16802\nSRR492183_bin.3.fa      92.84   27.7866 0.08    0       metagenomics-tk/work/90/b5b80ef93e064e7d723eca9bd92241/SRR492183_bin.3.fa      21121\nSRR492183_bin.1.fa      99.96   45.5264 0.01    0       metagenomics-tk/work/90/b5b80ef93e064e7d723eca9bd92241/SRR492183_bin.1.fa      46795\n</code></pre> <p>The dereplication module uses these columns as follows:</p> <ul> <li><code>COMPLETENESS</code>, <code>CONTAMINATION</code> \u2013 to filter MAGs before clustering.</li> <li><code>COVERAGE</code>, <code>N50</code>, <code>HETEROGENEITY</code> \u2013 to decide which genome becomes the representative of each cluster.</li> </ul> <p>After running Task 1, dereplication produces a <code>clusters.tsv</code> file with one row per MAG and three columns:</p> <ul> <li><code>CLUSTER</code> \u2013 cluster ID (all genomes in the same cluster are near-identical),</li> <li><code>GENOME</code> \u2013 path or identifier of the genome,</li> <li><code>REPRESENTATIVE</code> \u2013 <code>1</code> if the genome is the chosen representative, <code>0</code> otherwise.</li> </ul> <p>A typical file will look like this:</p> <pre><code>CLUSTER  GENOME                                 REPRESENTATIVE\n1        bin.1.fa                1\n1        bin.2.fa                0\n2        bin.8.fasta             1\n2        bin.9.fasta             0\n3        bin.10.fasta            0\n3        bin.32.fa               1\n</code></pre> <p>(The exact clustering in your run may differ; this is just an illustrative example.)</p> <p>Task 2</p> <p>Locate the dereplication clusters file and inspect how many clusters were formed and which genomes were chosen as representatives. Hint: There is a new directory in your output folder.</p> Solution <p>Change into the Toolkit session directory (if you are not already there):</p> <pre><code>cd /vol/volume/sessions/metagenomics_metagenomics-tk\n</code></pre> <p>First, locate the new directory in your output folder:</p> <p><pre><code>ls -la output/\n</code></pre> There is a new AGGREGATED directory, lets look at our <code>clusters.tsv</code> in the similarly named directory.</p> <pre><code>cat output/AGGREGATED/1/dereplication/0.1.1/bottomUpClustering/clusters/clusters.tsv\n</code></pre> <p>It should look similar to this:</p> <pre><code>CLUSTER GENOME  REPRESENTATIVE\n1       SRR492065_bin.2.fa      0.0\n3       SRR492065_bin.4.fa      1.0\n2       SRR492065_bin.5.fa      0.0\n2       SRR492183_bin.1.fa      1.0\n4       SRR492183_bin.2.fa      1.0\n1       SRR492183_bin.3.fa      1.0\n</code></pre> <p>This shows you: * how many clusters exist (number of unique values in the <code>CLUSTER</code> column), and * which MAG(s) per cluster have <code>REPRESENTATIVE=1</code>. Two MAGs where redundant and are now part of clusters represented only by its best MAG.</p>"},{"location":"tutorials/tutorial2/dereplication/#read-mapping-depth-coverage-of-example-mags","title":"Read Mapping Depth (Coverage) of Example MAGs","text":"<p>Coverage values are derived from mapping reads from all input samples back to each representative MAG.</p> <p>Task 3</p> <p>Let's have a look to examine the coverage (read mapping depth) of our representative MAGs. Explore the new directory, find the <code>readMapping</code> folder and look for the relative abundance coverage of our representatives with all SRR492065 reads. </p> Solution <p>Change again into the Toolkit session directory:</p> <pre><code>cd /vol/volume/sessions/metagenomics_metagenomics-tk\n</code></pre> <p>Explore the AGGREGATED directory:</p> <pre><code>ls -la output/AGGREGATED/1/\n</code></pre> <p>There is a <code>readMapping</code> directory. If we explore further:</p> <p><pre><code>ls -la output/AGGREGATED/1/readMapping/0.1.0/\n</code></pre> Also a <code>genomeCoverage</code> one. If we look at this, we see something like:</p> <p><pre><code>-rw-r--r-- 1 ubuntu ubuntu  111 Dec  9 18:50 SRR492065_count.tsv\n-rw-r--r-- 1 ubuntu ubuntu  120 Dec  9 18:50 SRR492065_mean.tsv\n-rw-r--r-- 1 ubuntu ubuntu  138 Dec  9 18:50 SRR492065_relative_abundance.tsv\n-rw-r--r-- 1 ubuntu ubuntu  120 Dec  9 18:50 SRR492065_rpkm.tsv\n-rw-r--r-- 1 ubuntu ubuntu  116 Dec  9 18:50 SRR492065_tpm.tsv\n-rw-r--r-- 1 ubuntu ubuntu  118 Dec  9 18:50 SRR492065_trimmed_mean.tsv\n-rw-r--r-- 1 ubuntu ubuntu  110 Dec  9 18:50 SRR492183_count.tsv\n-rw-r--r-- 1 ubuntu ubuntu  113 Dec  9 18:50 SRR492183_mean.tsv\n-rw-r--r-- 1 ubuntu ubuntu  130 Dec  9 18:50 SRR492183_relative_abundance.tsv\n-rw-r--r-- 1 ubuntu ubuntu  110 Dec  9 18:50 SRR492183_rpkm.tsv\n-rw-r--r-- 1 ubuntu ubuntu  112 Dec  9 18:50 SRR492183_tpm.tsv\n-rw-r--r-- 1 ubuntu ubuntu  113 Dec  9 18:50 SRR492183_trimmed_mean.tsv\ndrwxr-xr-x 2 ubuntu ubuntu 4096 Dec  9 18:50 logs\n</code></pre> In <code>SRR492065_relative_abundance.tsv</code> we find the right abundances.</p> <pre><code>cat output/AGGREGATED/1/readMapping/0.1.0/genomeCoverage/SRR492065_relative_abundance.tsv\n</code></pre> <pre><code>Genome  SRR492065\nunmapped        21.920425\nSRR492065_bin.4 22.608734\nSRR492183_bin.1 45.45554\nSRR492183_bin.2 2.6764407\nSRR492183_bin.3 7.338865\n</code></pre>"},{"location":"tutorials/tutorial2/introduction/","title":"Introduction","text":"<p>This tutorial is a short introduction to the Metagenomics-Toolkit which shows the main steps in analysing metagenomics data using the Metagenomics-Toolkit. A more detailed introduction can be found here.  In this part you will learn how to configure and run the Toolkit and what the output of a Toolkit run looks like.</p>"},{"location":"tutorials/tutorial2/introduction/#tutorial-scope-and-requirements","title":"Tutorial Scope and Requirements","text":"<p>The Metagenomics-Toolkit allows you to run either the full pipeline of assembly, binning, and many other downstream analysis tasks or the individual analyses separately.  In this tutorial you will only use the full pipeline mode. The full pipeline mode itself is structured into two parts. The first part runs the Toolkit on each sample separately (per-sample), and the second part runs a combined downstream analysis on the output of the per-sample runs; this step is called aggregation.  While there are several optimizations for running the Toolkit on a cloud-based setup,  during this workshop you will run the Toolkit on a single machine.</p>"},{"location":"tutorials/tutorial2/introduction/#requirements","title":"Requirements","text":"<p>Course Participants</p> <p>In case you are a course participant, it is very likely that a machine has been prepared for you and you can ignore this section. If in doubt, ask the course organizers. </p> <ul> <li> <p>Basic Linux command-line usage</p> </li> <li> <p>This tutorial has been tested on a machine with 28 CPUs and 64 GB of RAM with Ubuntu installed on it.</p> </li> <li> <p>Docker: Install Docker by following the official Docker installation instructions.</p> </li> <li> <p>Java: In order to run Nextflow, you need to install Java on your machine, which can be achieved via <code>sudo apt install default-jre</code>.</p> </li> <li> <p>Nextflow should be installed. Please check the official Nextflow instructions.</p> </li> </ul>"},{"location":"tutorials/tutorial2/introduction/#metagenomics-toolkit-introduction","title":"Metagenomics-Toolkit Introduction","text":""},{"location":"tutorials/tutorial2/introduction/#execution","title":"Execution","text":"<p>The Toolkit is based on Nextflow and can be executed with the following command-line pattern:</p> <pre><code>NXF_VER=NEXTFLOW_VERSION nextflow run metagenomics/metagenomics-tk NEXTFLOW_OPTIONS TOOLKIT_OPTIONS \n</code></pre> <ul> <li> <p><code>NEXTFLOW_VERSION</code> is the Nextflow version supported (or required) by the Toolkit. Every code snippet in this tutorial has a hard-coded version number.    If you ever choose the wrong version, then the Toolkit will print out the versions that are supported.</p> </li> <li> <p><code>NEXTFLOW_OPTIONS</code> are options that are implemented by Nextflow:</p> <ul> <li> <p><code>-profile</code> determines the technology which is used to execute the Toolkit. Here we support standard for running the workflow on a single machine and              slurm for running the Toolkit on a cluster which uses SLURM to distribute jobs. </p> </li> <li> <p><code>-params-file</code> points to a configuration file that tells the Toolkit which analyses to run and which resources it should use. An example configuration file will be explained in the next section.</p> </li> <li> <p><code>-resume</code> In some cases, you may want to resume the workflow execution, such as when you add an analysis.               Resuming the workflow forces Nextflow to reuse the results of the previous analyses that the new analysis depends on, rather than starting from scratch. </p> </li> <li> <p><code>-ansi-log</code> accepts a boolean (default: true) that, when set to true, tells Nextflow to print every update as a new line on the terminal. If false then Nextflow               prints a line for every process and updates the specific line on an update. We recommend setting -ansi-log to false because it is not possible to               print all possible processes on a terminal at once when running the Toolkit.</p> </li> <li> <p><code>-entry</code> specifies which entrypoint Nextflow should use to run the workflow. To run the full pipeline that you will use in this workshop, use the wFullPipeline entrypoint.             If you ever want to run separate modules, you can check on the modules-specific page (e.g. assembly).</p> </li> </ul> </li> <li> <p><code>TOOLKIT_OPTIONS</code> are options that are provided by the Toolkit. All Toolkit options are either in a configuration file or can be provided on the command line which will be explained in the following section. </p> </li> </ul> <p>Task 2</p> <p>Open the Metagenomics-Toolkit wiki in a second browser tab by clicking this link. Imagine you need to run the quality-control part separately. Can you tell the name of the entrypoint?  Use the wiki page you have opened on another tab to answer the question.</p> Solution <p>If you go to the quality control part, then you will find the wShortReadQualityControl entrypoint for short reads and the wOntQualityControl entrypoint for long reads.</p>"},{"location":"tutorials/tutorial2/introduction/#configuration","title":"Configuration","text":"<p>The Toolkit uses a YAML configuration file that specifies global parameters, the analyses that will be executed and the computational resources that can be used. </p> <p>The configuration file is divided into three parts:</p>"},{"location":"tutorials/tutorial2/introduction/#part-1-global-workflow-parameters","title":"Part 1: Global Workflow Parameters","text":"<p>The following snippet shows parameters that affect the whole execution of the  workflow. All parameters are explained in a dedicated Toolkit wiki section. </p> Example Configuration File Snippet 1<pre><code>summary: false\ns3SignIn: false \ninput:\n  paired:\n    path: \"test_data/tutorials/tutorial1/reads.tsv\"\n    watch: false\noutput: output\nlogDir: log\nrunid: 1\ndatabases: \"/vol/scratch/databases\"\npublishDirMode: \"symlink\"\nlogLevel: 1\nscratch: false \n</code></pre> <p>Computational Resources</p> <p>Please note that computational resources are also global parameters and will be handled in the third part of this configuration section. </p>"},{"location":"tutorials/tutorial2/introduction/#input-field","title":"Input Field","text":"<p>The input field (line 3, snippet 1) specifies the type of input data to process (Nanopore, Illumina, data hosted on SRA or a mirror) and you can find a dedicated wiki section here. Regardless of which input type is used, the user must provide a file containing a list of datasets to be processed. The list can be a list of remote or local files and in the case of SRA, a list of SRA run IDs.</p> <p>Since you will work with short read data in this tutorial, your input file looks like this:</p> Sample Sheet<pre><code>SAMPLE  READS1  READS2\nSRR492183   https://openstack.cebitec.uni-bielefeld.de:8080/ftp.era.ebi.ac.uk/vol1/fastq/SRR492/SRR492183/SRR492183_1.fastq.gz  https://openstack.cebitec.uni-bielefeld.de:8080/ftp.era.ebi.ac.uk/vol1/fastq/SRR492/SRR492183/SRR492183_2.fastq.gz\nSRR492065   https://openstack.cebitec.uni-bielefeld.de:8080/ftp.era.ebi.ac.uk/vol1/fastq/SRR492/SRR492065/SRR492065_1.fastq.gz  https://openstack.cebitec.uni-bielefeld.de:8080/ftp.era.ebi.ac.uk/vol1/fastq/SRR492/SRR492065/SRR492065_2.fastq.gz\n</code></pre> <p>The first column (SAMPLE) specifies the name of the dataset. The second (READS1) and third (READS2) columns specify the files containing the forward and reverse reads.</p>"},{"location":"tutorials/tutorial2/introduction/#part-2-toolkit-analyses-steps","title":"Part 2: Toolkit Analyses Steps","text":"<p>Analyses (also called modules) that the Toolkit executes are placed directly under the steps attribute in the configuration file. In the example below, the modules qc and assembly are placed directly under the steps attribute. Any tools or methods that are used as part of the module can be considered a property of the module. For example, MEGAHIT is executed as part of the assembly module. The level below the tool names is for configuring the tools and methods. Each analysis is listed on the modules page. </p> Example Configuration File Snippet 2<pre><code>steps:\n  qc:\n    fastp:\n       # For PE data, the adapter sequence auto-detection is disabled by default since the adapters can be trimmed by overlap analysis. However, you can specify --detect_adapter_for_pe to enable it.\n       # For PE data, fastp will run a little slower if you specify the sequence adapters or enable adapter auto-detection, but usually result in a slightly cleaner output, since the overlap analysis may fail due to sequencing errors or adapter dimers.\n       # -q, --qualified_quality_phred       the quality value that a base is qualified. Default 15 means phred quality &gt;=Q15 is qualified.\n       # --cut_front move a sliding window from front (5') to tail, drop the bases in the window if its mean quality is below cut_mean_quality, stop otherwise.\n       # --length_required  reads shorter than length_required will be discarded, default is 15. (int [=15])\n       # PE data, the front/tail trimming settings are given with -f, --trim_front1 and -t, --trim_tail1\n       additionalParams:\n         fastp: \" --detect_adapter_for_pe -q 20 --cut_front --trim_front1 3 --cut_tail --trim_tail1 3 --cut_mean_quality 10 --length_required 50 \"\n         reportOnly: false\n       timeLimit: \"AUTO\"\n    nonpareil:\n      additionalParams: \" -v 10 -r 1234 \"\n    filterHuman:\n      additionalParams: \"  \"\n      database:\n        download:\n          source: https://openstack.cebitec.uni-bielefeld.de:8080/databases/human_filter.db.20231218v2.gz\n          md5sum: cc92c0f926656565b1156d66a0db5a3c\n  assembly:\n    megahit:\n      # --mem-flag 0 to use minimum memory, --mem-flag 1 (default) moderate memory and --mem-flag 2 all memory.\n      # meta-sensitive: '--min-count 1 --k-list 21,29,39,49,...,129,141' \n      # meta-large:  '--k-min  27  --k-max 127 --k-step 10' (large &amp; complex metagenomes, like soil)\n      additionalParams: \" --min-contig-len 500 --presets meta-sensitive \"\n      fastg: true\n      resources:\n        RAM:\n          mode: 'PREDICT'\n          predictMinLabel: 'highmemLarge'\n</code></pre>"},{"location":"tutorials/tutorial2/introduction/#part-3-computational-resources","title":"Part 3: Computational Resources","text":"<p>The third part of a Toolkit configuration file is the resources attribute. The resources attribute lists computational resource configurations, where each configuration has a label and consists of the number of CPUs and amount of RAM assigned to it. Predefined labels are listed in the following example snippet. These labels are assigned to the processes that run the workflow-specific tools. You can read more about resource parameters here.</p> Example Configuration File Snippet 3<pre><code>resources:\n  highmemLarge:\n    cpus: 28\n    memory: 60\n  highmemMedium:\n    cpus: 14\n    memory: 30\n  large:\n    cpus: 28\n    memory: 58\n  medium:\n    cpus: 14\n    memory: 29\n  small:\n    cpus: 7\n    memory: 14\n  tiny:\n    cpus: 1\n    memory: 1\n</code></pre> <p>Task 3</p> <p>One of the first checks before running the Toolkit is to adjust the resource labels to the resources of your machine. You can run <code>nproc</code> to get the number of CPUs and <code>free -h --mega</code> to get the amount of RAM (row name: Mem, column name: total) available on your machine.  Is there enough RAM on your machine to run the Toolkit? </p> Solution <p>If you are using a machine as described in the \"Requirements\" section, then yes, there is enough RAM available for the workflow (more than 60 GB). </p>"},{"location":"tutorials/tutorial2/introduction/#configuration-file-vs-command-line-parameters","title":"Configuration File vs. Command-line Parameters","text":"<p>All parameters defined in the YAML configuration file can also be supplied as command-line arguments. To do this, prefix each parameter with a double dash (--).  If a parameter is nested within the hierarchy of the YAML file, represent it as a command-line argument by connecting each level of the hierarchy using a dot (.).</p> <p>For example, consider the CPU count of the highmemLarge resource label in the previous snippet. The corresponding command-line argument would be <code>--resources.highmemLarge.cpus</code>.</p> <p>Task 4</p> <p>Let`s say you want to specify a path to a different input TSV file (see Example Configuration File Snippet 1) that contains a different set of input datasets.  How would you specify the parameter on the command line?</p> Solution <p><code>--input.paired.path</code></p> <p>Command-line arguments supersede the configuration file. This is a quick way to change variables without touching files. </p>"},{"location":"tutorials/tutorial2/introduction/#output","title":"Output","text":"<p>The Toolkit output fulfills the following schema:</p> <pre><code>SAMPLE_NAME/RUN_ID/MODULE/MODULE_VERSION/TOOL\n</code></pre> <ul> <li> <p>RUN_ID: The run ID will be part of the output path and allows to distinguish between different pipeline configurations that were used for the same dataset.</p> </li> <li> <p>MODULE is the analysis that is executed by the Toolkit (e.g. qc, assembly, etc.). </p> </li> <li> <p>MODULE_VERSION is the version number of the module.</p> </li> <li> <p>TOOL is the tool or method that is executed by the Toolkit.</p> </li> </ul> <p>Below you can see an example output structure. Every output folder includes four log files:</p> <ul> <li> <p>.command.err: Contains the standard error.</p> </li> <li> <p>.command.out: Contains the standard output. </p> </li> <li> <p>.command.log: Contains the combined standard error and standard output.</p> </li> <li> <p>.command.sh: Contains the command that was executed. </p> </li> </ul> Example Output Directory<pre><code>output/\n\u2514\u2500\u2500 sample\n    \u2514\u2500\u2500 1\n        \u2514\u2500\u2500 qc\n            \u2514\u2500\u2500 0.3.0\n                \u251c\u2500\u2500 fastp\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 .command.err\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 .command.log\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 .command.out\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 .command.sh\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 sample_fastp.json\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 sample_fastp_summary_after.tsv\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 sample_fastp_summary_before.tsv\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 sample_interleaved.qc.fq.gz\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 sample_report.html\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 sample_unpaired.qc.fq.gz\n                \u2502\u00a0\u00a0 \u2514\u2500\u2500 sample_unpaired_summary.tsv\n                \u251c\u2500\u2500 filterHuman\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 .command.err\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 .command.log\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 .command.out\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 .command.sh\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 sample_interleaved.filtered.fq.gz\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 sample_interleaved.removed.fq.gz\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 sample_interleaved_summary_after.tsv\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 sample_interleaved_summary_before.tsv\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 sample_unpaired.filtered.fq.gz\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 sample_unpaired_summary_after.tsv\n                \u2502\u00a0\u00a0 \u2514\u2500\u2500 sample_unpaired_summary_before.tsv\n                \u251c\u2500\u2500 kmc\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 .command.err\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 .command.log\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 .command.out\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 .command.sh\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 sample.13.histo.tsv\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 sample.13.kmc.json\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 sample.21.histo.tsv\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 sample.21.kmc.json\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 sample.71.histo.tsv\n                \u2502\u00a0\u00a0 \u2514\u2500\u2500 sample.71.kmc.json\n                \u2514\u2500\u2500 nonpareil\n                    \u251c\u2500\u2500 .command.err\n                    \u251c\u2500\u2500 .command.log\n                    \u251c\u2500\u2500 .command.out\n                    \u251c\u2500\u2500 .command.sh\n                    \u251c\u2500\u2500 sample.npa\n                    \u251c\u2500\u2500 sample.npc\n                    \u251c\u2500\u2500 sample.npl\n                    \u251c\u2500\u2500 sample_nonpareil_curves.pdf\n                    \u2514\u2500\u2500 sample_nonpareil_index.tsv\n</code></pre> <p>\u27a1\ufe0f Continue to: From Reads To Genomes</p>"},{"location":"tutorials/tutorial2/reads_to_genomes/","title":"From Reads To Genomes","text":"<p>The first step is to run a thorough quality\u2011control pipeline that trims adapters and low\u2011quality ends, discards reads that become too short, and removes any host DNA contamination.  A Nonpareil analysis is then executed to estimate the sequencing depth needed to capture the full microbial diversity.</p> <p>With the cleaned reads in hand, the Toolkit proceeds to assembly. The MEGAHIT assembler, integrated into the Toolkit, is invoked to build contigs representing longer contiguous sequences in the sample. By examining each contig\u2019s k\u2011mer composition and its coverage, the binning tools cluster contigs that share similar signatures. These clusters, or \u201cBins,\u201d correspond to draft genomes reconstructed from the community, enabling researchers to recover and study individual organisms without prior knowledge of the sample\u2019s composition.</p> <p>In short, the guide describes how the Toolkit orchestrates QC, assembly and binning to turn raw metagenomic reads into biologically meaningful genome bins.</p> <p>We will inspect two SRA samples (SRR492065 and SRR492183) that belong to the same study \"Preborn infant gut metagenome\".</p>"},{"location":"tutorials/tutorial2/reads_to_genomes/#preparation","title":"Preparation","text":"<p>Due to the configuration of your VMs, you need to create a small config file with the following command, to make sure we can resume workflow runs.</p> <p>Task 1</p> <p>Change into the directory of the Toolkit session. <pre><code>cd /vol/volume/sessions/metagenomics_metagenomics-tk \n</code></pre></p> <p>Copy the following command to create the config file.</p> <pre><code>echo 'cleanup = false' &gt; nextflow.config\n</code></pre>"},{"location":"tutorials/tutorial2/reads_to_genomes/#metagenomics-toolkit-execution","title":"Metagenomics-Toolkit Execution","text":"<p>The Metagenomics-Toolkit offers multiple tools for each of the aforementioned processing steps. You will now execute the following Toolkit configuration:</p> QC,Assembly,Binning Configuration File Snippet 1<pre><code>tempdir: \"tmp\"\nsummary: false\ns3SignIn: false\ninput:\n  paired:\n    path: \"test_data/tutorials/tutorial2/samples.tsv\"\n    watch: false\noutput: output\nlogDir: log\nrunid: 1\ndatabases: \"/vol/volume/sessions/metagenomics_metagenomics-tk/databases/\"\npublishDirMode: \"copy\"\nlogLevel: 1\nscratch: false\nsteps:\n  qc:\n    fastp:\n       # For PE data, the adapter sequence auto-detection is disabled by default since the adapters can be trimmed by overlap analysis. However, you can specify --detect_adapter_for_pe to enable it.\n       # For PE data, fastp will run a little slower if you specify the sequence adapters or enable adapter auto-detection, but usually result in a slightly cleaner output, since the overlap analysis may fail due to sequencing errors or adapter dimers.\n       # -q, --qualified_quality_phred       the quality value that a base is qualified. Default 15 means phred quality &gt;=Q15 is qualified.\n       # --cut_front move a sliding window from front (5') to tail, drop the bases in the window if its mean quality is below cut_mean_quality, stop otherwise.\n       # --length_required  reads shorter than length_required will be discarded, default is 15. (int [=15])\n       # PE data, the front/tail trimming settings are given with -f, --trim_front1 and -t, --trim_tail1\n       additionalParams:\n         fastp: \" --detect_adapter_for_pe -q 20 --cut_front --trim_front1 3 --cut_tail --trim_tail1 3 --cut_mean_quality 10 --length_required 50 \"\n         reportOnly: false \n       timeLimit: \"AUTO\"\n    nonpareil:\n      additionalParams: \" -v 10 -r 1234 \"\n    kmc:\n      timeLimit: \"AUTO\"\n      additionalParams:\n        # Computes k-mer distribution based on k-mer length 13 and 21\n        #  -sm - use strict memory mode (memory limit from -m&lt;n&gt; switch will not be exceeded)\n        #  -cs&lt;value&gt; - maximal value of a counter\n        count: \" -sm -cs10000 \"\n        histo: \" -cx50000 \"\n    filterHuman:\n      additionalParams: \"  \"\n      database:\n        download:\n          source: https://openstack.cebitec.uni-bielefeld.de:8080/databases/human_filter.db.20231218v2.gz\n          md5sum: cc92c0f926656565b1156d66a0db5a3c\n  assembly:\n    megahit:\n      # --mem-flag 0 to use minimum memory, --mem-flag 1 (default) moderate memory and --mem-flag 2 all memory.\n      # meta-sensitive: '--min-count 1 --k-list 21,29,39,49,...,129,141' \n      # meta-large:  '--k-min  27  --k-max 127 --k-step 10' (large &amp; complex metagenomes, like soil)\n      additionalParams: \" --min-contig-len 1000 --presets meta-sensitive \"\n      fastg: true\n      resources:\n        RAM:\n          mode: 'PREDICT'\n          predictMinLabel: 'medium'\n  binning:\n    bwa2:\n      additionalParams: \n        bwa2: \" \"\n        # samtools flags are used to filter resulting bam file\n        samtoolsView: \" -F 3584 \" \n    contigsCoverage:\n      additionalParams: \" --min-covered-fraction 0 --min-read-percent-identity 100 --min-read-aligned-percent 100 \"\n    genomeCoverage:\n      additionalParams: \" --min-covered-fraction 0 --min-read-percent-identity 100 --min-read-aligned-percent 100 \"\n    # Primary binning tool\n    metabat:\n      # Set --seed positive numbers to reproduce the result exactly. Otherwise, random seed will be set each time.\n      additionalParams: \" --seed 234234  \"\nresources:\n  highmemLarge:\n    cpus: 28\n    memory: 230\n  highmemMedium:\n    cpus: 14\n    memory: 113\n  large:\n    cpus: 28\n    memory: 58\n  medium:\n    cpus: 14\n    memory: 29\n  small:\n    cpus: 7\n    memory: 14\n  tiny:\n    cpus: 1\n    memory: 1\n</code></pre> <p>Task 1</p> <p>You saw the introduction part of the tutorial regarding the structure of the config file.  Can you tell which tools or methods are executed in the QC part?</p> Solution <p>Fastp, nonpareil, filterHuman, kmc</p> <p>The following configuration runs the tools </p> <p>Task 2</p> <p>Change into the directory of the Toolkit session. <pre><code>cd /vol/volume/sessions/metagenomics_metagenomics-tk \n</code></pre></p> <p>Copy the following command to execute the Toolkit. The Toolkit will need about 10 to 12 minutes to complete.</p> <pre><code>NXF_VER=25.04.2 nextflow run metagenomics/metagenomics-tk \\\n      -profile standard \\\n      -params-file https://raw.githubusercontent.com/metagenomics/metagenomics-tk/refs/heads/doc/exhibition-day/tutorial/default/tutorials/tutorial2/fullPipeline_reads_to_genomes.yml \\\n      -ansi-log false \\\n      -entry wFullPipeline \\\n      -resume \\\n      --input.paired.path https://raw.githubusercontent.com/metagenomics/metagenomics-tk/refs/heads/doc/exhibition-day/tutorial/test_data/tutorials/tutorial2/samples.tsv \\\n      --logDir logs_reads_to_genomes \n</code></pre>"},{"location":"tutorials/tutorial2/reads_to_genomes/#output","title":"Output","text":"<p>In the following we will inspect the outputs of each analysis step.</p>"},{"location":"tutorials/tutorial2/reads_to_genomes/#quality-control","title":"Quality Control","text":""},{"location":"tutorials/tutorial2/reads_to_genomes/#fastp","title":"Fastp","text":"<p>Fastp is an efficient, versatile, open-source tool for preprocessing FASTQ files and quality control in next-generation sequencing (NGS) workflows. For quality and adapter trimming fastp trims reads from 5\u2019 end to 3\u2019 end using a sliding window. </p> <p>If the mean quality of bases inside a window drops below a specific q-score, the remainder of the read will be trimmed. If a read gets too short during this trimming, it will be discarded.</p> <p>Quality Report</p> <p>The Metagenomics-Toolkit allows you to run fastp by only reporting the quality of the data. This way you can easily inspect the data before actually further processing it. You can do this by setting the <code>reportOnly</code> parameter to <code>true</code> in the config below. </p> <p>Task 3</p> <p>Change into the directory of the Toolkit session. <pre><code>cd /vol/volume/sessions/metagenomics_metagenomics-tk \n</code></pre></p> <p>You can view the fastp output in the qc output directory: <pre><code>ls -1 output/*/1/qc/*/fastp/\n</code></pre></p> <p>which results in the following output:</p> <pre><code>output/SRR492065/1/qc/0.4.0/fastp:\nSRR492065_fastp.json\nSRR492065_fastp_summary_after.tsv\nSRR492065_fastp_summary_before.tsv\nSRR492065_interleaved.qc.fq.gz\nSRR492065_report.html\nSRR492065_unpaired.qc.fq.gz\nSRR492065_unpaired_summary.tsv\n\noutput/SRR492183/1/qc/0.4.0/fastp:\nSRR492183_fastp.json\nSRR492183_fastp_summary_after.tsv\nSRR492183_fastp_summary_before.tsv\nSRR492183_interleaved.qc.fq.gz\nSRR492183_report.html\nSRR492183_unpaired.qc.fq.gz\nSRR492183_unpaired_summary.tsv\n</code></pre> <p>The fastp output for your data will look like the following.</p> <p>Task 4</p> <p>How many reads of the wastewater dataset passed the filter? Between which quality score ranges does the read1 FASTQ file fall before and after QC.</p> Solution <p>90.374006% of the reads passed the filter. The quality score for read1 ranges between 26 and 38 before filtering and after QC between 34 and 39. </p>"},{"location":"tutorials/tutorial2/reads_to_genomes/#host-dna-removal","title":"Host DNA Removal","text":"<p>The Toolkit provides the SRA human-scrubber tool that uses a k-mer based approach to search for human sequences against a k-mer database using human reference sequences.</p> <p>Task 5</p> <p>You can find the result of the tool in the filterHuman directory: The output of the tool are the filtered sequences, and it also includes statistics about the sequences before and after filtering.   </p> <p>Change into the directory of the Toolkit session. <pre><code>cd /vol/volume/sessions/metagenomics_metagenomics-tk \n</code></pre></p> <pre><code>ls output/*/1/qc/*/filterHuman/\n</code></pre> <p>The following command tells you how many sequences are left after filtering:</p> <pre><code>column -t  -s$'\\t' output/*/1/qc/*/filterHuman/*_interleaved_summary_*\n</code></pre> <p>The num_seqs column tells you the number of sequences.</p> <p>How many reads were removed from the respective dataset after filtering?</p> Solution <p>3768 sequences were detected for SRR492065 and 644 for SRR492183.  </p> <p>Please note that it is important to remove human reads before publishing read datasets, since, according to Bush et al. [^1], even a small number of reads can be used to impute phenotypes.  </p>"},{"location":"tutorials/tutorial2/reads_to_genomes/#assembly-results","title":"Assembly Results","text":"<p>Once your raw reads have been quality-controlled, we will now perform an assembly using the Metagenomics-Toolkit. The Toolkit supports several assemblers, including MEGAHIT, which we will run in this section.</p> <p>Task 6</p> <p>Which additional parameters are used by MEGAHIT?</p> Solution <p>MEGAHIT with the additional parameters: minimum contig length of <code>500 bp</code> and the preset <code>meta-sensitive</code>. <code>meta-sensitive</code> sets the following parameters: <code>--min-count 1 --k-list 21,29,39,49,...,129,141</code>, which causes  MEGAHIT to use a longer list of k-mers. See the MEGAHIT home page for more info.</p> <p>We will now have a first look at some assembly statistics. First of all, locate your assembly results in your <code>output</code> directory for the SRR492065 sample.</p> <p>Task 7</p> <p>Where are the assembly results of MEGAHIT stored? And what files are generated per sample?</p> Solution <p>Change into the directory of the Toolkit session. <pre><code>cd /vol/volume/sessions/metagenomics_metagenomics-tk\n</code></pre> The assembly results are stored in <code>output/data/1/assembly/1.2.1/megahit/</code> <pre><code>ls -1 output/SRR492065/1/assembly/1.2.3/megahit/\n</code></pre> Output: <pre><code>    SRR492065_contigs.fa.gz  # the assembled sequences as gzipped fasta\n    SRR492065_contigs.fastg  # the assembly graph for inspection for example with Bandage\n    SRR492065_contigs_stats.tsv  # some assembly statistics\n</code></pre></p> <p>Task 8</p> <p>Have a look at the <code>SRR492065_contigs_stats.tsv</code> file - how large is the assembly and what is the N50?</p> Solution <p>Change into the directory of the Toolkit session. <pre><code>cd /vol/volume/sessions/metagenomics_metagenomics-tk\n</code></pre> View the contig stats: <pre><code>cat output/SRR492065/1/assembly/1.2.3/megahit/SRR492065_contigs_stats.tsv | column -s$'\\t' -t\n</code></pre> Output:  <pre><code>SAMPLE     file                     format  type  num_seqs  sum_len   min_len  avg_len  max_len  Q1      Q2      Q3      sum_gap  N50    Q20(%)  Q30(%)  GC(%)\nSRR492065  SRR492065_contigs.fa.gz  FASTA   DNA   1577      11206329  1000     7106.1   245235   1554.0  2659.0  6398.0  0        16801  0.00    0.00    40.46\n</code></pre> In this assembly run, the assembly length is  11,206,329 bp and the N50 is 16,801 bp. </p>"},{"location":"tutorials/tutorial2/reads_to_genomes/#binning-results","title":"Binning Results","text":"<p>Binning is a process in metagenomics that involves grouping DNA fragments, known as contigs, into bins that likely originate from the same organism or genome. This technique is essential for reconstructing genomes from mixed microbial communities, where DNA from various organisms is intermingled, and there is often no prior knowledge of the species present.</p>"},{"location":"tutorials/tutorial2/reads_to_genomes/#metabat","title":"MetaBAT","text":"<p>MetaBAT is an automated metagenome binning software which integrates empirical probabilistic distances of genome abundance and tetranucleotide frequency. See the MetaBAT home page for more information.</p> <p>Let's now inspect the bins created by MetaBAT:</p> <p>Task 9</p> <p>How many bins did metabat generate for the dataset SRR492065? Locate the metabat results and the fasta files for each bin in the output folder.</p> Solution <p>Change into the directory of the Toolkit session. <pre><code>cd /vol/volume/sessions/metagenomics_metagenomics-tk\n</code></pre> List the output of the MetaBAT tool:  <pre><code>ls -1 output/SRR492065/1/binning/*/metabat/\n</code></pre></p> <p>Below you can see that MetaBAT generated 5 bins. The file <code>SRR492065_notBinned.fa</code> contains contigs that could not be binned. <pre><code>SRR492065_bin.1.fa\nSRR492065_bin.2.fa\nSRR492065_bin.3.fa\nSRR492065_bin.4.fa\nSRR492065_bin.5.fa\nSRR492065_bin_contig_mapping.tsv\nSRR492065_bins_stats.tsv\nSRR492065_contigs_depth.tsv\nSRR492065_notBinned.fa\n</code></pre></p> <p>The next question you might want to ask is whether you can trust these bins and which organism they represent according to a taxonomy. Before that, let's have a look at what other information the Metagenomics-Toolkit provides as part of the binning output.</p> <p>Task 10</p> <p>There is a file containing some statistics on the generated bins <code>SRR492065_bins_stats.tsv</code>.  Find out, which bin has the highest coverage and which one has the highest N50.</p> Solution <p>Change into the directory of the Toolkit session. <pre><code>cd /vol/volume/sessions/metagenomics_metagenomics-tk\n</code></pre> You can have a look on some bin statistics with: <pre><code>cat output/SRR492065/1/binning/*/metabat/SRR492065_bins_stats.tsv  | column -s$'\\t' -t\n</code></pre> Output: <pre><code>SAMPLE     BIN_ID              format  type  num_seqs  sum_len  min_len  avg_len  max_len  Q1       Q2       Q3        sum_gap  N50     Q20(%)  Q30(%)  GC(%)  COVERAGE\nSRR492065  SRR492065_bin.1.fa  FASTA   DNA   16        1536873  4900     96054.6  245235   22104.0  83532.5  158838.0  0        161674  0.00    0.00    36.70  151.714\nSRR492065  SRR492065_bin.2.fa  FASTA   DNA   20        1280335  3167     64016.8  238570   23905.5  45943.5  65927.0   0        185023  0.00    0.00    38.14  169.393\nSRR492065  SRR492065_bin.3.fa  FASTA   DNA   174       2345130  2626     13477.8  68479    6239.0   9839.5   16573.0   0        16802   0.00    0.00    63.56  23.2949\nSRR492065  SRR492065_bin.4.fa  FASTA   DNA   436       2720223  2505     6239.0   36553    3386.0   4642.5   7408.5    0        7308    0.00    0.00    32.98  14.7728\nSRR492065  SRR492065_bin.5.fa  FASTA   DNA   96        1522358  2582     15857.9  63297    6157.0   10569.0  23635.5   0        25145   0.00    0.00    29.95  70.5654\n</code></pre> In this result, Bin 2 has the highest coverage (169.393) and highest N50 (185023).</p> <p>\u27a1\ufe0f Continue to: Assessing Bin Quality, Lineage, and Gene Function</p> <p>[^1:] https://pmc.ncbi.nlm.nih.gov/articles/PMC7478626/</p>"}]}